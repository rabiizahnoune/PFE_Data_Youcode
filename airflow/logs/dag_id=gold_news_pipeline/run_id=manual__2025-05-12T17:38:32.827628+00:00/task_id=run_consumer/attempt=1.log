[2025-05-12T17:38:40.262+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: gold_news_pipeline.run_consumer manual__2025-05-12T17:38:32.827628+00:00 [queued]>
[2025-05-12T17:38:40.302+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: gold_news_pipeline.run_consumer manual__2025-05-12T17:38:32.827628+00:00 [queued]>
[2025-05-12T17:38:40.305+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-05-12T17:38:40.377+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): run_consumer> on 2025-05-12 17:38:32.827628+00:00
[2025-05-12T17:38:40.399+0000] {standard_task_runner.py:57} INFO - Started process 4156 to run task
[2025-05-12T17:38:40.423+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'gold_news_pipeline', 'run_consumer', 'manual__2025-05-12T17:38:32.827628+00:00', '--job-id', '87', '--raw', '--subdir', 'DAGS_FOLDER/dag_news.py', '--cfg-path', '/tmp/tmp5m8eyrlb']
[2025-05-12T17:38:40.441+0000] {standard_task_runner.py:85} INFO - Job 87: Subtask run_consumer
[2025-05-12T17:38:40.580+0000] {task_command.py:415} INFO - Running <TaskInstance: gold_news_pipeline.run_consumer manual__2025-05-12T17:38:32.827628+00:00 [running]> on host 51d6065cf9f5
[2025-05-12T17:38:40.776+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='gold_news_pipeline' AIRFLOW_CTX_TASK_ID='run_consumer' AIRFLOW_CTX_EXECUTION_DATE='2025-05-12T17:38:32.827628+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-12T17:38:32.827628+00:00'
[2025-05-12T17:38:40.779+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-05-12T17:38:40.781+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec gold_price_project-spark-1 spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 /scripts/spark_news.py']
[2025-05-12T17:38:40.799+0000] {subprocess.py:86} INFO - Output:
[2025-05-12T17:38:50.660+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-05-12T17:38:50.962+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /root/.ivy2/cache
[2025-05-12T17:38:50.964+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /root/.ivy2/jars
[2025-05-12T17:38:50.980+0000] {subprocess.py:93} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-05-12T17:38:50.981+0000] {subprocess.py:93} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2025-05-12T17:38:50.984+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-156eed54-6a4f-40a5-a4a9-d681bc82ec24;1.0
[2025-05-12T17:38:50.988+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-05-12T17:38:55.214+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in local-m2-cache
[2025-05-12T17:38:55.821+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2025-05-12T17:38:55.993+0000] {subprocess.py:93} INFO - 	found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-05-12T17:38:56.166+0000] {subprocess.py:93} INFO - 	found org.lz4#lz4-java;1.8.0 in central
[2025-05-12T17:38:56.319+0000] {subprocess.py:93} INFO - 	found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2025-05-12T17:38:57.232+0000] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;2.0.7 in central
[2025-05-12T17:38:59.259+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-05-12T17:38:59.548+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-05-12T17:39:01.972+0000] {subprocess.py:93} INFO - 	found commons-logging#commons-logging;1.1.3 in central
[2025-05-12T17:39:02.103+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-05-12T17:39:04.243+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-pool2;2.11.1 in central
[2025-05-12T17:39:04.363+0000] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central
[2025-05-12T17:39:04.487+0000] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central
[2025-05-12T17:39:04.614+0000] {subprocess.py:93} INFO - 	found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2025-05-12T17:39:06.969+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2025-05-12T17:39:07.100+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#native-protocol;1.5.0 in central
[2025-05-12T17:39:07.228+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2025-05-12T17:39:07.351+0000] {subprocess.py:93} INFO - 	found com.typesafe#config;1.4.1 in central
[2025-05-12T17:39:08.235+0000] {subprocess.py:93} INFO - 	found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2025-05-12T17:39:08.374+0000] {subprocess.py:93} INFO - 	found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2025-05-12T17:39:08.493+0000] {subprocess.py:93} INFO - 	found org.reactivestreams#reactive-streams;1.0.3 in central
[2025-05-12T17:39:09.333+0000] {subprocess.py:93} INFO - 	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-05-12T17:39:09.455+0000] {subprocess.py:93} INFO - 	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2025-05-12T17:39:09.582+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.2 in central
[2025-05-12T17:39:09.732+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2025-05-12T17:39:09.857+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2025-05-12T17:39:11.034+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-lang3;3.10 in central
[2025-05-12T17:39:12.337+0000] {subprocess.py:93} INFO - 	found com.thoughtworks.paranamer#paranamer;2.8 in central
[2025-05-12T17:39:12.449+0000] {subprocess.py:93} INFO - 	found org.scala-lang#scala-reflect;2.12.11 in central
[2025-05-12T17:39:12.464+0000] {subprocess.py:93} INFO - downloading file:/root/.m2/repository/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
[2025-05-12T17:39:12.466+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (3ms)
[2025-05-12T17:39:12.521+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.5.0/spark-cassandra-connector_2.12-3.5.0.jar ...
[2025-05-12T17:39:12.946+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.5.0!spark-cassandra-connector_2.12.jar (477ms)
[2025-05-12T17:39:12.994+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
[2025-05-12T17:39:13.050+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (104ms)
[2025-05-12T17:39:13.105+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[2025-05-12T17:39:13.793+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (743ms)
[2025-05-12T17:39:13.849+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2025-05-12T17:39:13.915+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (121ms)
[2025-05-12T17:39:13.970+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2025-05-12T17:39:15.975+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (2057ms)
[2025-05-12T17:39:16.024+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2025-05-12T17:39:16.126+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (151ms)
[2025-05-12T17:39:16.186+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2025-05-12T17:39:16.399+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (273ms)
[2025-05-12T17:39:16.449+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2025-05-12T17:39:16.508+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (107ms)
[2025-05-12T17:39:16.556+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2025-05-12T17:39:17.871+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (1362ms)
[2025-05-12T17:39:17.926+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2025-05-12T17:39:17.998+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (119ms)
[2025-05-12T17:39:18.048+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.5.0/spark-cassandra-connector-driver_2.12-3.5.0.jar ...
[2025-05-12T17:39:18.192+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0!spark-cassandra-connector-driver_2.12.jar (180ms)
[2025-05-12T17:39:18.229+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.11.0/scala-collection-compat_2.12-2.11.0.jar ...
[2025-05-12T17:39:18.308+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.11.0!scala-collection-compat_2.12.jar (131ms)
[2025-05-12T17:39:18.361+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...
[2025-05-12T17:39:19.445+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (1137ms)
[2025-05-12T17:39:19.499+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...
[2025-05-12T17:39:19.559+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (114ms)
[2025-05-12T17:39:19.607+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...
[2025-05-12T17:39:19.694+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (133ms)
[2025-05-12T17:39:19.744+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...
[2025-05-12T17:39:19.798+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (103ms)
[2025-05-12T17:39:19.850+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...
[2025-05-12T17:39:20.209+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (410ms)
[2025-05-12T17:39:20.262+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...
[2025-05-12T17:39:20.370+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (143ms)
[2025-05-12T17:39:20.420+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...
[2025-05-12T17:39:20.701+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (344ms)
[2025-05-12T17:39:20.754+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...
[2025-05-12T17:39:20.844+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (139ms)
[2025-05-12T17:39:20.907+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...
[2025-05-12T17:39:20.986+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (135ms)
[2025-05-12T17:39:21.039+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...
[2025-05-12T17:39:21.112+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (126ms)
[2025-05-12T17:39:21.165+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...
[2025-05-12T17:39:21.220+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (104ms)
[2025-05-12T17:39:21.268+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...
[2025-05-12T17:39:21.318+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (99ms)
[2025-05-12T17:39:21.366+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...
[2025-05-12T17:39:21.418+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (97ms)
[2025-05-12T17:39:21.467+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...
[2025-05-12T17:39:21.518+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (99ms)
[2025-05-12T17:39:21.566+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...
[2025-05-12T17:39:21.638+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (119ms)
[2025-05-12T17:39:21.640+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 21479ms :: artifacts dl 9175ms
[2025-05-12T17:39:21.642+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-05-12T17:39:21.643+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2025-05-12T17:39:21.643+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2025-05-12T17:39:21.644+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2025-05-12T17:39:21.648+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2025-05-12T17:39:21.650+0000] {subprocess.py:93} INFO - 	com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2025-05-12T17:39:21.652+0000] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]
[2025-05-12T17:39:21.653+0000] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]
[2025-05-12T17:39:21.654+0000] {subprocess.py:93} INFO - 	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2025-05-12T17:39:21.655+0000] {subprocess.py:93} INFO - 	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-05-12T17:39:21.655+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2025-05-12T17:39:21.656+0000] {subprocess.py:93} INFO - 	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2025-05-12T17:39:21.656+0000] {subprocess.py:93} INFO - 	com.typesafe#config;1.4.1 from central in [default]
[2025-05-12T17:39:21.657+0000] {subprocess.py:93} INFO - 	commons-logging#commons-logging;1.1.3 from central in [default]
[2025-05-12T17:39:21.658+0000] {subprocess.py:93} INFO - 	io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2025-05-12T17:39:21.658+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-lang3;3.10 from central in [default]
[2025-05-12T17:39:21.664+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2025-05-12T17:39:21.666+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2025-05-12T17:39:21.667+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-05-12T17:39:21.668+0000] {subprocess.py:93} INFO - 	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
[2025-05-12T17:39:21.668+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from local-m2-cache in [default]
[2025-05-12T17:39:21.669+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
[2025-05-12T17:39:21.670+0000] {subprocess.py:93} INFO - 	org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2025-05-12T17:39:21.670+0000] {subprocess.py:93} INFO - 	org.lz4#lz4-java;1.8.0 from central in [default]
[2025-05-12T17:39:21.671+0000] {subprocess.py:93} INFO - 	org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2025-05-12T17:39:21.671+0000] {subprocess.py:93} INFO - 	org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2025-05-12T17:39:21.672+0000] {subprocess.py:93} INFO - 	org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2025-05-12T17:39:21.672+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;2.0.7 from central in [default]
[2025-05-12T17:39:21.673+0000] {subprocess.py:93} INFO - 	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2025-05-12T17:39:21.674+0000] {subprocess.py:93} INFO - 	:: evicted modules:
[2025-05-12T17:39:21.684+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]
[2025-05-12T17:39:21.685+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.7] in [default]
[2025-05-12T17:39:21.686+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T17:39:21.686+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-05-12T17:39:21.687+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-12T17:39:21.687+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T17:39:21.689+0000] {subprocess.py:93} INFO - 	|      default     |   30  |   29  |   29  |   2   ||   28  |   28  |
[2025-05-12T17:39:21.696+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T17:39:21.698+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-156eed54-6a4f-40a5-a4a9-d681bc82ec24
[2025-05-12T17:39:21.701+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-05-12T17:39:21.727+0000] {subprocess.py:93} INFO - 	1 artifacts copied, 27 already retrieved (422kB/53ms)
[2025-05-12T17:39:22.226+0000] {subprocess.py:93} INFO - 25/05/12 17:39:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-12T17:39:24.296+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SparkContext: Running Spark version 3.5.1
[2025-05-12T17:39:24.297+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-12T17:39:24.298+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SparkContext: Java version 11.0.22
[2025-05-12T17:39:24.337+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO ResourceUtils: ==============================================================
[2025-05-12T17:39:24.338+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-05-12T17:39:24.338+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO ResourceUtils: ==============================================================
[2025-05-12T17:39:24.339+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SparkContext: Submitted application: GoldNewsStreaming
[2025-05-12T17:39:24.379+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-05-12T17:39:24.397+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO ResourceProfile: Limiting resource is cpu
[2025-05-12T17:39:24.397+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-05-12T17:39:24.484+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SecurityManager: Changing view acls to: root
[2025-05-12T17:39:24.486+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SecurityManager: Changing modify acls to: root
[2025-05-12T17:39:24.488+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SecurityManager: Changing view acls groups to:
[2025-05-12T17:39:24.489+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SecurityManager: Changing modify acls groups to:
[2025-05-12T17:39:24.490+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
[2025-05-12T17:39:24.919+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO Utils: Successfully started service 'sparkDriver' on port 37775.
[2025-05-12T17:39:24.981+0000] {subprocess.py:93} INFO - 25/05/12 17:39:24 INFO SparkEnv: Registering MapOutputTracker
[2025-05-12T17:39:25.040+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkEnv: Registering BlockManagerMaster
[2025-05-12T17:39:25.088+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-05-12T17:39:25.089+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-05-12T17:39:25.101+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-05-12T17:39:25.176+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9e27448f-c6c5-4b43-a2d2-4111ea68e43a
[2025-05-12T17:39:25.208+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-05-12T17:39:25.228+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-05-12T17:39:25.507+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-05-12T17:39:25.727+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-05-12T17:39:25.876+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://d4e9ca837c07:37775/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.878+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://d4e9ca837c07:37775/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.879+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://d4e9ca837c07:37775/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.882+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://d4e9ca837c07:37775/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747071564279
[2025-05-12T17:39:25.883+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://d4e9ca837c07:37775/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747071564279
[2025-05-12T17:39:25.883+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://d4e9ca837c07:37775/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747071564279
[2025-05-12T17:39:25.884+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://d4e9ca837c07:37775/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.884+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://d4e9ca837c07:37775/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747071564279
[2025-05-12T17:39:25.885+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://d4e9ca837c07:37775/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747071564279
[2025-05-12T17:39:25.886+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://d4e9ca837c07:37775/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747071564279
[2025-05-12T17:39:25.886+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://d4e9ca837c07:37775/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747071564279
[2025-05-12T17:39:25.889+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://d4e9ca837c07:37775/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.890+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://d4e9ca837c07:37775/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.892+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.893+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.894+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://d4e9ca837c07:37775/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747071564279
[2025-05-12T17:39:25.895+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://d4e9ca837c07:37775/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747071564279
[2025-05-12T17:39:25.895+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://d4e9ca837c07:37775/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747071564279
[2025-05-12T17:39:25.896+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://d4e9ca837c07:37775/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.897+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747071564279
[2025-05-12T17:39:25.897+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://d4e9ca837c07:37775/jars/com.typesafe_config-1.4.1.jar with timestamp 1747071564279
[2025-05-12T17:39:25.898+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://d4e9ca837c07:37775/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747071564279
[2025-05-12T17:39:25.898+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://d4e9ca837c07:37775/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747071564279
[2025-05-12T17:39:25.899+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://d4e9ca837c07:37775/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747071564279
[2025-05-12T17:39:25.900+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://d4e9ca837c07:37775/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747071564279
[2025-05-12T17:39:25.900+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://d4e9ca837c07:37775/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747071564279
[2025-05-12T17:39:25.901+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://d4e9ca837c07:37775/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747071564279
[2025-05-12T17:39:25.902+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.907+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.910+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:39:25.963+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:25.969+0000] {subprocess.py:93} INFO - 25/05/12 17:39:25 INFO Utils: Copying /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T17:39:26.020+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:26.032+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:39:26.034+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747071564279
[2025-05-12T17:39:26.036+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T17:39:26.061+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747071564279
[2025-05-12T17:39:26.063+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T17:39:26.070+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747071564279
[2025-05-12T17:39:26.073+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T17:39:26.219+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747071564279
[2025-05-12T17:39:26.227+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T17:39:26.369+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747071564279
[2025-05-12T17:39:26.372+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T17:39:26.401+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747071564279
[2025-05-12T17:39:26.402+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T17:39:26.416+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747071564279
[2025-05-12T17:39:26.418+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T17:39:26.527+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747071564279
[2025-05-12T17:39:26.534+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T17:39:26.542+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:26.542+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T17:39:26.564+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747071564279
[2025-05-12T17:39:26.565+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T17:39:26.571+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:26.572+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T17:39:26.650+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:26.655+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T17:39:26.663+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747071564279
[2025-05-12T17:39:26.664+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T17:39:26.680+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747071564279
[2025-05-12T17:39:26.687+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T17:39:26.707+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747071564279
[2025-05-12T17:39:26.708+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T17:39:26.738+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:26.739+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T17:39:26.747+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747071564279
[2025-05-12T17:39:26.748+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T17:39:26.767+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar at file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1747071564279
[2025-05-12T17:39:26.768+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.typesafe_config-1.4.1.jar
[2025-05-12T17:39:26.777+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747071564279
[2025-05-12T17:39:26.778+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T17:39:26.782+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747071564279
[2025-05-12T17:39:26.783+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T17:39:26.793+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747071564279
[2025-05-12T17:39:26.794+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T17:39:26.797+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747071564279
[2025-05-12T17:39:26.798+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T17:39:26.802+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747071564279
[2025-05-12T17:39:26.806+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T17:39:26.810+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747071564279
[2025-05-12T17:39:26.811+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T17:39:26.818+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:26.820+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T17:39:26.953+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Executor: Starting executor ID driver on host d4e9ca837c07
[2025-05-12T17:39:26.954+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-12T17:39:26.954+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Executor: Java version 11.0.22
[2025-05-12T17:39:26.968+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-05-12T17:39:26.969+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@252761b2 for default.
[2025-05-12T17:39:26.992+0000] {subprocess.py:93} INFO - 25/05/12 17:39:26 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.021+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:39:27.026+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747071564279
[2025-05-12T17:39:27.027+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T17:39:27.031+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747071564279
[2025-05-12T17:39:27.060+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T17:39:27.071+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747071564279
[2025-05-12T17:39:27.093+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T17:39:27.100+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747071564279
[2025-05-12T17:39:27.102+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T17:39:27.106+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747071564279
[2025-05-12T17:39:27.107+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T17:39:27.118+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.118+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T17:39:27.122+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.133+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T17:39:27.137+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1747071564279
[2025-05-12T17:39:27.137+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.typesafe_config-1.4.1.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.typesafe_config-1.4.1.jar
[2025-05-12T17:39:27.145+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747071564279
[2025-05-12T17:39:27.147+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T17:39:27.152+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747071564279
[2025-05-12T17:39:27.153+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T17:39:27.161+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.162+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:39:27.166+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747071564279
[2025-05-12T17:39:27.167+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T17:39:27.170+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747071564279
[2025-05-12T17:39:27.175+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T17:39:27.178+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.179+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T17:39:27.183+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747071564279
[2025-05-12T17:39:27.184+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T17:39:27.190+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.191+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T17:39:27.194+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747071564279
[2025-05-12T17:39:27.195+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T17:39:27.198+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747071564279
[2025-05-12T17:39:27.198+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T17:39:27.205+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747071564279
[2025-05-12T17:39:27.206+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T17:39:27.209+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747071564279
[2025-05-12T17:39:27.212+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T17:39:27.214+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.221+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T17:39:27.226+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747071564279
[2025-05-12T17:39:27.227+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T17:39:27.230+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747071564279
[2025-05-12T17:39:27.236+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T17:39:27.238+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.238+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T17:39:27.241+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.242+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T17:39:27.245+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.251+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T17:39:27.255+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747071564279
[2025-05-12T17:39:27.259+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T17:39:27.271+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747071564279
[2025-05-12T17:39:27.334+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO TransportClientFactory: Successfully created connection to d4e9ca837c07/172.18.0.9:37775 after 46 ms (0 ms spent in bootstraps)
[2025-05-12T17:39:27.349+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp13434073287397394575.tmp
[2025-05-12T17:39:27.381+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp13434073287397394575.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T17:39:27.391+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2025-05-12T17:39:27.392+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747071564279
[2025-05-12T17:39:27.392+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp16740537837296304865.tmp
[2025-05-12T17:39:27.397+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp16740537837296304865.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T17:39:27.405+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.commons_commons-lang3-3.10.jar to class loader default
[2025-05-12T17:39:27.406+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.406+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp7920656080145849195.tmp
[2025-05-12T17:39:27.409+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp7920656080145849195.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:39:27.419+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
[2025-05-12T17:39:27.420+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747071564279
[2025-05-12T17:39:27.421+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp10454454686595616259.tmp
[2025-05-12T17:39:27.451+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp10454454686595616259.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T17:39:27.456+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
[2025-05-12T17:39:27.457+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747071564279
[2025-05-12T17:39:27.458+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp7463413887947524343.tmp
[2025-05-12T17:39:27.466+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp7463413887947524343.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T17:39:27.467+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.google.code.findbugs_jsr305-3.0.2.jar to class loader default
[2025-05-12T17:39:27.468+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747071564279
[2025-05-12T17:39:27.469+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp11069268384607636652.tmp
[2025-05-12T17:39:27.473+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp11069268384607636652.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T17:39:27.479+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.hdrhistogram_HdrHistogram-2.1.12.jar to class loader default
[2025-05-12T17:39:27.480+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747071564279
[2025-05-12T17:39:27.481+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp487037389209911648.tmp
[2025-05-12T17:39:27.485+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp487037389209911648.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T17:39:27.488+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/commons-logging_commons-logging-1.1.3.jar to class loader default
[2025-05-12T17:39:27.490+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747071564279
[2025-05-12T17:39:27.495+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp1154507856052222776.tmp
[2025-05-12T17:39:27.496+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp1154507856052222776.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T17:39:27.500+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to class loader default
[2025-05-12T17:39:27.501+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747071564279
[2025-05-12T17:39:27.502+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp8261865433832815251.tmp
[2025-05-12T17:39:27.503+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp8261865433832815251.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T17:39:27.510+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.reactivestreams_reactive-streams-1.0.3.jar to class loader default
[2025-05-12T17:39:27.512+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747071564279
[2025-05-12T17:39:27.513+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp8745934313797673673.tmp
[2025-05-12T17:39:27.744+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp8745934313797673673.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T17:39:27.752+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
[2025-05-12T17:39:27.754+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.756+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp7595184158659283933.tmp
[2025-05-12T17:39:27.762+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp7595184158659283933.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T17:39:27.770+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to class loader default
[2025-05-12T17:39:27.771+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747071564279
[2025-05-12T17:39:27.772+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp10929988559431140471.tmp
[2025-05-12T17:39:27.773+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp10929988559431140471.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T17:39:27.777+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.thoughtworks.paranamer_paranamer-2.8.jar to class loader default
[2025-05-12T17:39:27.778+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747071564279
[2025-05-12T17:39:27.778+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp17261203330956956407.tmp
[2025-05-12T17:39:27.803+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp17261203330956956407.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T17:39:27.805+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.scala-lang_scala-reflect-2.12.11.jar to class loader default
[2025-05-12T17:39:27.806+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747071564279
[2025-05-12T17:39:27.807+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp14363377231311766928.tmp
[2025-05-12T17:39:27.808+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp14363377231311766928.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T17:39:27.814+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/io.dropwizard.metrics_metrics-core-4.1.18.jar to class loader default
[2025-05-12T17:39:27.816+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.817+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp14359848519152274672.tmp
[2025-05-12T17:39:27.819+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp14359848519152274672.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T17:39:27.822+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_native-protocol-1.5.0.jar to class loader default
[2025-05-12T17:39:27.822+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.823+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp15640350640201040991.tmp
[2025-05-12T17:39:27.864+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp15640350640201040991.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T17:39:27.869+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to class loader default
[2025-05-12T17:39:27.870+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.870+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp3751812501883914549.tmp
[2025-05-12T17:39:27.878+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp3751812501883914549.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T17:39:27.880+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-query-builder-4.13.0.jar to class loader default
[2025-05-12T17:39:27.881+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747071564279
[2025-05-12T17:39:27.882+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp16193014445620670469.tmp
[2025-05-12T17:39:27.883+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp16193014445620670469.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T17:39:27.886+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to class loader default
[2025-05-12T17:39:27.887+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747071564279
[2025-05-12T17:39:27.890+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp15710548786750363433.tmp
[2025-05-12T17:39:27.913+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp15710548786750363433.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T17:39:27.923+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
[2025-05-12T17:39:27.924+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747071564279
[2025-05-12T17:39:27.925+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp2035054139410906255.tmp
[2025-05-12T17:39:27.941+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp2035054139410906255.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T17:39:27.945+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to class loader default
[2025-05-12T17:39:27.946+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747071564279
[2025-05-12T17:39:27.947+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp14163201801527690074.tmp
[2025-05-12T17:39:27.954+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp14163201801527690074.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T17:39:27.957+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to class loader default
[2025-05-12T17:39:27.958+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747071564279
[2025-05-12T17:39:27.959+0000] {subprocess.py:93} INFO - 25/05/12 17:39:27 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp6070457840344842532.tmp
[2025-05-12T17:39:28.053+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp6070457840344842532.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T17:39:28.063+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
[2025-05-12T17:39:28.064+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747071564279
[2025-05-12T17:39:28.065+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp6151514251625336424.tmp
[2025-05-12T17:39:28.066+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp6151514251625336424.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T17:39:28.078+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to class loader default
[2025-05-12T17:39:28.084+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747071564279
[2025-05-12T17:39:28.096+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp3029794880211149907.tmp
[2025-05-12T17:39:28.099+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp3029794880211149907.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T17:39:28.104+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.lz4_lz4-java-1.8.0.jar to class loader default
[2025-05-12T17:39:28.107+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:28.108+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp18048769559415256250.tmp
[2025-05-12T17:39:28.125+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp18048769559415256250.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T17:39:28.128+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to class loader default
[2025-05-12T17:39:28.129+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747071564279
[2025-05-12T17:39:28.129+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp10217475337332799219.tmp
[2025-05-12T17:39:28.130+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp10217475337332799219.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T17:39:28.137+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.slf4j_slf4j-api-2.0.7.jar to class loader default
[2025-05-12T17:39:28.138+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/com.typesafe_config-1.4.1.jar with timestamp 1747071564279
[2025-05-12T17:39:28.139+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp6870412523039470627.tmp
[2025-05-12T17:39:28.141+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp6870412523039470627.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.typesafe_config-1.4.1.jar
[2025-05-12T17:39:28.145+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/com.typesafe_config-1.4.1.jar to class loader default
[2025-05-12T17:39:28.145+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Fetching spark://d4e9ca837c07:37775/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071564279
[2025-05-12T17:39:28.146+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: Fetching spark://d4e9ca837c07:37775/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp66328700580076616.tmp
[2025-05-12T17:39:28.152+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/fetchFileTemp66328700580076616.tmp has been previously copied to /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:39:28.155+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Executor: Adding file:/tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/userFiles-a78eacab-c7cd-40b9-b058-51811485fe2e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
[2025-05-12T17:39:28.170+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36533.
[2025-05-12T17:39:28.171+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO NettyBlockTransferService: Server created on d4e9ca837c07:36533
[2025-05-12T17:39:28.172+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-05-12T17:39:28.183+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d4e9ca837c07, 36533, None)
[2025-05-12T17:39:28.187+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO BlockManagerMasterEndpoint: Registering block manager d4e9ca837c07:36533 with 434.4 MiB RAM, BlockManagerId(driver, d4e9ca837c07, 36533, None)
[2025-05-12T17:39:28.190+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d4e9ca837c07, 36533, None)
[2025-05-12T17:39:28.192+0000] {subprocess.py:93} INFO - 25/05/12 17:39:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d4e9ca837c07, 36533, None)
[2025-05-12T17:39:29.011+0000] {subprocess.py:93} INFO - 25/05/12 17:39:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-05-12T17:39:29.025+0000] {subprocess.py:93} INFO - 25/05/12 17:39:29 INFO SharedState: Warehouse path is 'file:/opt/spark-apps/spark-warehouse'.
[2025-05-12T17:39:38.133+0000] {subprocess.py:93} INFO - 25/05/12 17:39:38 INFO DefaultMavenCoordinates: DataStax Java driver for Apache Cassandra(R) (com.datastax.oss:java-driver-core-shaded) version 4.13.0
[2025-05-12T17:39:38.771+0000] {subprocess.py:93} INFO - 25/05/12 17:39:38 INFO Native: Unable to load JNR native implementation. This could be normal if JNR is excluded from the classpath
[2025-05-12T17:39:38.772+0000] {subprocess.py:93} INFO - java.lang.NoClassDefFoundError: jnr/posix/POSIXHandler
[2025-05-12T17:39:38.772+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native$LibcLoader.load(Native.java:42)
[2025-05-12T17:39:38.773+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native.<clinit>(Native.java:59)
[2025-05-12T17:39:38.774+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.Clock.getInstance(Clock.java:41)
[2025-05-12T17:39:38.774+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.buildClock(MonotonicTimestampGenerator.java:109)
[2025-05-12T17:39:38.775+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.<init>(MonotonicTimestampGenerator.java:43)
[2025-05-12T17:39:38.775+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.AtomicTimestampGenerator.<init>(AtomicTimestampGenerator.java:52)
[2025-05-12T17:39:38.776+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-12T17:39:38.776+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-12T17:39:38.777+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-12T17:39:38.777+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-12T17:39:38.778+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.resolveClass(Reflection.java:329)
[2025-05-12T17:39:38.779+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:235)
[2025-05-12T17:39:38.780+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:110)
[2025-05-12T17:39:38.783+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.buildTimestampGenerator(DefaultDriverContext.java:377)
[2025-05-12T17:39:38.784+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.concurrent.LazyReference.get(LazyReference.java:55)
[2025-05-12T17:39:38.785+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.getTimestampGenerator(DefaultDriverContext.java:773)
[2025-05-12T17:39:38.788+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.init(DefaultSession.java:349)
[2025-05-12T17:39:38.788+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.access$1100(DefaultSession.java:300)
[2025-05-12T17:39:38.789+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession.lambda$init$0(DefaultSession.java:146)
[2025-05-12T17:39:38.789+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
[2025-05-12T17:39:38.790+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
[2025-05-12T17:39:38.790+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)
[2025-05-12T17:39:38.791+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2025-05-12T17:39:38.791+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-05-12T17:39:38.792+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-05-12T17:39:38.792+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-12T17:39:38.793+0000] {subprocess.py:93} INFO - Caused by: java.lang.ClassNotFoundException: jnr.posix.POSIXHandler
[2025-05-12T17:39:38.793+0000] {subprocess.py:93} INFO - 	at java.base/java.net.URLClassLoader.findClass(Unknown Source)
[2025-05-12T17:39:38.793+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
[2025-05-12T17:39:38.794+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
[2025-05-12T17:39:38.794+0000] {subprocess.py:93} INFO - 	... 26 more
[2025-05-12T17:39:38.795+0000] {subprocess.py:93} INFO - 25/05/12 17:39:38 INFO Clock: Could not access native clock (see debug logs for details), falling back to Java system clock
[2025-05-12T17:39:42.278+0000] {subprocess.py:93} INFO - 25/05/12 17:39:42 INFO CassandraConnector: Connected to Cassandra cluster.
[2025-05-12T17:39:42.585+0000] {subprocess.py:93} INFO - 25/05/12 17:39:42 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-05-12T17:39:42.742+0000] {subprocess.py:93} INFO - 25/05/12 17:39:42 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint/cassandra resolved to file:/tmp/spark-checkpoint/cassandra.
[2025-05-12T17:39:42.743+0000] {subprocess.py:93} INFO - 25/05/12 17:39:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-05-12T17:39:43.125+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO MicroBatchExecution: Starting [id = 71460171-bd4d-4562-8003-9df0ff8d9d3e, runId = d7166bf3-68f6-41d4-a471-e50fb5f74d11]. Use file:/tmp/spark-checkpoint/cassandra to store the query checkpoint.
[2025-05-12T17:39:43.171+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3be00fb9] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5c76f474]
[2025-05-12T17:39:43.221+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9 resolved to file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9.
[2025-05-12T17:39:43.225+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-05-12T17:39:43.249+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 WARN MicroBatchExecution: The read limit MaxRows: 100 for KafkaV2[Subscribe[gold-news]] is ignored when Trigger.Once is used.
[2025-05-12T17:39:43.263+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/metadata using temp file file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/.metadata.18ac6aac-9f88-456e-b1c2-0f66e9b04bc7.tmp
[2025-05-12T17:39:43.272+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO OffsetSeqLog: BatchIds found from listing: 0
[2025-05-12T17:39:43.302+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO OffsetSeqLog: Getting latest batch 0
[2025-05-12T17:39:43.342+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/.metadata.18ac6aac-9f88-456e-b1c2-0f66e9b04bc7.tmp to file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/metadata
[2025-05-12T17:39:43.351+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO OffsetSeqLog: BatchIds found from listing: 0
[2025-05-12T17:39:43.352+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO OffsetSeqLog: Getting latest batch 0
[2025-05-12T17:39:43.364+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO CommitLog: BatchIds found from listing: 0
[2025-05-12T17:39:43.366+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO MicroBatchExecution: Starting [id = a00b2518-af6b-486f-8f08-f380ecf7f530, runId = 66fda5a3-a8a9-471c-a836-6e75f5ebabf3]. Use file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9 to store the query checkpoint.
[2025-05-12T17:39:43.366+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO CommitLog: Getting latest batch 0
[2025-05-12T17:39:43.369+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3be00fb9] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5c76f474]
[2025-05-12T17:39:43.372+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 WARN MicroBatchExecution: The read limit MaxRows: 100 for KafkaV2[Subscribe[gold-news]] is ignored when Trigger.Once is used.
[2025-05-12T17:39:43.378+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO OffsetSeqLog: BatchIds found from listing:
[2025-05-12T17:39:43.380+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO OffsetSeqLog: BatchIds found from listing:
[2025-05-12T17:39:43.381+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO MicroBatchExecution: Starting new streaming query.
[2025-05-12T17:39:43.381+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO MicroBatchExecution: Stream started from {}
[2025-05-12T17:39:43.382+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO MicroBatchExecution: Resuming at batch 1 with committed offsets {KafkaV2[Subscribe[gold-news]]: {"gold-news":{"0":30}}} and available offsets {KafkaV2[Subscribe[gold-news]]: {"gold-news":{"0":30}}}
[2025-05-12T17:39:43.383+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[gold-news]]: {"gold-news":{"0":30}}}
[2025-05-12T17:39:43.490+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO AdminClientConfig: AdminClientConfig values:
[2025-05-12T17:39:43.494+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:39:43.495+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:39:43.502+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:39:43.505+0000] {subprocess.py:93} INFO - 	client.id =
[2025-05-12T17:39:43.509+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-05-12T17:39:43.515+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:39:43.516+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:39:43.518+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:39:43.518+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:39:43.519+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:39:43.520+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:39:43.521+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:39:43.521+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:39:43.522+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:39:43.522+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:39:43.523+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-05-12T17:39:43.523+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:39:43.524+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:39:43.524+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:39:43.525+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:39:43.525+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:39:43.526+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:39:43.526+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:39:43.529+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:39:43.530+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:39:43.531+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:39:43.531+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:39:43.531+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:39:43.532+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:39:43.532+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:39:43.533+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:39:43.533+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:39:43.533+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:39:43.534+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:39:43.534+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:39:43.535+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:39:43.535+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:39:43.536+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:39:43.536+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:39:43.537+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:39:43.537+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:39:43.537+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:39:43.538+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:39:43.538+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:39:43.539+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:39:43.540+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:39:43.540+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:39:43.541+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:39:43.541+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:39:43.542+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:39:43.545+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:39:43.546+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:39:43.547+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:39:43.547+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:39:43.548+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:39:43.548+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:39:43.549+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:39:43.549+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:39:43.550+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:39:43.550+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:39:43.550+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:39:43.551+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:39:43.551+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:39:43.551+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:39:43.552+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:39:43.552+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:39:43.553+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:39:43.553+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:39:43.554+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:39:43.554+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:39:43.662+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-05-12T17:39:43.667+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:39:43.668+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:39:43.669+0000] {subprocess.py:93} INFO - 25/05/12 17:39:43 INFO AppInfoParser: Kafka startTimeMs: 1747071583660
[2025-05-12T17:39:44.023+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO AdminClientConfig: AdminClientConfig values:
[2025-05-12T17:39:44.023+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:39:44.024+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:39:44.025+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:39:44.026+0000] {subprocess.py:93} INFO - 	client.id =
[2025-05-12T17:39:44.026+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-05-12T17:39:44.027+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:39:44.028+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:39:44.029+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:39:44.029+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:39:44.032+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:39:44.035+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:39:44.035+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:39:44.036+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:39:44.036+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:39:44.037+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:39:44.037+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-05-12T17:39:44.038+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:39:44.038+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:39:44.039+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:39:44.039+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:39:44.040+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:39:44.041+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:39:44.041+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:39:44.042+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:39:44.042+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:39:44.043+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:39:44.043+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:39:44.043+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:39:44.044+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:39:44.044+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:39:44.045+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:39:44.045+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:39:44.046+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:39:44.047+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:39:44.048+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:39:44.049+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:39:44.050+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:39:44.050+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:39:44.050+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:39:44.051+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:39:44.051+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:39:44.052+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:39:44.052+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:39:44.052+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:39:44.053+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:39:44.053+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:39:44.053+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:39:44.054+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:39:44.054+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:39:44.055+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:39:44.055+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:39:44.056+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:39:44.056+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:39:44.057+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:39:44.057+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:39:44.057+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:39:44.058+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:39:44.058+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:39:44.059+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:39:44.059+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:39:44.060+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:39:44.060+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:39:44.061+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:39:44.062+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:39:44.062+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:39:44.063+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:39:44.064+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:39:44.064+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:39:44.065+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:39:44.065+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:39:44.066+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-05-12T17:39:44.066+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:39:44.067+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:39:44.068+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO AppInfoParser: Kafka startTimeMs: 1747071584026
[2025-05-12T17:39:44.525+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/sources/0/0 using temp file file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/sources/0/.0.92e58642-ebff-4024-86b8-02f0135666b3.tmp
[2025-05-12T17:39:44.539+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/cassandra/offsets/1 using temp file file:/tmp/spark-checkpoint/cassandra/offsets/.1.189a90fb-53f0-488d-8297-65cf977337c9.tmp
[2025-05-12T17:39:44.567+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/sources/0/.0.92e58642-ebff-4024-86b8-02f0135666b3.tmp to file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/sources/0/0
[2025-05-12T17:39:44.570+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO KafkaMicroBatchStream: Initial offsets: {"gold-news":{"0":0}}
[2025-05-12T17:39:44.585+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/cassandra/offsets/.1.189a90fb-53f0-488d-8297-65cf977337c9.tmp to file:/tmp/spark-checkpoint/cassandra/offsets/1
[2025-05-12T17:39:44.587+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1747071584522,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-05-12T17:39:44.594+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/offsets/0 using temp file file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/offsets/.0.b4165721-025d-45db-86ec-034834b6f322.tmp
[2025-05-12T17:39:44.631+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/offsets/.0.b4165721-025d-45db-86ec-034834b6f322.tmp to file:/tmp/spark-checkpoint/d6329c11-dd33-478d-934f-8cfaf36018b9/offsets/0
[2025-05-12T17:39:44.632+0000] {subprocess.py:93} INFO - 25/05/12 17:39:44 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1747071584581,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-05-12T17:39:45.565+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:45.566+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:45.703+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:45.717+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:45.861+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:45.863+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:45.867+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:45.868+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:45.984+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:45.994+0000] {subprocess.py:93} INFO - 25/05/12 17:39:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:39:46.834+0000] {subprocess.py:93} INFO - 25/05/12 17:39:46 INFO CodeGenerator: Code generated in 542.535347 ms
[2025-05-12T17:39:47.135+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@30bba1b7,com.datastax.spark.connector.cql.CassandraConnector@fb7e4a1,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@1a08963f)]. The input RDD has 1 partitions.
[2025-05-12T17:39:47.424+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO SparkContext: Starting job: start at <unknown>:0
[2025-05-12T17:39:47.437+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO CodeGenerator: Code generated in 30.596538 ms
[2025-05-12T17:39:47.490+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO SparkContext: Starting job: start at <unknown>:0
[2025-05-12T17:39:47.506+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-05-12T17:39:47.508+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-05-12T17:39:47.509+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:39:47.517+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:39:47.525+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at start at <unknown>:0), which has no missing parents
[2025-05-12T17:39:47.936+0000] {subprocess.py:93} INFO - 25/05/12 17:39:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KiB, free 434.4 MiB)
[2025-05-12T17:39:48.036+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 434.4 MiB)
[2025-05-12T17:39:48.041+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on d4e9ca837c07:36533 (size: 13.5 KiB, free: 434.4 MiB)
[2025-05-12T17:39:48.053+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:39:48.132+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:39:48.133+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-05-12T17:39:48.176+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-05-12T17:39:48.177+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-05-12T17:39:48.178+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:39:48.181+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:39:48.187+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-05-12T17:39:48.220+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 434.3 MiB)
[2025-05-12T17:39:48.242+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 15.7 KiB, free 434.3 MiB)
[2025-05-12T17:39:48.244+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on d4e9ca837c07:36533 (size: 15.7 KiB, free: 434.4 MiB)
[2025-05-12T17:39:48.248+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:39:48.250+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:39:48.251+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-05-12T17:39:48.270+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 13811 bytes)
[2025-05-12T17:39:48.289+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 13812 bytes)
[2025-05-12T17:39:48.297+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-05-12T17:39:48.316+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-05-12T17:39:48.644+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO CodeGenerator: Code generated in 79.491419 ms
[2025-05-12T17:39:48.731+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO CodeGenerator: Code generated in 34.175929 ms
[2025-05-12T17:39:48.753+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO CodeGenerator: Code generated in 15.526756 ms
[2025-05-12T17:39:48.760+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=0 untilOffset=40, for query queryId=a00b2518-af6b-486f-8f08-f380ecf7f530 batchId=0 taskId=1 partitionId=0
[2025-05-12T17:39:48.883+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO CodeGenerator: Code generated in 34.657549 ms
[2025-05-12T17:39:48.944+0000] {subprocess.py:93} INFO - 25/05/12 17:39:48 INFO CodeGenerator: Code generated in 43.749754 ms
[2025-05-12T17:39:49.013+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO ConsumerConfig: ConsumerConfig values:
[2025-05-12T17:39:49.015+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-05-12T17:39:49.016+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-05-12T17:39:49.018+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:39:49.019+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-05-12T17:39:49.021+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:39:49.029+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-05-12T17:39:49.030+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:39:49.031+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1
[2025-05-12T17:39:49.032+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-05-12T17:39:49.035+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-05-12T17:39:49.043+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:39:49.044+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-05-12T17:39:49.048+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-05-12T17:39:49.050+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-05-12T17:39:49.051+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-05-12T17:39:49.055+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-05-12T17:39:49.057+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor
[2025-05-12T17:39:49.064+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-05-12T17:39:49.065+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-05-12T17:39:49.065+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-05-12T17:39:49.066+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-05-12T17:39:49.067+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-05-12T17:39:49.070+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-05-12T17:39:49.072+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:39:49.073+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-05-12T17:39:49.074+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-05-12T17:39:49.075+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-05-12T17:39:49.076+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:39:49.077+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:39:49.078+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:39:49.079+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:39:49.079+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:39:49.080+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-05-12T17:39:49.085+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:39:49.087+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:39:49.087+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:39:49.088+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:39:49.089+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:39:49.090+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:39:49.091+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:39:49.091+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:39:49.092+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:39:49.093+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:39:49.094+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:39:49.094+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:39:49.095+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:39:49.095+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:39:49.096+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:39:49.096+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:39:49.097+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:39:49.097+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:39:49.101+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:39:49.101+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:39:49.102+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:39:49.102+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:39:49.103+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:39:49.103+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:39:49.103+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:39:49.104+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:39:49.104+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:39:49.104+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:39:49.105+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:39:49.105+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:39:49.106+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:39:49.106+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:39:49.107+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:39:49.107+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:39:49.107+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:39:49.108+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:39:49.108+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-05-12T17:39:49.109+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:39:49.109+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:39:49.110+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:39:49.110+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:39:49.111+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:39:49.111+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:39:49.112+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:39:49.112+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:39:49.113+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:39:49.115+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:39:49.117+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:39:49.118+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:39:49.118+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:39:49.119+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:39:49.120+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:39:49.120+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:39:49.121+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:39:49.121+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:39:49.121+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:39:49.122+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:39:49.122+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:39:49.123+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:39:49.124+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:39:49.157+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:39:49.158+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:39:49.161+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO AppInfoParser: Kafka startTimeMs: 1747071589154
[2025-05-12T17:39:49.164+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Assigned to partition(s): gold-news-0
[2025-05-12T17:39:49.189+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Seeking to offset 0 for partition gold-news-0
[2025-05-12T17:39:49.217+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Resetting the last seen epoch of partition gold-news-0 to 0 since the associated topicId changed from null to EyFovrTXQQOm3GPuq8IGNA
[2025-05-12T17:39:49.218+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Cluster ID: HTSY0p8VS7eCaEFoyzBH3g
[2025-05-12T17:39:49.322+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T17:39:49.326+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=30 untilOffset=40, for query queryId=71460171-bd4d-4562-8003-9df0ff8d9d3e batchId=1 taskId=0 partitionId=0
[2025-05-12T17:39:49.355+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO ConsumerConfig: ConsumerConfig values:
[2025-05-12T17:39:49.356+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-05-12T17:39:49.358+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-05-12T17:39:49.359+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:39:49.360+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-05-12T17:39:49.361+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:39:49.362+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-05-12T17:39:49.362+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:39:49.363+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2
[2025-05-12T17:39:49.364+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-05-12T17:39:49.365+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-05-12T17:39:49.365+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:39:49.366+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-05-12T17:39:49.366+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-05-12T17:39:49.367+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-05-12T17:39:49.367+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-05-12T17:39:49.368+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-05-12T17:39:49.369+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor
[2025-05-12T17:39:49.370+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-05-12T17:39:49.371+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-05-12T17:39:49.374+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-05-12T17:39:49.376+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-05-12T17:39:49.377+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-05-12T17:39:49.378+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-05-12T17:39:49.379+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:39:49.380+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-05-12T17:39:49.380+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-05-12T17:39:49.381+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-05-12T17:39:49.381+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:39:49.382+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:39:49.383+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:39:49.383+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:39:49.384+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:39:49.384+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-05-12T17:39:49.385+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:39:49.385+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:39:49.386+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:39:49.387+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:39:49.387+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:39:49.389+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:39:49.392+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:39:49.393+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:39:49.393+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:39:49.394+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:39:49.395+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:39:49.396+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:39:49.397+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:39:49.397+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:39:49.398+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:39:49.399+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:39:49.400+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:39:49.401+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:39:49.401+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:39:49.402+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:39:49.403+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:39:49.404+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:39:49.406+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:39:49.407+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:39:49.408+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:39:49.408+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:39:49.409+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:39:49.409+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:39:49.410+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:39:49.410+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:39:49.412+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:39:49.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:39:49.414+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:39:49.415+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:39:49.416+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:39:49.417+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:39:49.422+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-05-12T17:39:49.427+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:39:49.428+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:39:49.429+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:39:49.429+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:39:49.430+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:39:49.431+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:39:49.432+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:39:49.432+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:39:49.433+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:39:49.433+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:39:49.439+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:39:49.441+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:39:49.442+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:39:49.443+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:39:49.443+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:39:49.444+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:39:49.444+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:39:49.445+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:39:49.445+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:39:49.445+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:39:49.446+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:39:49.446+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:39:49.446+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:39:49.447+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:39:49.447+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:39:49.448+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO AppInfoParser: Kafka startTimeMs: 1747071589418
[2025-05-12T17:39:49.454+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Assigned to partition(s): gold-news-0
[2025-05-12T17:39:49.460+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Seeking to offset 30 for partition gold-news-0
[2025-05-12T17:39:49.471+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Resetting the last seen epoch of partition gold-news-0 to 0 since the associated topicId changed from null to EyFovrTXQQOm3GPuq8IGNA
[2025-05-12T17:39:49.472+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Cluster ID: HTSY0p8VS7eCaEFoyzBH3g
[2025-05-12T17:39:49.519+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T17:39:49.829+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:39:49.835+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T17:39:49.847+0000] {subprocess.py:93} INFO - 25/05/12 17:39:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=40, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:39:50.024+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:39:50.028+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T17:39:50.033+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=40, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:39:50.167+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor read 1 records through 1 polls (polled  out 40 records), taking 652197983 nanos, during time span of 990259513 nanos.
[2025-05-12T17:39:50.219+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1957 bytes result sent to driver
[2025-05-12T17:39:50.249+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1957 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:39:50.264+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-05-12T17:39:50.317+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-05-12T17:39:50.345+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 2.152 s
[2025-05-12T17:39:50.359+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:39:50.395+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-05-12T17:39:50.401+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 2.901349 s
[2025-05-12T17:39:50.532+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-05-12T17:39:50.536+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor read 10 records through 1 polls (polled  out 10 records), taking 578078814 nanos, during time span of 1077541921 nanos.
[2025-05-12T17:39:50.586+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1851 bytes result sent to driver
[2025-05-12T17:39:50.587+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2361 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:39:50.588+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-05-12T17:39:50.591+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 2.958 s
[2025-05-12T17:39:50.592+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:39:50.593+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-05-12T17:39:50.597+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 3.173956 s
[2025-05-12T17:39:50.598+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@30bba1b7,com.datastax.spark.connector.cql.CassandraConnector@fb7e4a1,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@1a08963f)] is committing.
[2025-05-12T17:39:50.601+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@30bba1b7,com.datastax.spark.connector.cql.CassandraConnector@fb7e4a1,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@1a08963f)] committed.
[2025-05-12T17:39:50.631+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/cassandra/commits/1 using temp file file:/tmp/spark-checkpoint/cassandra/commits/.1.00deba04-9f88-4be0-8f58-e2ad938cb131.tmp
[2025-05-12T17:39:50.659+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO CodeGenerator: Code generated in 37.349366 ms
[2025-05-12T17:39:50.755+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T17:39:50.758+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Got job 2 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T17:39:50.759+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Final stage: ResultStage 2 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:39:50.759+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:39:50.760+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:39:50.763+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:39:50.770+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 42.3 KiB, free 434.3 MiB)
[2025-05-12T17:39:50.786+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 434.2 MiB)
[2025-05-12T17:39:50.789+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on d4e9ca837c07:36533 (size: 16.1 KiB, free: 434.4 MiB)
[2025-05-12T17:39:50.791+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:39:50.793+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/cassandra/commits/.1.00deba04-9f88-4be0-8f58-e2ad938cb131.tmp to file:/tmp/spark-checkpoint/cassandra/commits/1
[2025-05-12T17:39:50.795+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:39:50.796+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-05-12T17:39:50.801+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 13812 bytes)
[2025-05-12T17:39:50.805+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-05-12T17:39:50.822+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on d4e9ca837c07:36533 in memory (size: 15.7 KiB, free: 434.4 MiB)
[2025-05-12T17:39:50.872+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO CodeGenerator: Code generated in 16.2754 ms
[2025-05-12T17:39:50.876+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=0 untilOffset=40, for query queryId=a00b2518-af6b-486f-8f08-f380ecf7f530 batchId=0 taskId=2 partitionId=0
[2025-05-12T17:39:50.895+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO MicroBatchExecution: Streaming query made progress: {
[2025-05-12T17:39:50.896+0000] {subprocess.py:93} INFO -   "id" : "71460171-bd4d-4562-8003-9df0ff8d9d3e",
[2025-05-12T17:39:50.897+0000] {subprocess.py:93} INFO -   "runId" : "d7166bf3-68f6-41d4-a471-e50fb5f74d11",
[2025-05-12T17:39:50.898+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-05-12T17:39:50.898+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-05-12T17:39:43.254Z",
[2025-05-12T17:39:50.899+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-05-12T17:39:50.900+0000] {subprocess.py:93} INFO -   "numInputRows" : 10,
[2025-05-12T17:39:50.901+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-05-12T17:39:50.903+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.3259082471492973,
[2025-05-12T17:39:50.903+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-05-12T17:39:50.904+0000] {subprocess.py:93} INFO -     "addBatch" : 4821,
[2025-05-12T17:39:50.905+0000] {subprocess.py:93} INFO -     "commitOffsets" : 181,
[2025-05-12T17:39:50.906+0000] {subprocess.py:93} INFO -     "getBatch" : 5,
[2025-05-12T17:39:50.906+0000] {subprocess.py:93} INFO -     "latestOffset" : 1136,
[2025-05-12T17:39:50.907+0000] {subprocess.py:93} INFO -     "queryPlanning" : 1171,
[2025-05-12T17:39:50.909+0000] {subprocess.py:93} INFO -     "triggerExecution" : 7538,
[2025-05-12T17:39:50.909+0000] {subprocess.py:93} INFO -     "walCommit" : 62
[2025-05-12T17:39:50.910+0000] {subprocess.py:93} INFO -   },
[2025-05-12T17:39:50.911+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-05-12T17:39:50.912+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-05-12T17:39:50.913+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[gold-news]]",
[2025-05-12T17:39:50.913+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-05-12T17:39:50.913+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T17:39:50.914+0000] {subprocess.py:93} INFO -         "0" : 30
[2025-05-12T17:39:50.914+0000] {subprocess.py:93} INFO -       }
[2025-05-12T17:39:50.915+0000] {subprocess.py:93} INFO -     },
[2025-05-12T17:39:50.916+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-05-12T17:39:50.916+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T17:39:50.917+0000] {subprocess.py:93} INFO -         "0" : 40
[2025-05-12T17:39:50.917+0000] {subprocess.py:93} INFO -       }
[2025-05-12T17:39:50.918+0000] {subprocess.py:93} INFO -     },
[2025-05-12T17:39:50.919+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-05-12T17:39:50.919+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T17:39:50.920+0000] {subprocess.py:93} INFO -         "0" : 40
[2025-05-12T17:39:50.920+0000] {subprocess.py:93} INFO -       }
[2025-05-12T17:39:50.921+0000] {subprocess.py:93} INFO -     },
[2025-05-12T17:39:50.922+0000] {subprocess.py:93} INFO -     "numInputRows" : 10,
[2025-05-12T17:39:50.922+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-05-12T17:39:50.923+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.3259082471492973,
[2025-05-12T17:39:50.923+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-05-12T17:39:50.923+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-05-12T17:39:50.924+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-05-12T17:39:50.925+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-05-12T17:39:50.925+0000] {subprocess.py:93} INFO -     }
[2025-05-12T17:39:50.925+0000] {subprocess.py:93} INFO -   } ],
[2025-05-12T17:39:50.926+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-05-12T17:39:50.926+0000] {subprocess.py:93} INFO -     "description" : "CassandraTable(org.apache.spark.sql.SparkSession@30bba1b7,org.apache.spark.sql.util.CaseInsensitiveStringMap@6e619b4c,com.datastax.spark.connector.cql.CassandraConnector@10de9964,default,DefaultTableMetadata@88aa72be(gold_news.articles),None)",
[2025-05-12T17:39:50.927+0000] {subprocess.py:93} INFO -     "numOutputRows" : 10
[2025-05-12T17:39:50.927+0000] {subprocess.py:93} INFO -   }
[2025-05-12T17:39:50.928+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:39:50.928+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2025-05-12T17:39:50.929+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Seeking to offset 0 for partition gold-news-0
[2025-05-12T17:39:50.930+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:39:50.931+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:39:50.931+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO Metrics: Metrics reporters closed
[2025-05-12T17:39:50.932+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO MicroBatchExecution: Async log purge executor pool for query [id = 71460171-bd4d-4562-8003-9df0ff8d9d3e, runId = d7166bf3-68f6-41d4-a471-e50fb5f74d11] has been shutdown
[2025-05-12T17:39:50.932+0000] {subprocess.py:93} INFO - 25/05/12 17:39:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T17:39:51.425+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:39:51.426+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T17:39:51.433+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=40, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:39:51.476+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor read 40 records through 1 polls (polled  out 40 records), taking 526187080 nanos, during time span of 577245808 nanos.
[2025-05-12T17:39:51.482+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 7662 bytes result sent to driver
[2025-05-12T17:39:51.486+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 686 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:39:51.488+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO DAGScheduler: ResultStage 2 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.722 s
[2025-05-12T17:39:51.490+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:39:51.491+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-05-12T17:39:51.491+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-05-12T17:39:51.492+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO DAGScheduler: Job 2 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.734710 s
[2025-05-12T17:39:51.549+0000] {subprocess.py:93} INFO - Traitement du batch 0 avec 40 descriptions
[2025-05-12T17:39:51.583+0000] {subprocess.py:93} INFO - 25/05/12 17:39:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on d4e9ca837c07:36533 in memory (size: 13.5 KiB, free: 434.4 MiB)
[2025-05-12T17:40:02.563+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.564+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:40:02.564+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.565+0000] {subprocess.py:93} INFO -   "description_number": "1",
[2025-05-12T17:40:02.565+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.566+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase is driven by easing tariff tensions and potential US-China agreements. This could reduce safe-haven demand for gold, but it's not a direct impact.",
[2025-05-12T17:40:02.566+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.567+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.567+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.568+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:40:02.568+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.568+0000] {subprocess.py:93} INFO -   "description_number": "2",
[2025-05-12T17:40:02.569+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.569+0000] {subprocess.py:93} INFO -   "impact_explanation": "The Google settlement is a domestic US legal issue with little to no direct effect on international gold markets.",
[2025-05-12T17:40:02.569+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.570+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.570+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.571+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:40:02.571+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.571+0000] {subprocess.py:93} INFO -   "description_number": "3",
[2025-05-12T17:40:02.571+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:40:02.572+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold's role as a safe haven asset during market volatility, suggesting increased demand and price.",
[2025-05-12T17:40:02.572+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:40:02.572+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.573+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.573+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:40:02.573+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.573+0000] {subprocess.py:93} INFO -   "description_number": "4",
[2025-05-12T17:40:02.574+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.574+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about SEO and has no relevance to the gold market.",
[2025-05-12T17:40:02.574+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.574+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.575+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.575+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:40:02.575+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.576+0000] {subprocess.py:93} INFO -   "description_number": "5",
[2025-05-12T17:40:02.576+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:40:02.577+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar typically increases the price of dollar-denominated gold, making it more attractive to foreign investors.",
[2025-05-12T17:40:02.578+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:40:02.578+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.579+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.579+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:40:02.580+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.580+0000] {subprocess.py:93} INFO -   "description_number": "6",
[2025-05-12T17:40:02.580+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.581+0000] {subprocess.py:93} INFO -   "impact_explanation": "This focuses on political finance and doesn't have a direct effect on gold prices.",
[2025-05-12T17:40:02.581+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.581+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.582+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.582+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:40:02.583+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.583+0000] {subprocess.py:93} INFO -   "description_number": "7",
[2025-05-12T17:40:02.583+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.584+0000] {subprocess.py:93} INFO -   "impact_explanation": "Discussion of Web3 investing has no direct bearing on the gold market.",
[2025-05-12T17:40:02.584+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.584+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.585+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.585+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:40:02.585+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.585+0000] {subprocess.py:93} INFO -   "description_number": "8",
[2025-05-12T17:40:02.586+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.586+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about cybersecurity and unrelated to gold.",
[2025-05-12T17:40:02.586+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.587+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.587+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.587+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:40:02.588+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.588+0000] {subprocess.py:93} INFO -   "description_number": "9",
[2025-05-12T17:40:02.588+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.589+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about tourism and is irrelevant to gold prices.",
[2025-05-12T17:40:02.589+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.590+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.590+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.590+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:40:02.591+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.592+0000] {subprocess.py:93} INFO -   "description_number": "10",
[2025-05-12T17:40:02.592+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.594+0000] {subprocess.py:93} INFO -   "impact_explanation": "While related to finance, XRP ETF news has no direct, immediate impact on gold.",
[2025-05-12T17:40:02.594+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.595+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.595+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.596+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:40:02.596+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.597+0000] {subprocess.py:93} INFO -   "description_number": "11",
[2025-05-12T17:40:02.597+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.598+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 1.",
[2025-05-12T17:40:02.598+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.599+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.599+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.600+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:40:02.600+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.600+0000] {subprocess.py:93} INFO -   "description_number": "12",
[2025-05-12T17:40:02.601+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.601+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 2.",
[2025-05-12T17:40:02.601+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.602+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.602+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.602+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:40:02.603+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.603+0000] {subprocess.py:93} INFO -   "description_number": "13",
[2025-05-12T17:40:02.603+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:40:02.604+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 3.",
[2025-05-12T17:40:02.604+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:40:02.605+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.605+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.605+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:40:02.606+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.607+0000] {subprocess.py:93} INFO -   "description_number": "14",
[2025-05-12T17:40:02.607+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.608+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 4.",
[2025-05-12T17:40:02.609+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.610+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.610+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.611+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:40:02.612+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.612+0000] {subprocess.py:93} INFO -   "description_number": "15",
[2025-05-12T17:40:02.613+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:40:02.613+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 5.",
[2025-05-12T17:40:02.614+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:40:02.614+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.614+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.615+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:40:02.615+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.616+0000] {subprocess.py:93} INFO -   "description_number": "16",
[2025-05-12T17:40:02.616+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.617+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 6.",
[2025-05-12T17:40:02.617+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.618+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.619+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.619+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:40:02.620+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.620+0000] {subprocess.py:93} INFO -   "description_number": "17",
[2025-05-12T17:40:02.620+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.621+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 7.",
[2025-05-12T17:40:02.621+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.622+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.622+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.622+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:40:02.623+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.624+0000] {subprocess.py:93} INFO -   "description_number": "18",
[2025-05-12T17:40:02.625+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.625+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 8.",
[2025-05-12T17:40:02.626+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.626+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.627+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.627+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:40:02.628+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.628+0000] {subprocess.py:93} INFO -   "description_number": "19",
[2025-05-12T17:40:02.629+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.630+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 9.",
[2025-05-12T17:40:02.630+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.631+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.631+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.632+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:40:02.632+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.632+0000] {subprocess.py:93} INFO -   "description_number": "20",
[2025-05-12T17:40:02.633+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.633+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 10.",
[2025-05-12T17:40:02.634+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.634+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.634+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.635+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:40:02.635+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.636+0000] {subprocess.py:93} INFO -   "description_number": "21",
[2025-05-12T17:40:02.636+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.636+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 1.",
[2025-05-12T17:40:02.637+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.637+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.638+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.639+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:40:02.640+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.641+0000] {subprocess.py:93} INFO -   "description_number": "22",
[2025-05-12T17:40:02.641+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.642+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 2.",
[2025-05-12T17:40:02.642+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.643+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.643+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.644+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:40:02.644+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.644+0000] {subprocess.py:93} INFO -   "description_number": "23",
[2025-05-12T17:40:02.645+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:40:02.645+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 3.",
[2025-05-12T17:40:02.646+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:40:02.646+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.647+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.647+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:40:02.648+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.648+0000] {subprocess.py:93} INFO -   "description_number": "24",
[2025-05-12T17:40:02.649+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.649+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 4.",
[2025-05-12T17:40:02.650+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.651+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.652+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.652+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:40:02.653+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.654+0000] {subprocess.py:93} INFO -   "description_number": "25",
[2025-05-12T17:40:02.655+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:40:02.656+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 5.",
[2025-05-12T17:40:02.656+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:40:02.657+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.658+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.658+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:40:02.659+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.659+0000] {subprocess.py:93} INFO -   "description_number": "26",
[2025-05-12T17:40:02.660+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.661+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 6.",
[2025-05-12T17:40:02.661+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.662+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.663+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.663+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:40:02.664+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.664+0000] {subprocess.py:93} INFO -   "description_number": "27",
[2025-05-12T17:40:02.665+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.666+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 7.",
[2025-05-12T17:40:02.666+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.667+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.667+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.668+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:40:02.669+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.670+0000] {subprocess.py:93} INFO -   "description_number": "28",
[2025-05-12T17:40:02.671+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.672+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 8.",
[2025-05-12T17:40:02.672+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.673+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.674+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.674+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:40:02.675+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.676+0000] {subprocess.py:93} INFO -   "description_number": "29",
[2025-05-12T17:40:02.676+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.677+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 9.",
[2025-05-12T17:40:02.677+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.678+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.678+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.679+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:40:02.679+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.680+0000] {subprocess.py:93} INFO -   "description_number": "30",
[2025-05-12T17:40:02.680+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.681+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 10.",
[2025-05-12T17:40:02.681+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.682+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.683+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.685+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:40:02.686+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.687+0000] {subprocess.py:93} INFO -   "description_number": "31",
[2025-05-12T17:40:02.687+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.688+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 1.",
[2025-05-12T17:40:02.688+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.689+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.689+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.690+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:40:02.690+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.691+0000] {subprocess.py:93} INFO -   "description_number": "32",
[2025-05-12T17:40:02.691+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.692+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 2.",
[2025-05-12T17:40:02.692+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.693+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.693+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.693+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:40:02.694+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.694+0000] {subprocess.py:93} INFO -   "description_number": "33",
[2025-05-12T17:40:02.695+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:40:02.695+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 3.",
[2025-05-12T17:40:02.696+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:40:02.697+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.697+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.698+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:40:02.698+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.699+0000] {subprocess.py:93} INFO -   "description_number": "34",
[2025-05-12T17:40:02.699+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.700+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 4.",
[2025-05-12T17:40:02.701+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.702+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.702+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.703+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:40:02.703+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.704+0000] {subprocess.py:93} INFO -   "description_number": "35",
[2025-05-12T17:40:02.704+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:40:02.705+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 5.",
[2025-05-12T17:40:02.705+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:40:02.706+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.706+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.707+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:40:02.707+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.708+0000] {subprocess.py:93} INFO -   "description_number": "36",
[2025-05-12T17:40:02.708+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.709+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 6.",
[2025-05-12T17:40:02.709+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.710+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.710+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.711+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:40:02.711+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.712+0000] {subprocess.py:93} INFO -   "description_number": "37",
[2025-05-12T17:40:02.712+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.713+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 7.",
[2025-05-12T17:40:02.714+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.715+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.716+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.717+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:40:02.718+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.718+0000] {subprocess.py:93} INFO -   "description_number": "38",
[2025-05-12T17:40:02.719+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.719+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 8.",
[2025-05-12T17:40:02.720+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.720+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.721+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.721+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:40:02.722+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.722+0000] {subprocess.py:93} INFO -   "description_number": "39",
[2025-05-12T17:40:02.723+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.723+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 9.",
[2025-05-12T17:40:02.724+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.724+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.725+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:02.725+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:40:02.726+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:40:02.726+0000] {subprocess.py:93} INFO -   "description_number": "40",
[2025-05-12T17:40:02.727+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:40:02.727+0000] {subprocess.py:93} INFO -   "impact_explanation": "Same as description 10.",
[2025-05-12T17:40:02.728+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:40:02.728+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:40:02.834+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO CodeGenerator: Code generated in 20.080796 ms
[2025-05-12T17:40:02.846+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO AppendDataExec: Start processing data source write support: CassandraBulkWrite(org.apache.spark.sql.SparkSession@30bba1b7,com.datastax.spark.connector.cql.CassandraConnector@7179dd06,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(description,StringType,true),StructField(id,StringType,true),StructField(ingestion_time,StringType,true),StructField(published_at,StringType,true),StructField(recommendation,StringType,true),StructField(sentiment,StringType,true),StructField(source,StringType,true),StructField(title,StringType,true),StructField(url,StringType,true)),org.apache.spark.SparkConf@bbf8bc6). The input RDD has 8 partitions.
[2025-05-12T17:40:02.850+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO SparkContext: Starting job: save at <unknown>:0
[2025-05-12T17:40:02.854+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO DAGScheduler: Got job 3 (save at <unknown>:0) with 8 output partitions
[2025-05-12T17:40:02.855+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO DAGScheduler: Final stage: ResultStage 3 (save at <unknown>:0)
[2025-05-12T17:40:02.856+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:40:02.859+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:40:02.861+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at save at <unknown>:0), which has no missing parents
[2025-05-12T17:40:02.877+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.8 KiB, free 434.3 MiB)
[2025-05-12T17:40:02.901+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 434.3 MiB)
[2025-05-12T17:40:02.902+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on d4e9ca837c07:36533 (size: 10.7 KiB, free: 434.4 MiB)
[2025-05-12T17:40:02.903+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:40:02.904+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at save at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T17:40:02.905+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO TaskSchedulerImpl: Adding task set 3.0 with 8 tasks resource profile 0
[2025-05-12T17:40:02.906+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on d4e9ca837c07:36533 in memory (size: 16.1 KiB, free: 434.4 MiB)
[2025-05-12T17:40:02.910+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 15438 bytes)
[2025-05-12T17:40:02.915+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (d4e9ca837c07, executor driver, partition 1, PROCESS_LOCAL, 15096 bytes)
[2025-05-12T17:40:02.917+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5) (d4e9ca837c07, executor driver, partition 2, PROCESS_LOCAL, 15438 bytes)
[2025-05-12T17:40:02.918+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 6) (d4e9ca837c07, executor driver, partition 3, PROCESS_LOCAL, 15096 bytes)
[2025-05-12T17:40:02.919+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 7) (d4e9ca837c07, executor driver, partition 4, PROCESS_LOCAL, 15438 bytes)
[2025-05-12T17:40:02.920+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 8) (d4e9ca837c07, executor driver, partition 5, PROCESS_LOCAL, 15096 bytes)
[2025-05-12T17:40:02.921+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 9) (d4e9ca837c07, executor driver, partition 6, PROCESS_LOCAL, 15438 bytes)
[2025-05-12T17:40:02.922+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 10) (d4e9ca837c07, executor driver, partition 7, PROCESS_LOCAL, 15096 bytes)
[2025-05-12T17:40:02.923+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
[2025-05-12T17:40:02.923+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-05-12T17:40:02.938+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO Executor: Running task 2.0 in stage 3.0 (TID 5)
[2025-05-12T17:40:02.940+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO Executor: Running task 4.0 in stage 3.0 (TID 7)
[2025-05-12T17:40:02.946+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO Executor: Running task 3.0 in stage 3.0 (TID 6)
[2025-05-12T17:40:02.952+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO Executor: Running task 5.0 in stage 3.0 (TID 8)
[2025-05-12T17:40:02.964+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO Executor: Running task 6.0 in stage 3.0 (TID 9)
[2025-05-12T17:40:02.970+0000] {subprocess.py:93} INFO - 25/05/12 17:40:02 INFO Executor: Running task 7.0 in stage 3.0 (TID 10)
[2025-05-12T17:40:03.711+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO CodeGenerator: Code generated in 38.182733 ms
[2025-05-12T17:40:03.807+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO PythonRunner: Times: total = 673, boot = 619, init = 54, finish = 0
[2025-05-12T17:40:03.808+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO PythonRunner: Times: total = 673, boot = 598, init = 75, finish = 0
[2025-05-12T17:40:03.810+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO DataWritingSparkTask: Commit authorized for partition 4 (task 7, attempt 0, stage 3.0)
[2025-05-12T17:40:03.810+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO DataWritingSparkTask: Commit authorized for partition 5 (task 8, attempt 0, stage 3.0)
[2025-05-12T17:40:03.818+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO PythonRunner: Times: total = 681, boot = 626, init = 55, finish = 0
[2025-05-12T17:40:03.823+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO DataWritingSparkTask: Commit authorized for partition 3 (task 6, attempt 0, stage 3.0)
[2025-05-12T17:40:03.827+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO PythonRunner: Times: total = 694, boot = 624, init = 69, finish = 1
[2025-05-12T17:40:03.842+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO DataWritingSparkTask: Commit authorized for partition 1 (task 4, attempt 0, stage 3.0)
[2025-05-12T17:40:03.847+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO DataWritingSparkTask: Committed partition 3 (task 6, attempt 0, stage 3.0)
[2025-05-12T17:40:03.848+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO DataWritingSparkTask: Committed partition 4 (task 7, attempt 0, stage 3.0)
[2025-05-12T17:40:06.604+0000] {subprocess.py:93} INFO - 25/05/12 17:40:03 INFO PythonRunner: Times: total = 661, boot = 599, init = 62, finish = 0
[2025-05-12T17:40:06.605+0000] {subprocess.py:93} INFO - 25/05/12 16:40:08 INFO DataWritingSparkTask: Committed partition 5 (task 8, attempt 0, stage 3.0)
[2025-05-12T17:40:06.606+0000] {subprocess.py:93} INFO - 25/05/12 16:40:08 INFO DataWritingSparkTask: Commit authorized for partition 7 (task 10, attempt 0, stage 3.0)
[2025-05-12T17:40:06.606+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 WARN MonotonicTimestampGenerator: Clock skew detected: current tick (1747068008061000) was 3595808000 microseconds behind the last generated timestamp (1747071603869000), returned timestamps will be artificially incremented to guarantee monotonicity.
[2025-05-12T17:40:06.620+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO PythonRunner: Times: total = 702, boot = 630, init = 71, finish = 1
[2025-05-12T17:40:06.621+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO Executor: Finished task 4.0 in stage 3.0 (TID 7). 1939 bytes result sent to driver
[2025-05-12T17:40:06.622+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO PythonRunner: Times: total = 715, boot = 650, init = 63, finish = 2
[2025-05-12T17:40:06.624+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO Executor: Finished task 5.0 in stage 3.0 (TID 8). 1939 bytes result sent to driver
[2025-05-12T17:40:06.626+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DataWritingSparkTask: Commit authorized for partition 6 (task 9, attempt 0, stage 3.0)
[2025-05-12T17:40:06.627+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-05-12T17:40:06.637+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DataWritingSparkTask: Committed partition 7 (task 10, attempt 0, stage 3.0)
[2025-05-12T17:40:06.639+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DataWritingSparkTask: Committed partition 1 (task 4, attempt 0, stage 3.0)
[2025-05-12T17:40:06.640+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO Executor: Finished task 7.0 in stage 3.0 (TID 10). 1896 bytes result sent to driver
[2025-05-12T17:40:06.643+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 1896 bytes result sent to driver
[2025-05-12T17:40:06.645+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 8) in 3725 ms on d4e9ca837c07 (executor driver) (1/8)
[2025-05-12T17:40:06.648+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 10) in 3724 ms on d4e9ca837c07 (executor driver) (2/8)
[2025-05-12T17:40:06.651+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51439
[2025-05-12T17:40:06.668+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 7) in 3739 ms on d4e9ca837c07 (executor driver) (3/8)
[2025-05-12T17:40:06.670+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 3758 ms on d4e9ca837c07 (executor driver) (4/8)
[2025-05-12T17:40:06.671+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO Executor: Finished task 3.0 in stage 3.0 (TID 6). 1939 bytes result sent to driver
[2025-05-12T17:40:06.672+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 6) in 3754 ms on d4e9ca837c07 (executor driver) (5/8)
[2025-05-12T17:40:06.683+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO PythonRunner: Times: total = 662, boot = 590, init = 71, finish = 1
[2025-05-12T17:40:06.684+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DataWritingSparkTask: Committed partition 6 (task 9, attempt 0, stage 3.0)
[2025-05-12T17:40:06.685+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DataWritingSparkTask: Commit authorized for partition 2 (task 5, attempt 0, stage 3.0)
[2025-05-12T17:40:06.698+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO Executor: Finished task 6.0 in stage 3.0 (TID 9). 1939 bytes result sent to driver
[2025-05-12T17:40:06.701+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 9) in 3780 ms on d4e9ca837c07 (executor driver) (6/8)
[2025-05-12T17:40:06.706+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-05-12T17:40:06.743+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DataWritingSparkTask: Committed partition 2 (task 5, attempt 0, stage 3.0)
[2025-05-12T17:40:06.744+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1896 bytes result sent to driver
[2025-05-12T17:40:06.745+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO Executor: Finished task 2.0 in stage 3.0 (TID 5). 1896 bytes result sent to driver
[2025-05-12T17:40:06.746+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 3830 ms on d4e9ca837c07 (executor driver) (7/8)
[2025-05-12T17:40:06.747+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 3838 ms on d4e9ca837c07 (executor driver) (8/8)
[2025-05-12T17:40:06.747+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-05-12T17:40:06.750+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DAGScheduler: ResultStage 3 (save at <unknown>:0) finished in 3.886 s
[2025-05-12T17:40:06.751+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:40:06.752+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-05-12T17:40:06.752+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO DAGScheduler: Job 3 finished: save at <unknown>:0, took 1.174683 s
[2025-05-12T17:40:06.758+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@30bba1b7,com.datastax.spark.connector.cql.CassandraConnector@7179dd06,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(description,StringType,true),StructField(id,StringType,true),StructField(ingestion_time,StringType,true),StructField(published_at,StringType,true),StructField(recommendation,StringType,true),StructField(sentiment,StringType,true),StructField(source,StringType,true),StructField(title,StringType,true),StructField(url,StringType,true)),org.apache.spark.SparkConf@bbf8bc6) is committing.
[2025-05-12T17:40:06.759+0000] {subprocess.py:93} INFO - 25/05/12 17:40:06 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@30bba1b7,com.datastax.spark.connector.cql.CassandraConnector@7179dd06,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(description,StringType,true),StructField(id,StringType,true),StructField(ingestion_time,StringType,true),StructField(published_at,StringType,true),StructField(recommendation,StringType,true),StructField(sentiment,StringType,true),StructField(source,StringType,true),StructField(title,StringType,true),StructField(url,StringType,true)),org.apache.spark.SparkConf@bbf8bc6) committed.
[2025-05-12T17:40:07.238+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO CodeGenerator: Code generated in 49.930765 ms
[2025-05-12T17:40:07.283+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: Registering RDD 20 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) as input to shuffle 0
[2025-05-12T17:40:07.292+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: Got map stage job 4 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 8 output partitions
[2025-05-12T17:40:07.293+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:40:07.294+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:40:07.297+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:40:07.300+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:40:07.319+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 25.9 KiB, free 434.3 MiB)
[2025-05-12T17:40:07.332+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 434.3 MiB)
[2025-05-12T17:40:07.335+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on d4e9ca837c07:36533 (size: 11.6 KiB, free: 434.4 MiB)
[2025-05-12T17:40:07.338+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO BlockManagerInfo: Removed broadcast_3_piece0 on d4e9ca837c07:36533 in memory (size: 10.7 KiB, free: 434.4 MiB)
[2025-05-12T17:40:07.339+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:40:07.342+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T17:40:07.343+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSchedulerImpl: Adding task set 4.0 with 8 tasks resource profile 0
[2025-05-12T17:40:07.346+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 15427 bytes)
[2025-05-12T17:40:07.347+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 12) (d4e9ca837c07, executor driver, partition 1, PROCESS_LOCAL, 15085 bytes)
[2025-05-12T17:40:07.349+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 13) (d4e9ca837c07, executor driver, partition 2, PROCESS_LOCAL, 15427 bytes)
[2025-05-12T17:40:07.352+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 14) (d4e9ca837c07, executor driver, partition 3, PROCESS_LOCAL, 15085 bytes)
[2025-05-12T17:40:07.353+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 15) (d4e9ca837c07, executor driver, partition 4, PROCESS_LOCAL, 15427 bytes)
[2025-05-12T17:40:07.354+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 16) (d4e9ca837c07, executor driver, partition 5, PROCESS_LOCAL, 15085 bytes)
[2025-05-12T17:40:07.355+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 17) (d4e9ca837c07, executor driver, partition 6, PROCESS_LOCAL, 15427 bytes)
[2025-05-12T17:40:07.356+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 18) (d4e9ca837c07, executor driver, partition 7, PROCESS_LOCAL, 15085 bytes)
[2025-05-12T17:40:07.358+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Running task 0.0 in stage 4.0 (TID 11)
[2025-05-12T17:40:07.359+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Running task 5.0 in stage 4.0 (TID 16)
[2025-05-12T17:40:07.359+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Running task 1.0 in stage 4.0 (TID 12)
[2025-05-12T17:40:07.360+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Running task 4.0 in stage 4.0 (TID 15)
[2025-05-12T17:40:07.360+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Running task 7.0 in stage 4.0 (TID 18)
[2025-05-12T17:40:07.361+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Running task 2.0 in stage 4.0 (TID 13)
[2025-05-12T17:40:07.361+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Running task 6.0 in stage 4.0 (TID 17)
[2025-05-12T17:40:07.362+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Running task 3.0 in stage 4.0 (TID 14)
[2025-05-12T17:40:07.485+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO CodeGenerator: Code generated in 79.121103 ms
[2025-05-12T17:40:07.515+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO PythonRunner: Times: total = 52, boot = -3656, init = 3708, finish = 0
[2025-05-12T17:40:07.519+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO PythonRunner: Times: total = 71, boot = -3616, init = 3686, finish = 1
[2025-05-12T17:40:07.522+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO PythonRunner: Times: total = 52, boot = -3652, init = 3703, finish = 1
[2025-05-12T17:40:07.523+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO PythonRunner: Times: total = 68, boot = -3629, init = 3697, finish = 0
[2025-05-12T17:40:07.523+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO PythonRunner: Times: total = 61, boot = -3636, init = 3696, finish = 1
[2025-05-12T17:40:07.524+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO PythonRunner: Times: total = 52, boot = -3655, init = 3706, finish = 1
[2025-05-12T17:40:07.524+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO PythonRunner: Times: total = 63, boot = -3643, init = 3705, finish = 1
[2025-05-12T17:40:07.525+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO PythonRunner: Times: total = 62, boot = -3621, init = 3683, finish = 0
[2025-05-12T17:40:07.605+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Finished task 2.0 in stage 4.0 (TID 13). 2362 bytes result sent to driver
[2025-05-12T17:40:07.618+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Finished task 6.0 in stage 4.0 (TID 17). 2362 bytes result sent to driver
[2025-05-12T17:40:07.619+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 13) in 270 ms on d4e9ca837c07 (executor driver) (1/8)
[2025-05-12T17:40:07.620+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Finished task 0.0 in stage 4.0 (TID 11). 2319 bytes result sent to driver
[2025-05-12T17:40:07.621+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Finished task 1.0 in stage 4.0 (TID 12). 2362 bytes result sent to driver
[2025-05-12T17:40:07.627+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Finished task 4.0 in stage 4.0 (TID 15). 2319 bytes result sent to driver
[2025-05-12T17:40:07.630+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Finished task 7.0 in stage 4.0 (TID 18). 2319 bytes result sent to driver
[2025-05-12T17:40:07.631+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 17) in 269 ms on d4e9ca837c07 (executor driver) (2/8)
[2025-05-12T17:40:07.642+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Finished task 3.0 in stage 4.0 (TID 14). 2319 bytes result sent to driver
[2025-05-12T17:40:07.643+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO Executor: Finished task 5.0 in stage 4.0 (TID 16). 2362 bytes result sent to driver
[2025-05-12T17:40:07.651+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 304 ms on d4e9ca837c07 (executor driver) (3/8)
[2025-05-12T17:40:07.652+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 15) in 298 ms on d4e9ca837c07 (executor driver) (4/8)
[2025-05-12T17:40:07.653+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 12) in 303 ms on d4e9ca837c07 (executor driver) (5/8)
[2025-05-12T17:40:07.654+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 18) in 295 ms on d4e9ca837c07 (executor driver) (6/8)
[2025-05-12T17:40:07.655+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 14) in 302 ms on d4e9ca837c07 (executor driver) (7/8)
[2025-05-12T17:40:07.656+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 16) in 298 ms on d4e9ca837c07 (executor driver) (8/8)
[2025-05-12T17:40:07.658+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-05-12T17:40:07.679+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: ShuffleMapStage 4 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.373 s
[2025-05-12T17:40:07.688+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: looking for newly runnable stages
[2025-05-12T17:40:07.689+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: running: Set()
[2025-05-12T17:40:07.692+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: waiting: Set()
[2025-05-12T17:40:07.693+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO DAGScheduler: failed: Set()
[2025-05-12T17:40:07.951+0000] {subprocess.py:93} INFO - 25/05/12 17:40:07 INFO CodeGenerator: Code generated in 53.835243 ms
[2025-05-12T17:40:08.011+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T17:40:08.017+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Got job 5 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T17:40:08.018+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Final stage: ResultStage 6 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:40:08.019+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2025-05-12T17:40:08.023+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:40:08.029+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:40:08.059+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.1 KiB, free 434.3 MiB)
[2025-05-12T17:40:08.132+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.3 MiB)
[2025-05-12T17:40:08.148+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on d4e9ca837c07:36533 (size: 7.4 KiB, free: 434.4 MiB)
[2025-05-12T17:40:08.149+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:40:08.181+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on d4e9ca837c07:36533 in memory (size: 11.6 KiB, free: 434.4 MiB)
[2025-05-12T17:40:08.183+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:40:08.185+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-05-12T17:40:08.195+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 19) (d4e9ca837c07, executor driver, partition 0, NODE_LOCAL, 12874 bytes)
[2025-05-12T17:40:08.198+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO Executor: Running task 0.0 in stage 6.0 (TID 19)
[2025-05-12T17:40:08.352+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO ShuffleBlockFetcherIterator: Getting 8 (640.0 B) non-empty blocks including 8 (640.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-12T17:40:08.367+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 65 ms
[2025-05-12T17:40:08.458+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO CodeGenerator: Code generated in 70.05547 ms
[2025-05-12T17:40:08.519+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO Executor: Finished task 0.0 in stage 6.0 (TID 19). 4056 bytes result sent to driver
[2025-05-12T17:40:08.529+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 19) in 374 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:40:08.529+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-05-12T17:40:08.541+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: ResultStage 6 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.484 s
[2025-05-12T17:40:08.543+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:40:08.544+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-05-12T17:40:08.544+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Job 5 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.526287 s
[2025-05-12T17:40:08.920+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO CodeGenerator: Code generated in 126.315593 ms
[2025-05-12T17:40:08.949+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Registering RDD 25 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) as input to shuffle 1
[2025-05-12T17:40:08.949+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Got map stage job 6 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 8 output partitions
[2025-05-12T17:40:08.950+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:40:08.954+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:40:08.956+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:40:08.958+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[25] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:40:08.971+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.3 KiB, free 434.3 MiB)
[2025-05-12T17:40:08.995+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 434.3 MiB)
[2025-05-12T17:40:09.002+0000] {subprocess.py:93} INFO - 25/05/12 17:40:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on d4e9ca837c07:36533 (size: 18.4 KiB, free: 434.4 MiB)
[2025-05-12T17:40:09.004+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:40:09.005+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on d4e9ca837c07:36533 in memory (size: 7.4 KiB, free: 434.4 MiB)
[2025-05-12T17:40:09.006+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[25] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T17:40:09.007+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSchedulerImpl: Adding task set 7.0 with 8 tasks resource profile 0
[2025-05-12T17:40:09.010+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 20) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 15427 bytes)
[2025-05-12T17:40:09.011+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 21) (d4e9ca837c07, executor driver, partition 1, PROCESS_LOCAL, 15085 bytes)
[2025-05-12T17:40:09.015+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 22) (d4e9ca837c07, executor driver, partition 2, PROCESS_LOCAL, 15427 bytes)
[2025-05-12T17:40:09.016+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 23) (d4e9ca837c07, executor driver, partition 3, PROCESS_LOCAL, 15085 bytes)
[2025-05-12T17:40:09.019+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 24) (d4e9ca837c07, executor driver, partition 4, PROCESS_LOCAL, 15427 bytes)
[2025-05-12T17:40:09.021+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 25) (d4e9ca837c07, executor driver, partition 5, PROCESS_LOCAL, 15085 bytes)
[2025-05-12T17:40:09.024+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 26) (d4e9ca837c07, executor driver, partition 6, PROCESS_LOCAL, 15427 bytes)
[2025-05-12T17:40:09.026+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 27) (d4e9ca837c07, executor driver, partition 7, PROCESS_LOCAL, 15085 bytes)
[2025-05-12T17:40:09.028+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Running task 1.0 in stage 7.0 (TID 21)
[2025-05-12T17:40:09.029+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Running task 0.0 in stage 7.0 (TID 20)
[2025-05-12T17:40:09.064+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Running task 2.0 in stage 7.0 (TID 22)
[2025-05-12T17:40:09.065+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Running task 3.0 in stage 7.0 (TID 23)
[2025-05-12T17:40:09.065+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Running task 4.0 in stage 7.0 (TID 24)
[2025-05-12T17:40:09.072+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Running task 5.0 in stage 7.0 (TID 25)
[2025-05-12T17:40:09.085+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Running task 6.0 in stage 7.0 (TID 26)
[2025-05-12T17:40:09.087+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Running task 7.0 in stage 7.0 (TID 27)
[2025-05-12T17:40:09.210+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO CodeGenerator: Code generated in 121.223331 ms
[2025-05-12T17:40:09.239+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO CodeGenerator: Code generated in 17.914152 ms
[2025-05-12T17:40:09.277+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO CodeGenerator: Code generated in 16.612523 ms
[2025-05-12T17:40:09.314+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO CodeGenerator: Code generated in 10.72131 ms
[2025-05-12T17:40:09.340+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO CodeGenerator: Code generated in 14.367909 ms
[2025-05-12T17:40:09.362+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO PythonRunner: Times: total = 66, boot = -1644, init = 1710, finish = 0
[2025-05-12T17:40:09.364+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO PythonRunner: Times: total = 81, boot = -1625, init = 1706, finish = 0
[2025-05-12T17:40:09.367+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO PythonRunner: Times: total = 52, boot = -1612, init = 1664, finish = 0
[2025-05-12T17:40:09.368+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO PythonRunner: Times: total = 63, boot = -1611, init = 1673, finish = 1
[2025-05-12T17:40:09.370+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO PythonRunner: Times: total = 96, boot = -1632, init = 1727, finish = 1
[2025-05-12T17:40:09.371+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO PythonRunner: Times: total = 46, boot = -1618, init = 1663, finish = 1
[2025-05-12T17:40:09.372+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO PythonRunner: Times: total = 67, boot = -1637, init = 1703, finish = 1
[2025-05-12T17:40:09.372+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO PythonRunner: Times: total = 90, boot = -1568, init = 1657, finish = 1
[2025-05-12T17:40:09.442+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Finished task 5.0 in stage 7.0 (TID 25). 2882 bytes result sent to driver
[2025-05-12T17:40:09.447+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Finished task 0.0 in stage 7.0 (TID 20). 2882 bytes result sent to driver
[2025-05-12T17:40:09.448+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Finished task 3.0 in stage 7.0 (TID 23). 2882 bytes result sent to driver
[2025-05-12T17:40:09.449+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Finished task 6.0 in stage 7.0 (TID 26). 2882 bytes result sent to driver
[2025-05-12T17:40:09.450+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Finished task 4.0 in stage 7.0 (TID 24). 2882 bytes result sent to driver
[2025-05-12T17:40:09.450+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 25) in 429 ms on d4e9ca837c07 (executor driver) (1/8)
[2025-05-12T17:40:09.451+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 26) in 429 ms on d4e9ca837c07 (executor driver) (2/8)
[2025-05-12T17:40:09.452+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Finished task 7.0 in stage 7.0 (TID 27). 2882 bytes result sent to driver
[2025-05-12T17:40:09.452+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 23) in 440 ms on d4e9ca837c07 (executor driver) (3/8)
[2025-05-12T17:40:09.454+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 20) in 445 ms on d4e9ca837c07 (executor driver) (4/8)
[2025-05-12T17:40:09.458+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Finished task 1.0 in stage 7.0 (TID 21). 2882 bytes result sent to driver
[2025-05-12T17:40:09.459+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 21) in 446 ms on d4e9ca837c07 (executor driver) (5/8)
[2025-05-12T17:40:09.459+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 27) in 433 ms on d4e9ca837c07 (executor driver) (6/8)
[2025-05-12T17:40:09.460+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Finished task 2.0 in stage 7.0 (TID 22). 2882 bytes result sent to driver
[2025-05-12T17:40:09.460+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 24) in 444 ms on d4e9ca837c07 (executor driver) (7/8)
[2025-05-12T17:40:09.461+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 22) in 449 ms on d4e9ca837c07 (executor driver) (8/8)
[2025-05-12T17:40:09.461+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-05-12T17:40:09.471+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: ShuffleMapStage 7 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.510 s
[2025-05-12T17:40:09.473+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: looking for newly runnable stages
[2025-05-12T17:40:09.473+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: running: Set()
[2025-05-12T17:40:09.474+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: waiting: Set()
[2025-05-12T17:40:09.475+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: failed: Set()
[2025-05-12T17:40:09.491+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-05-12T17:40:09.514+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2025-05-12T17:40:09.533+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO CodeGenerator: Code generated in 9.76067 ms
[2025-05-12T17:40:09.567+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T17:40:09.569+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: Got job 7 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T17:40:09.570+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: Final stage: ResultStage 9 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:40:09.570+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[2025-05-12T17:40:09.571+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:40:09.571+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[28] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:40:09.579+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 40.5 KiB, free 434.3 MiB)
[2025-05-12T17:40:09.604+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 434.3 MiB)
[2025-05-12T17:40:09.608+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on d4e9ca837c07:36533 (size: 19.5 KiB, free: 434.4 MiB)
[2025-05-12T17:40:09.613+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:40:09.614+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on d4e9ca837c07:36533 in memory (size: 18.4 KiB, free: 434.4 MiB)
[2025-05-12T17:40:09.615+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[28] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:40:09.617+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-05-12T17:40:09.618+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 28) (d4e9ca837c07, executor driver, partition 0, NODE_LOCAL, 12874 bytes)
[2025-05-12T17:40:09.619+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Running task 0.0 in stage 9.0 (TID 28)
[2025-05-12T17:40:09.637+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO ShuffleBlockFetcherIterator: Getting 8 (864.0 B) non-empty blocks including 8 (864.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-12T17:40:09.641+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2025-05-12T17:40:09.676+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO CodeGenerator: Code generated in 35.563311 ms
[2025-05-12T17:40:09.709+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Executor: Finished task 0.0 in stage 9.0 (TID 28). 5258 bytes result sent to driver
[2025-05-12T17:40:09.712+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 28) in 99 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:40:09.713+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-05-12T17:40:09.715+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: ResultStage 9 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.135 s
[2025-05-12T17:40:09.716+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:40:09.718+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-05-12T17:40:09.742+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO DAGScheduler: Job 7 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.147144 s
[2025-05-12T17:40:09.766+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 ERROR MicroBatchExecution: Query [id = a00b2518-af6b-486f-8f08-f380ecf7f530, runId = 66fda5a3-a8a9-471c-a836-6e75f5ebabf3] terminated with error
[2025-05-12T17:40:09.768+0000] {subprocess.py:93} INFO - py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
[2025-05-12T17:40:09.772+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
[2025-05-12T17:40:09.779+0000] {subprocess.py:93} INFO -     return_value = getattr(self.pool[obj_id], method)(*params)
[2025-05-12T17:40:09.779+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
[2025-05-12T17:40:09.780+0000] {subprocess.py:93} INFO -     raise e
[2025-05-12T17:40:09.781+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
[2025-05-12T17:40:09.781+0000] {subprocess.py:93} INFO -     self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
[2025-05-12T17:40:09.782+0000] {subprocess.py:93} INFO -   File "/scripts/spark_news.py", line 175, in process_batch
[2025-05-12T17:40:09.782+0000] {subprocess.py:93} INFO -     dominant_recommendation = max(recommendation_counts, key=lambda x: x["count"])["recommendation"]
[2025-05-12T17:40:09.783+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 174, in wrapped
[2025-05-12T17:40:09.784+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-12T17:40:09.785+0000] {subprocess.py:93} INFO - TypeError: max() got an unexpected keyword argument 'key'
[2025-05-12T17:40:09.785+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:09.786+0000] {subprocess.py:93} INFO - 	at py4j.Protocol.getReturnValue(Protocol.java:476)
[2025-05-12T17:40:09.786+0000] {subprocess.py:93} INFO - 	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
[2025-05-12T17:40:09.787+0000] {subprocess.py:93} INFO - 	at com.sun.proxy.$Proxy37.call(Unknown Source)
[2025-05-12T17:40:09.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
[2025-05-12T17:40:09.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
[2025-05-12T17:40:09.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
[2025-05-12T17:40:09.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
[2025-05-12T17:40:09.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-05-12T17:40:09.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-05-12T17:40:09.790+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-05-12T17:40:09.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-05-12T17:40:09.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-05-12T17:40:09.794+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-05-12T17:40:09.795+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-05-12T17:40:09.796+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-05-12T17:40:09.797+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-05-12T17:40:09.797+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-05-12T17:40:09.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-05-12T17:40:09.799+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-05-12T17:40:09.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-05-12T17:40:09.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-05-12T17:40:09.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-05-12T17:40:09.801+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-05-12T17:40:09.801+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
[2025-05-12T17:40:09.802+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-05-12T17:40:09.802+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-05-12T17:40:09.802+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-05-12T17:40:09.803+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-05-12T17:40:09.803+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-05-12T17:40:09.804+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-05-12T17:40:09.804+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-05-12T17:40:09.805+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-05-12T17:40:09.805+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-05-12T17:40:09.806+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
[2025-05-12T17:40:09.807+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:40:09.809+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:40:09.809+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Metrics: Metrics reporters closed
[2025-05-12T17:40:09.810+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO MicroBatchExecution: Async log purge executor pool for query [id = a00b2518-af6b-486f-8f08-f380ecf7f530, runId = 66fda5a3-a8a9-471c-a836-6e75f5ebabf3] has been shutdown
[2025-05-12T17:40:09.860+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-05-12T17:40:09.861+0000] {subprocess.py:93} INFO -   File "/scripts/spark_news.py", line 228, in <module>
[2025-05-12T17:40:09.864+0000] {subprocess.py:93} INFO -     analysis_query.awaitTermination()
[2025-05-12T17:40:09.865+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
[2025-05-12T17:40:09.865+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-05-12T17:40:09.866+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-05-12T17:40:09.875+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = a00b2518-af6b-486f-8f08-f380ecf7f530, runId = 66fda5a3-a8a9-471c-a836-6e75f5ebabf3] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
[2025-05-12T17:40:09.876+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
[2025-05-12T17:40:09.877+0000] {subprocess.py:93} INFO -     return_value = getattr(self.pool[obj_id], method)(*params)
[2025-05-12T17:40:09.877+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
[2025-05-12T17:40:09.878+0000] {subprocess.py:93} INFO -     raise e
[2025-05-12T17:40:09.878+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
[2025-05-12T17:40:09.878+0000] {subprocess.py:93} INFO -     self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
[2025-05-12T17:40:09.879+0000] {subprocess.py:93} INFO -   File "/scripts/spark_news.py", line 175, in process_batch
[2025-05-12T17:40:09.879+0000] {subprocess.py:93} INFO -     dominant_recommendation = max(recommendation_counts, key=lambda x: x["count"])["recommendation"]
[2025-05-12T17:40:09.879+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 174, in wrapped
[2025-05-12T17:40:09.880+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-12T17:40:09.880+0000] {subprocess.py:93} INFO - TypeError: max() got an unexpected keyword argument 'key'
[2025-05-12T17:40:09.881+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:40:09.957+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-05-12T17:40:09.960+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1, groupId=spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-05-12T17:40:09.964+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:40:09.965+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:40:09.966+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Metrics: Metrics reporters closed
[2025-05-12T17:40:09.967+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-cc48baa4-bffc-4ca4-baec-fbc14566ab27--1782948799-executor-1 unregistered
[2025-05-12T17:40:09.970+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-05-12T17:40:09.971+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2, groupId=spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-05-12T17:40:09.972+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:40:09.973+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:40:09.973+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO Metrics: Metrics reporters closed
[2025-05-12T17:40:09.981+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1808b5d0-663f-4ef1-871d-1af47c09f77d-1979046052-executor-2 unregistered
[2025-05-12T17:40:09.982+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO SparkContext: Invoking stop() from shutdown hook
[2025-05-12T17:40:09.982+0000] {subprocess.py:93} INFO - 25/05/12 17:40:09 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-05-12T17:40:10.002+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO CassandraConnector: Disconnected from Cassandra cluster.
[2025-05-12T17:40:10.003+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
[2025-05-12T17:40:10.015+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO SparkUI: Stopped Spark web UI at http://d4e9ca837c07:4040
[2025-05-12T17:40:10.050+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-05-12T17:40:10.077+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO MemoryStore: MemoryStore cleared
[2025-05-12T17:40:10.078+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO BlockManager: BlockManager stopped
[2025-05-12T17:40:10.087+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-05-12T17:40:10.091+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-05-12T17:40:10.102+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO SparkContext: Successfully stopped SparkContext
[2025-05-12T17:40:10.103+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO ShutdownHookManager: Shutdown hook called
[2025-05-12T17:40:10.103+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269
[2025-05-12T17:40:10.107+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-a11c186e-a918-421c-967e-1922f270f16b
[2025-05-12T17:40:10.110+0000] {subprocess.py:93} INFO - 25/05/12 17:40:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-3fde075d-a775-47ad-b490-199c79e02269/pyspark-12b27e29-cde3-4e82-9980-8680e91fc0f9
[2025-05-12T17:40:10.239+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-05-12T17:40:10.251+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-05-12T17:40:10.257+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=gold_news_pipeline, task_id=run_consumer, execution_date=20250512T173832, start_date=20250512T173840, end_date=20250512T174010
[2025-05-12T17:40:10.273+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 87 for task run_consumer (Bash command failed. The command returned a non-zero exit code 1.; 4156)
[2025-05-12T17:40:10.317+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-05-12T17:40:10.350+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
