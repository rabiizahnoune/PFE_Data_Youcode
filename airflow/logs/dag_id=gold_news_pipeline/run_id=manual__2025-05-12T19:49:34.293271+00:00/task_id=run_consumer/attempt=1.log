[2025-05-12T19:49:43.705+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: gold_news_pipeline.run_consumer manual__2025-05-12T19:49:34.293271+00:00 [queued]>
[2025-05-12T19:49:43.737+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: gold_news_pipeline.run_consumer manual__2025-05-12T19:49:34.293271+00:00 [queued]>
[2025-05-12T19:49:43.738+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 4
[2025-05-12T19:49:43.777+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): run_consumer> on 2025-05-12 19:49:34.293271+00:00
[2025-05-12T19:49:43.786+0000] {standard_task_runner.py:57} INFO - Started process 2600 to run task
[2025-05-12T19:49:43.798+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'gold_news_pipeline', 'run_consumer', 'manual__2025-05-12T19:49:34.293271+00:00', '--job-id', '156', '--raw', '--subdir', 'DAGS_FOLDER/dag_news.py', '--cfg-path', '/tmp/tmp2q2yam19']
[2025-05-12T19:49:43.803+0000] {standard_task_runner.py:85} INFO - Job 156: Subtask run_consumer
[2025-05-12T19:49:43.908+0000] {task_command.py:415} INFO - Running <TaskInstance: gold_news_pipeline.run_consumer manual__2025-05-12T19:49:34.293271+00:00 [running]> on host a84f8d9926dd
[2025-05-12T19:49:44.108+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='gold_news_pipeline' AIRFLOW_CTX_TASK_ID='run_consumer' AIRFLOW_CTX_EXECUTION_DATE='2025-05-12T19:49:34.293271+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-12T19:49:34.293271+00:00'
[2025-05-12T19:49:44.113+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-05-12T19:49:44.121+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec gold_price_project-spark-1 bash -c "export PYSPARK_PYTHON=/usr/bin/python3.8 && spark-submit --conf spark.pyspark.python=/usr/bin/python3.8 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 /scripts/spark_news.py"']
[2025-05-12T19:49:44.149+0000] {subprocess.py:86} INFO - Output:
[2025-05-12T19:49:48.564+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-05-12T19:49:48.830+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /root/.ivy2/cache
[2025-05-12T19:49:48.831+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /root/.ivy2/jars
[2025-05-12T19:49:48.842+0000] {subprocess.py:93} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-05-12T19:49:48.844+0000] {subprocess.py:93} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2025-05-12T19:49:48.847+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-281a4e20-ed1c-441e-841d-0e40ed30a957;1.0
[2025-05-12T19:49:48.848+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-05-12T19:49:49.200+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
[2025-05-12T19:49:49.338+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2025-05-12T19:49:49.403+0000] {subprocess.py:93} INFO - 	found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-05-12T19:49:49.460+0000] {subprocess.py:93} INFO - 	found org.lz4#lz4-java;1.8.0 in central
[2025-05-12T19:49:49.507+0000] {subprocess.py:93} INFO - 	found org.xerial.snappy#snappy-java;1.1.10.3 in spark-list
[2025-05-12T19:49:49.551+0000] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;2.0.7 in central
[2025-05-12T19:49:49.638+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-05-12T19:49:49.717+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-05-12T19:49:49.777+0000] {subprocess.py:93} INFO - 	found commons-logging#commons-logging;1.1.3 in central
[2025-05-12T19:49:49.807+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-05-12T19:49:49.843+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-pool2;2.11.1 in central
[2025-05-12T19:49:49.916+0000] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in spark-list
[2025-05-12T19:49:49.959+0000] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central
[2025-05-12T19:49:49.990+0000] {subprocess.py:93} INFO - 	found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2025-05-12T19:49:50.052+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2025-05-12T19:49:50.099+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#native-protocol;1.5.0 in central
[2025-05-12T19:49:50.130+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2025-05-12T19:49:50.161+0000] {subprocess.py:93} INFO - 	found com.typesafe#config;1.4.1 in central
[2025-05-12T19:49:50.195+0000] {subprocess.py:93} INFO - 	found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2025-05-12T19:49:50.222+0000] {subprocess.py:93} INFO - 	found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2025-05-12T19:49:50.250+0000] {subprocess.py:93} INFO - 	found org.reactivestreams#reactive-streams;1.0.3 in central
[2025-05-12T19:49:50.270+0000] {subprocess.py:93} INFO - 	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-05-12T19:49:50.295+0000] {subprocess.py:93} INFO - 	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2025-05-12T19:49:50.314+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.2 in central
[2025-05-12T19:49:50.351+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2025-05-12T19:49:50.393+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2025-05-12T19:49:50.440+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-lang3;3.10 in central
[2025-05-12T19:49:50.465+0000] {subprocess.py:93} INFO - 	found com.thoughtworks.paranamer#paranamer;2.8 in central
[2025-05-12T19:49:50.486+0000] {subprocess.py:93} INFO - 	found org.scala-lang#scala-reflect;2.12.11 in central
[2025-05-12T19:49:50.563+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 1670ms :: artifacts dl 45ms
[2025-05-12T19:49:50.567+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-05-12T19:49:50.579+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2025-05-12T19:49:50.583+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2025-05-12T19:49:50.585+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2025-05-12T19:49:50.587+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2025-05-12T19:49:50.589+0000] {subprocess.py:93} INFO - 	com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2025-05-12T19:49:50.590+0000] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]
[2025-05-12T19:49:50.593+0000] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from spark-list in [default]
[2025-05-12T19:49:50.594+0000] {subprocess.py:93} INFO - 	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2025-05-12T19:49:50.595+0000] {subprocess.py:93} INFO - 	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-05-12T19:49:50.600+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2025-05-12T19:49:50.602+0000] {subprocess.py:93} INFO - 	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2025-05-12T19:49:50.603+0000] {subprocess.py:93} INFO - 	com.typesafe#config;1.4.1 from central in [default]
[2025-05-12T19:49:50.604+0000] {subprocess.py:93} INFO - 	commons-logging#commons-logging;1.1.3 from central in [default]
[2025-05-12T19:49:50.604+0000] {subprocess.py:93} INFO - 	io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2025-05-12T19:49:50.605+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-lang3;3.10 from central in [default]
[2025-05-12T19:49:50.606+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2025-05-12T19:49:50.608+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2025-05-12T19:49:50.609+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-05-12T19:49:50.610+0000] {subprocess.py:93} INFO - 	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
[2025-05-12T19:49:50.613+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
[2025-05-12T19:49:50.617+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
[2025-05-12T19:49:50.619+0000] {subprocess.py:93} INFO - 	org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2025-05-12T19:49:50.622+0000] {subprocess.py:93} INFO - 	org.lz4#lz4-java;1.8.0 from central in [default]
[2025-05-12T19:49:50.623+0000] {subprocess.py:93} INFO - 	org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2025-05-12T19:49:50.623+0000] {subprocess.py:93} INFO - 	org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2025-05-12T19:49:50.624+0000] {subprocess.py:93} INFO - 	org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2025-05-12T19:49:50.624+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;2.0.7 from central in [default]
[2025-05-12T19:49:50.624+0000] {subprocess.py:93} INFO - 	org.xerial.snappy#snappy-java;1.1.10.3 from spark-list in [default]
[2025-05-12T19:49:50.628+0000] {subprocess.py:93} INFO - 	:: evicted modules:
[2025-05-12T19:49:50.629+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]
[2025-05-12T19:49:50.631+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.7] in [default]
[2025-05-12T19:49:50.632+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T19:49:50.633+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-05-12T19:49:50.634+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-12T19:49:50.635+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T19:49:50.635+0000] {subprocess.py:93} INFO - 	|      default     |   30  |   0   |   0   |   2   ||   28  |   0   |
[2025-05-12T19:49:50.636+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T19:49:50.636+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-281a4e20-ed1c-441e-841d-0e40ed30a957
[2025-05-12T19:49:50.637+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-05-12T19:49:50.677+0000] {subprocess.py:93} INFO - 	0 artifacts copied, 28 already retrieved (0kB/49ms)
[2025-05-12T19:49:51.196+0000] {subprocess.py:93} INFO - 25/05/12 19:49:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-12T19:49:53.456+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO SparkContext: Running Spark version 3.5.1
[2025-05-12T19:49:53.457+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-12T19:49:53.457+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO SparkContext: Java version 11.0.22
[2025-05-12T19:49:53.491+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO ResourceUtils: ==============================================================
[2025-05-12T19:49:53.492+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-05-12T19:49:53.492+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO ResourceUtils: ==============================================================
[2025-05-12T19:49:53.493+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO SparkContext: Submitted application: GoldNewsStreaming
[2025-05-12T19:49:53.540+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-05-12T19:49:53.566+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO ResourceProfile: Limiting resource is cpu
[2025-05-12T19:49:53.567+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-05-12T19:49:53.684+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO SecurityManager: Changing view acls to: root
[2025-05-12T19:49:53.686+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO SecurityManager: Changing modify acls to: root
[2025-05-12T19:49:53.689+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO SecurityManager: Changing view acls groups to:
[2025-05-12T19:49:53.690+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO SecurityManager: Changing modify acls groups to:
[2025-05-12T19:49:53.691+0000] {subprocess.py:93} INFO - 25/05/12 19:49:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
[2025-05-12T19:49:54.120+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO Utils: Successfully started service 'sparkDriver' on port 42803.
[2025-05-12T19:49:54.173+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkEnv: Registering MapOutputTracker
[2025-05-12T19:49:54.238+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkEnv: Registering BlockManagerMaster
[2025-05-12T19:49:54.265+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-05-12T19:49:54.266+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-05-12T19:49:54.278+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-05-12T19:49:54.325+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d663101c-1744-4575-91c5-4b935c36d127
[2025-05-12T19:49:54.349+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-05-12T19:49:54.385+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-05-12T19:49:54.683+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-05-12T19:49:54.854+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-05-12T19:49:54.990+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://407aab94b4b5:42803/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:54.993+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://407aab94b4b5:42803/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:54.996+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://407aab94b4b5:42803/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.003+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://407aab94b4b5:42803/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.012+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://407aab94b4b5:42803/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.013+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://407aab94b4b5:42803/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747079393440
[2025-05-12T19:49:55.014+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://407aab94b4b5:42803/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.015+0000] {subprocess.py:93} INFO - 25/05/12 19:49:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://407aab94b4b5:42803/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747079393440
[2025-05-12T19:49:55.017+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://407aab94b4b5:42803/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747079393440
[2025-05-12T19:49:55.017+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://407aab94b4b5:42803/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747079393440
[2025-05-12T19:49:55.018+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://407aab94b4b5:42803/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747079393440
[2025-05-12T19:49:55.019+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://407aab94b4b5:42803/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.019+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://407aab94b4b5:42803/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.020+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.020+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.021+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://407aab94b4b5:42803/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747079393440
[2025-05-12T19:49:55.023+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://407aab94b4b5:42803/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747079393440
[2025-05-12T19:49:55.027+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://407aab94b4b5:42803/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747079393440
[2025-05-12T19:49:55.028+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://407aab94b4b5:42803/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.029+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.029+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://407aab94b4b5:42803/jars/com.typesafe_config-1.4.1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.030+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://407aab94b4b5:42803/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747079393440
[2025-05-12T19:49:55.031+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://407aab94b4b5:42803/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747079393440
[2025-05-12T19:49:55.032+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://407aab94b4b5:42803/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747079393440
[2025-05-12T19:49:55.032+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://407aab94b4b5:42803/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.033+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://407aab94b4b5:42803/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747079393440
[2025-05-12T19:49:55.033+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://407aab94b4b5:42803/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747079393440
[2025-05-12T19:49:55.034+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.035+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.035+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T19:49:55.048+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.049+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T19:49:55.064+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.068+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T19:49:55.074+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.076+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T19:49:55.094+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.095+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T19:49:55.108+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747079393440
[2025-05-12T19:49:55.109+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T19:49:55.174+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.175+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T19:49:55.182+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747079393440
[2025-05-12T19:49:55.183+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T19:49:55.195+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747079393440
[2025-05-12T19:49:55.196+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T19:49:55.202+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747079393440
[2025-05-12T19:49:55.203+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T19:49:55.234+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747079393440
[2025-05-12T19:49:55.236+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T19:49:55.241+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.241+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T19:49:55.248+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.249+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T19:49:55.254+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.255+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T19:49:55.269+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.271+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T19:49:55.275+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747079393440
[2025-05-12T19:49:55.276+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T19:49:55.281+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747079393440
[2025-05-12T19:49:55.282+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T19:49:55.287+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747079393440
[2025-05-12T19:49:55.288+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T19:49:55.297+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.299+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T19:49:55.304+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.305+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T19:49:55.315+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar at file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.316+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.typesafe_config-1.4.1.jar
[2025-05-12T19:49:55.320+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747079393440
[2025-05-12T19:49:55.322+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T19:49:55.327+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747079393440
[2025-05-12T19:49:55.328+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T19:49:55.334+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747079393440
[2025-05-12T19:49:55.335+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T19:49:55.340+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.341+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T19:49:55.348+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747079393440
[2025-05-12T19:49:55.349+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T19:49:55.354+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747079393440
[2025-05-12T19:49:55.356+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T19:49:55.362+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.363+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T19:49:55.495+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Starting executor ID driver on host 407aab94b4b5
[2025-05-12T19:49:55.498+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-12T19:49:55.500+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Java version 11.0.22
[2025-05-12T19:49:55.514+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-05-12T19:49:55.517+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3306db02 for default.
[2025-05-12T19:49:55.545+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.589+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T19:49:55.595+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747079393440
[2025-05-12T19:49:55.598+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T19:49:55.604+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747079393440
[2025-05-12T19:49:55.629+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T19:49:55.635+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747079393440
[2025-05-12T19:49:55.659+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T19:49:55.663+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.668+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T19:49:55.674+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747079393440
[2025-05-12T19:49:55.675+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T19:49:55.682+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.683+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T19:49:55.687+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.696+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T19:49:55.699+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.700+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.typesafe_config-1.4.1.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.typesafe_config-1.4.1.jar
[2025-05-12T19:49:55.705+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747079393440
[2025-05-12T19:49:55.707+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T19:49:55.715+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747079393440
[2025-05-12T19:49:55.716+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T19:49:55.720+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.721+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T19:49:55.728+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747079393440
[2025-05-12T19:49:55.729+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T19:49:55.731+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.732+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T19:49:55.735+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.736+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T19:49:55.743+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.744+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T19:49:55.746+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.747+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T19:49:55.751+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747079393440
[2025-05-12T19:49:55.752+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T19:49:55.758+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747079393440
[2025-05-12T19:49:55.759+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T19:49:55.761+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747079393440
[2025-05-12T19:49:55.762+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T19:49:55.766+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747079393440
[2025-05-12T19:49:55.769+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T19:49:55.775+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.777+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T19:49:55.781+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747079393440
[2025-05-12T19:49:55.782+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T19:49:55.788+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747079393440
[2025-05-12T19:49:55.789+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T19:49:55.791+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.792+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T19:49:55.796+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.798+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T19:49:55.805+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747079393440
[2025-05-12T19:49:55.806+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T19:49:55.809+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747079393440
[2025-05-12T19:49:55.819+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T19:49:55.824+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747079393440
[2025-05-12T19:49:55.886+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO TransportClientFactory: Successfully created connection to 407aab94b4b5/172.18.0.3:42803 after 43 ms (0 ms spent in bootstraps)
[2025-05-12T19:49:55.900+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp12782850740061104185.tmp
[2025-05-12T19:49:55.935+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp12782850740061104185.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T19:49:55.942+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.hdrhistogram_HdrHistogram-2.1.12.jar to class loader default
[2025-05-12T19:49:55.943+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747079393440
[2025-05-12T19:49:55.944+0000] {subprocess.py:93} INFO - 25/05/12 19:49:55 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp11802654634445720986.tmp
[2025-05-12T19:49:56.158+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp11802654634445720986.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T19:49:56.166+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
[2025-05-12T19:49:56.167+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.170+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp14710620311277782486.tmp
[2025-05-12T19:49:56.173+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp14710620311277782486.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T19:49:56.176+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to class loader default
[2025-05-12T19:49:56.177+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747079393440
[2025-05-12T19:49:56.178+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp16614952449895556183.tmp
[2025-05-12T19:49:56.185+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp16614952449895556183.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T19:49:56.189+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.commons_commons-lang3-3.10.jar to class loader default
[2025-05-12T19:49:56.190+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.191+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp10052312362714350761.tmp
[2025-05-12T19:49:56.208+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp10052312362714350761.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T19:49:56.213+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to class loader default
[2025-05-12T19:49:56.215+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.216+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp13653273931175510814.tmp
[2025-05-12T19:49:56.223+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp13653273931175510814.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T19:49:56.226+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
[2025-05-12T19:49:56.227+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747079393440
[2025-05-12T19:49:56.230+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp5163678739182178443.tmp
[2025-05-12T19:49:56.233+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp5163678739182178443.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T19:49:56.237+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.reactivestreams_reactive-streams-1.0.3.jar to class loader default
[2025-05-12T19:49:56.238+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.239+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp16302101915287471304.tmp
[2025-05-12T19:49:56.243+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp16302101915287471304.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T19:49:56.248+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to class loader default
[2025-05-12T19:49:56.251+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747079393440
[2025-05-12T19:49:56.252+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp537754301526124639.tmp
[2025-05-12T19:49:56.282+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp537754301526124639.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T19:49:56.284+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
[2025-05-12T19:49:56.285+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.286+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp9723147505347524949.tmp
[2025-05-12T19:49:56.297+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp9723147505347524949.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T19:49:56.300+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to class loader default
[2025-05-12T19:49:56.301+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747079393440
[2025-05-12T19:49:56.302+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp8774320819502702570.tmp
[2025-05-12T19:49:56.318+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp8774320819502702570.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T19:49:56.324+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to class loader default
[2025-05-12T19:49:56.325+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747079393440
[2025-05-12T19:49:56.326+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp8541844397853711984.tmp
[2025-05-12T19:49:56.343+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp8541844397853711984.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T19:49:56.346+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.scala-lang_scala-reflect-2.12.11.jar to class loader default
[2025-05-12T19:49:56.347+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747079393440
[2025-05-12T19:49:56.348+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp15845879571510483239.tmp
[2025-05-12T19:49:56.361+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp15845879571510483239.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T19:49:56.364+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
[2025-05-12T19:49:56.368+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747079393440
[2025-05-12T19:49:56.370+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp6395116720696839973.tmp
[2025-05-12T19:49:56.555+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp6395116720696839973.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T19:49:56.568+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
[2025-05-12T19:49:56.570+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747079393440
[2025-05-12T19:49:56.571+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp157054899860080535.tmp
[2025-05-12T19:49:56.578+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp157054899860080535.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T19:49:56.590+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.slf4j_slf4j-api-2.0.7.jar to class loader default
[2025-05-12T19:49:56.592+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747079393440
[2025-05-12T19:49:56.593+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp4455810584203863881.tmp
[2025-05-12T19:49:56.598+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp4455810584203863881.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T19:49:56.601+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to class loader default
[2025-05-12T19:49:56.603+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747079393440
[2025-05-12T19:49:56.603+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp9535860296036020079.tmp
[2025-05-12T19:49:56.608+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp9535860296036020079.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T19:49:56.617+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.thoughtworks.paranamer_paranamer-2.8.jar to class loader default
[2025-05-12T19:49:56.622+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.625+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp17849151740366224293.tmp
[2025-05-12T19:49:56.630+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp17849151740366224293.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T19:49:56.634+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_native-protocol-1.5.0.jar to class loader default
[2025-05-12T19:49:56.636+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747079393440
[2025-05-12T19:49:56.644+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp1097503820443139999.tmp
[2025-05-12T19:49:56.647+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp1097503820443139999.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T19:49:56.652+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2025-05-12T19:49:56.653+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.654+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp6929585922544658087.tmp
[2025-05-12T19:49:56.665+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp6929585922544658087.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T19:49:56.669+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.lz4_lz4-java-1.8.0.jar to class loader default
[2025-05-12T19:49:56.673+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747079393440
[2025-05-12T19:49:56.675+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp16218523186279889233.tmp
[2025-05-12T19:49:56.677+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp16218523186279889233.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T19:49:56.680+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/io.dropwizard.metrics_metrics-core-4.1.18.jar to class loader default
[2025-05-12T19:49:56.682+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.683+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp9799976513083260309.tmp
[2025-05-12T19:49:56.690+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp9799976513083260309.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T19:49:56.695+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-query-builder-4.13.0.jar to class loader default
[2025-05-12T19:49:56.698+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.typesafe_config-1.4.1.jar with timestamp 1747079393440
[2025-05-12T19:49:56.699+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp1323712771934773485.tmp
[2025-05-12T19:49:56.706+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp1323712771934773485.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.typesafe_config-1.4.1.jar
[2025-05-12T19:49:56.710+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.typesafe_config-1.4.1.jar to class loader default
[2025-05-12T19:49:56.711+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747079393440
[2025-05-12T19:49:56.712+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp7014991890179813978.tmp
[2025-05-12T19:49:56.714+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp7014991890179813978.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T19:49:56.719+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to class loader default
[2025-05-12T19:49:56.720+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747079393440
[2025-05-12T19:49:56.721+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp13200711214679869273.tmp
[2025-05-12T19:49:56.724+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp13200711214679869273.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T19:49:56.729+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/commons-logging_commons-logging-1.1.3.jar to class loader default
[2025-05-12T19:49:56.730+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.732+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp5957104087073705130.tmp
[2025-05-12T19:49:56.785+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp5957104087073705130.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T19:49:56.792+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to class loader default
[2025-05-12T19:49:56.797+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747079393440
[2025-05-12T19:49:56.798+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp474448220589805698.tmp
[2025-05-12T19:49:56.799+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp474448220589805698.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T19:49:56.801+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/com.google.code.findbugs_jsr305-3.0.2.jar to class loader default
[2025-05-12T19:49:56.802+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Fetching spark://407aab94b4b5:42803/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747079393440
[2025-05-12T19:49:56.805+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Fetching spark://407aab94b4b5:42803/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp6386544502429731176.tmp
[2025-05-12T19:49:56.811+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/fetchFileTemp6386544502429731176.tmp has been previously copied to /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T19:49:56.814+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Executor: Adding file:/tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/userFiles-4cc98c26-6b9a-4b28-8b3a-255cb73e2606/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
[2025-05-12T19:49:56.843+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36387.
[2025-05-12T19:49:56.844+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO NettyBlockTransferService: Server created on 407aab94b4b5:36387
[2025-05-12T19:49:56.847+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-05-12T19:49:56.863+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 407aab94b4b5, 36387, None)
[2025-05-12T19:49:56.873+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO BlockManagerMasterEndpoint: Registering block manager 407aab94b4b5:36387 with 434.4 MiB RAM, BlockManagerId(driver, 407aab94b4b5, 36387, None)
[2025-05-12T19:49:56.877+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 407aab94b4b5, 36387, None)
[2025-05-12T19:49:56.881+0000] {subprocess.py:93} INFO - 25/05/12 19:49:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 407aab94b4b5, 36387, None)
[2025-05-12T19:49:57.713+0000] {subprocess.py:93} INFO - 25/05/12 19:49:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-05-12T19:49:57.718+0000] {subprocess.py:93} INFO - 25/05/12 19:49:57 INFO SharedState: Warehouse path is 'file:/opt/spark-apps/spark-warehouse'.
[2025-05-12T19:50:01.062+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-05-12T19:50:01.208+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint/impact_analysis resolved to file:/tmp/spark-checkpoint/impact_analysis.
[2025-05-12T19:50:01.210+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-05-12T19:50:01.581+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO MicroBatchExecution: Starting [id = 456a207a-18f1-456a-87bc-ab7592296d01, runId = 0155082a-f1bc-4e0a-8ffa-c260073d080b]. Use file:/tmp/spark-checkpoint/impact_analysis to store the query checkpoint.
[2025-05-12T19:50:01.608+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@204815fd] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@1b539caa]
[2025-05-12T19:50:01.655+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 WARN MicroBatchExecution: The read limit MaxRows: 100 for KafkaV2[Subscribe[gold-news]] is ignored when Trigger.Once is used.
[2025-05-12T19:50:01.670+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2
[2025-05-12T19:50:01.690+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO OffsetSeqLog: Getting latest batch 2
[2025-05-12T19:50:01.745+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2
[2025-05-12T19:50:01.746+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO OffsetSeqLog: Getting latest batch 2
[2025-05-12T19:50:01.762+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO CommitLog: BatchIds found from listing: 0, 1, 2
[2025-05-12T19:50:01.763+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO CommitLog: Getting latest batch 2
[2025-05-12T19:50:01.771+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO MicroBatchExecution: Resuming at batch 3 with committed offsets {KafkaV2[Subscribe[gold-news]]: {"gold-news":{"0":40}}} and available offsets {KafkaV2[Subscribe[gold-news]]: {"gold-news":{"0":40}}}
[2025-05-12T19:50:01.772+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[gold-news]]: {"gold-news":{"0":40}}}
[2025-05-12T19:50:01.834+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO AdminClientConfig: AdminClientConfig values:
[2025-05-12T19:50:01.835+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T19:50:01.836+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T19:50:01.836+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T19:50:01.837+0000] {subprocess.py:93} INFO - 	client.id =
[2025-05-12T19:50:01.837+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-05-12T19:50:01.838+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T19:50:01.838+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T19:50:01.839+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T19:50:01.839+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T19:50:01.839+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T19:50:01.840+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T19:50:01.840+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T19:50:01.840+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T19:50:01.841+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T19:50:01.841+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T19:50:01.842+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-05-12T19:50:01.842+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T19:50:01.843+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T19:50:01.843+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T19:50:01.845+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T19:50:01.848+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T19:50:01.849+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T19:50:01.850+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T19:50:01.851+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T19:50:01.851+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T19:50:01.852+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T19:50:01.852+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T19:50:01.853+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T19:50:01.853+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T19:50:01.853+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T19:50:01.854+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T19:50:01.854+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T19:50:01.854+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T19:50:01.855+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T19:50:01.855+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T19:50:01.855+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T19:50:01.856+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T19:50:01.856+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T19:50:01.857+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T19:50:01.857+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T19:50:01.858+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T19:50:01.858+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T19:50:01.858+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T19:50:01.859+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T19:50:01.859+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T19:50:01.860+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T19:50:01.862+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T19:50:01.864+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T19:50:01.864+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T19:50:01.865+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T19:50:01.865+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T19:50:01.865+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T19:50:01.866+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T19:50:01.866+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T19:50:01.867+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T19:50:01.867+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T19:50:01.868+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T19:50:01.868+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T19:50:01.868+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T19:50:01.869+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T19:50:01.869+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T19:50:01.869+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T19:50:01.870+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T19:50:01.870+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T19:50:01.870+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T19:50:01.871+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T19:50:01.871+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T19:50:01.871+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T19:50:01.872+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T19:50:01.872+0000] {subprocess.py:93} INFO - 
[2025-05-12T19:50:01.992+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-05-12T19:50:01.996+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T19:50:01.998+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T19:50:02.000+0000] {subprocess.py:93} INFO - 25/05/12 19:50:01 INFO AppInfoParser: Kafka startTimeMs: 1747079401991
[2025-05-12T19:50:02.575+0000] {subprocess.py:93} INFO - 25/05/12 19:50:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/impact_analysis/offsets/3 using temp file file:/tmp/spark-checkpoint/impact_analysis/offsets/.3.a0119480-d574-4a30-8157-4778a0f6ec4f.tmp
[2025-05-12T19:50:02.606+0000] {subprocess.py:93} INFO - 25/05/12 19:50:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/impact_analysis/offsets/.3.a0119480-d574-4a30-8157-4778a0f6ec4f.tmp to file:/tmp/spark-checkpoint/impact_analysis/offsets/3
[2025-05-12T19:50:02.606+0000] {subprocess.py:93} INFO - 25/05/12 19:50:02 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1747079402555,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-05-12T19:50:03.279+0000] {subprocess.py:93} INFO - 25/05/12 19:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T19:50:03.375+0000] {subprocess.py:93} INFO - 25/05/12 19:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T19:50:03.499+0000] {subprocess.py:93} INFO - 25/05/12 19:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T19:50:03.502+0000] {subprocess.py:93} INFO - 25/05/12 19:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T19:50:04.247+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO CodeGenerator: Code generated in 348.36348 ms
[2025-05-12T19:50:04.394+0000] {subprocess.py:93} INFO - Python version: 3.8.10 (default, Nov 22 2023, 10:22:35)
[2025-05-12T19:50:04.395+0000] {subprocess.py:93} INFO - [GCC 9.4.0]
[2025-05-12T19:50:04.397+0000] {subprocess.py:93} INFO - Python executable: /usr/bin/python3.8
[2025-05-12T19:50:04.398+0000] {subprocess.py:93} INFO - Processing batch 3
[2025-05-12T19:50:04.447+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO CodeGenerator: Code generated in 13.493923 ms
[2025-05-12T19:50:04.478+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-05-12T19:50:04.502+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-05-12T19:50:04.504+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-05-12T19:50:04.505+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T19:50:04.510+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO DAGScheduler: Missing parents: List()
[2025-05-12T19:50:04.515+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at start at <unknown>:0), which has no missing parents
[2025-05-12T19:50:04.791+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 41.1 KiB, free 434.4 MiB)
[2025-05-12T19:50:04.822+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.0 KiB, free 434.3 MiB)
[2025-05-12T19:50:04.828+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 407aab94b4b5:36387 (size: 16.0 KiB, free: 434.4 MiB)
[2025-05-12T19:50:04.833+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-05-12T19:50:04.855+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-05-12T19:50:04.857+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-05-12T19:50:04.918+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (407aab94b4b5, executor driver, partition 0, PROCESS_LOCAL, 13811 bytes)
[2025-05-12T19:50:04.946+0000] {subprocess.py:93} INFO - 25/05/12 19:50:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-05-12T19:50:05.205+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO CodeGenerator: Code generated in 69.522755 ms
[2025-05-12T19:50:05.241+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO CodeGenerator: Code generated in 31.258293 ms
[2025-05-12T19:50:05.269+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO CodeGenerator: Code generated in 15.22435 ms
[2025-05-12T19:50:05.273+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=40 untilOffset=50, for query queryId=456a207a-18f1-456a-87bc-ab7592296d01 batchId=3 taskId=0 partitionId=0
[2025-05-12T19:50:05.405+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO CodeGenerator: Code generated in 25.442407 ms
[2025-05-12T19:50:05.458+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO CodeGenerator: Code generated in 35.579747 ms
[2025-05-12T19:50:05.495+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO ConsumerConfig: ConsumerConfig values:
[2025-05-12T19:50:05.497+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-05-12T19:50:05.498+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-05-12T19:50:05.498+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T19:50:05.499+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-05-12T19:50:05.499+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T19:50:05.500+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-05-12T19:50:05.500+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T19:50:05.501+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1
[2025-05-12T19:50:05.501+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-05-12T19:50:05.501+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-05-12T19:50:05.502+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T19:50:05.502+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-05-12T19:50:05.503+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-05-12T19:50:05.504+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-05-12T19:50:05.504+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-05-12T19:50:05.504+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-05-12T19:50:05.505+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor
[2025-05-12T19:50:05.505+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-05-12T19:50:05.505+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-05-12T19:50:05.506+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-05-12T19:50:05.506+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-05-12T19:50:05.506+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-05-12T19:50:05.507+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-05-12T19:50:05.507+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T19:50:05.508+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-05-12T19:50:05.510+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-05-12T19:50:05.511+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-05-12T19:50:05.512+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T19:50:05.513+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T19:50:05.513+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T19:50:05.513+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T19:50:05.514+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T19:50:05.514+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-05-12T19:50:05.514+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T19:50:05.515+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T19:50:05.515+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T19:50:05.516+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T19:50:05.516+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T19:50:05.516+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T19:50:05.516+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T19:50:05.517+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T19:50:05.517+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T19:50:05.518+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T19:50:05.518+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T19:50:05.518+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T19:50:05.518+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T19:50:05.519+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T19:50:05.519+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T19:50:05.519+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T19:50:05.520+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T19:50:05.520+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T19:50:05.520+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T19:50:05.520+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T19:50:05.521+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T19:50:05.521+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T19:50:05.521+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T19:50:05.522+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T19:50:05.522+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T19:50:05.522+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T19:50:05.524+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T19:50:05.525+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T19:50:05.526+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T19:50:05.527+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T19:50:05.528+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T19:50:05.528+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T19:50:05.529+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T19:50:05.529+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T19:50:05.529+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T19:50:05.530+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T19:50:05.530+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-05-12T19:50:05.531+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T19:50:05.531+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T19:50:05.531+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T19:50:05.532+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T19:50:05.532+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T19:50:05.532+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T19:50:05.533+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T19:50:05.533+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T19:50:05.534+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T19:50:05.534+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T19:50:05.535+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T19:50:05.535+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T19:50:05.536+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T19:50:05.536+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T19:50:05.537+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T19:50:05.537+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T19:50:05.538+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T19:50:05.539+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T19:50:05.541+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T19:50:05.542+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T19:50:05.543+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T19:50:05.543+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T19:50:05.544+0000] {subprocess.py:93} INFO - 
[2025-05-12T19:50:05.611+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T19:50:05.612+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T19:50:05.613+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO AppInfoParser: Kafka startTimeMs: 1747079405609
[2025-05-12T19:50:05.621+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Assigned to partition(s): gold-news-0
[2025-05-12T19:50:05.642+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Seeking to offset 40 for partition gold-news-0
[2025-05-12T19:50:05.664+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Resetting the last seen epoch of partition gold-news-0 to 0 since the associated topicId changed from null to kIXDhw6vSqatE61G-o9I0g
[2025-05-12T19:50:05.666+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Cluster ID: HTSY0p8VS7eCaEFoyzBH3g
[2025-05-12T19:50:05.778+0000] {subprocess.py:93} INFO - 25/05/12 19:50:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T19:50:06.284+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T19:50:06.285+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T19:50:06.292+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=50, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T19:50:06.724+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor read 1 records through 1 polls (polled  out 10 records), taking 651042069 nanos, during time span of 1091823651 nanos.
[2025-05-12T19:50:06.815+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1957 bytes result sent to driver
[2025-05-12T19:50:06.857+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1955 ms on 407aab94b4b5 (executor driver) (1/1)
[2025-05-12T19:50:06.864+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-05-12T19:50:06.884+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 2.342 s
[2025-05-12T19:50:06.894+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T19:50:06.895+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-05-12T19:50:06.903+0000] {subprocess.py:93} INFO - 25/05/12 19:50:06 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 2.421117 s
[2025-05-12T19:50:07.171+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO CodeGenerator: Code generated in 53.072591 ms
[2025-05-12T19:50:07.241+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T19:50:07.246+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO DAGScheduler: Got job 1 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T19:50:07.252+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO DAGScheduler: Final stage: ResultStage 1 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T19:50:07.255+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T19:50:07.257+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO DAGScheduler: Missing parents: List()
[2025-05-12T19:50:07.262+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T19:50:07.275+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 42.9 KiB, free 434.3 MiB)
[2025-05-12T19:50:07.362+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 434.3 MiB)
[2025-05-12T19:50:07.371+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 407aab94b4b5:36387 (size: 16.4 KiB, free: 434.4 MiB)
[2025-05-12T19:50:07.393+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-05-12T19:50:07.398+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T19:50:07.403+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-05-12T19:50:07.421+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (407aab94b4b5, executor driver, partition 0, PROCESS_LOCAL, 13811 bytes)
[2025-05-12T19:50:07.429+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-05-12T19:50:07.567+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 407aab94b4b5:36387 in memory (size: 16.0 KiB, free: 434.4 MiB)
[2025-05-12T19:50:07.628+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO CodeGenerator: Code generated in 48.671151 ms
[2025-05-12T19:50:07.639+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=40 untilOffset=50, for query queryId=456a207a-18f1-456a-87bc-ab7592296d01 batchId=3 taskId=1 partitionId=0
[2025-05-12T19:50:07.669+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Seeking to offset 40 for partition gold-news-0
[2025-05-12T19:50:07.675+0000] {subprocess.py:93} INFO - 25/05/12 19:50:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T19:50:08.176+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T19:50:08.177+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T19:50:08.178+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=50, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T19:50:08.199+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor read 10 records through 1 polls (polled  out 10 records), taking 512811055 nanos, during time span of 531849527 nanos.
[2025-05-12T19:50:08.202+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 6039 bytes result sent to driver
[2025-05-12T19:50:08.205+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 810 ms on 407aab94b4b5 (executor driver) (1/1)
[2025-05-12T19:50:08.206+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-05-12T19:50:08.210+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO DAGScheduler: ResultStage 1 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.945 s
[2025-05-12T19:50:08.211+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T19:50:08.212+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-05-12T19:50:08.214+0000] {subprocess.py:93} INFO - 25/05/12 19:50:08 INFO DAGScheduler: Job 1 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.969080 s
[2025-05-12T19:50:09.415+0000] {subprocess.py:93} INFO - 25/05/12 19:50:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 407aab94b4b5:36387 in memory (size: 16.4 KiB, free: 434.4 MiB)
[2025-05-12T19:50:11.108+0000] {subprocess.py:93} INFO - 25/05/12 19:50:11 INFO DefaultMavenCoordinates: DataStax Java driver for Apache Cassandra(R) (com.datastax.oss:java-driver-core-shaded) version 4.13.0
[2025-05-12T19:50:11.358+0000] {subprocess.py:93} INFO - 25/05/12 19:50:11 INFO Native: Unable to load JNR native implementation. This could be normal if JNR is excluded from the classpath
[2025-05-12T19:50:11.359+0000] {subprocess.py:93} INFO - java.lang.NoClassDefFoundError: jnr/posix/POSIXHandler
[2025-05-12T19:50:11.359+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native$LibcLoader.load(Native.java:42)
[2025-05-12T19:50:11.360+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native.<clinit>(Native.java:59)
[2025-05-12T19:50:11.360+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.Clock.getInstance(Clock.java:41)
[2025-05-12T19:50:11.360+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.buildClock(MonotonicTimestampGenerator.java:109)
[2025-05-12T19:50:11.361+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.<init>(MonotonicTimestampGenerator.java:43)
[2025-05-12T19:50:11.361+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.AtomicTimestampGenerator.<init>(AtomicTimestampGenerator.java:52)
[2025-05-12T19:50:11.362+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-12T19:50:11.362+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-12T19:50:11.363+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-12T19:50:11.364+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-12T19:50:11.364+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.resolveClass(Reflection.java:329)
[2025-05-12T19:50:11.364+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:235)
[2025-05-12T19:50:11.365+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:110)
[2025-05-12T19:50:11.366+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.buildTimestampGenerator(DefaultDriverContext.java:377)
[2025-05-12T19:50:11.366+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.concurrent.LazyReference.get(LazyReference.java:55)
[2025-05-12T19:50:11.366+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.getTimestampGenerator(DefaultDriverContext.java:773)
[2025-05-12T19:50:11.367+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.init(DefaultSession.java:349)
[2025-05-12T19:50:11.367+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.access$1100(DefaultSession.java:300)
[2025-05-12T19:50:11.367+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession.lambda$init$0(DefaultSession.java:146)
[2025-05-12T19:50:11.368+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
[2025-05-12T19:50:11.368+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
[2025-05-12T19:50:11.370+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)
[2025-05-12T19:50:11.372+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2025-05-12T19:50:11.373+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-05-12T19:50:11.374+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-05-12T19:50:11.375+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-12T19:50:11.375+0000] {subprocess.py:93} INFO - Caused by: java.lang.ClassNotFoundException: jnr.posix.POSIXHandler
[2025-05-12T19:50:11.375+0000] {subprocess.py:93} INFO - 	at java.base/java.net.URLClassLoader.findClass(Unknown Source)
[2025-05-12T19:50:11.376+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
[2025-05-12T19:50:11.376+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
[2025-05-12T19:50:11.376+0000] {subprocess.py:93} INFO - 	... 26 more
[2025-05-12T19:50:11.377+0000] {subprocess.py:93} INFO - 25/05/12 19:50:11 INFO Clock: Could not access native clock (see debug logs for details), falling back to Java system clock
[2025-05-12T19:50:15.195+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO CassandraConnector: Connected to Cassandra cluster.
[2025-05-12T19:50:15.529+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO CodeGenerator: Code generated in 14.588473 ms
[2025-05-12T19:50:15.547+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO AppendDataExec: Start processing data source write support: CassandraBulkWrite(org.apache.spark.sql.SparkSession@f2d2bff,com.datastax.spark.connector.cql.CassandraConnector@5c8a5e7,TableDef(gold_news,raw_news,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,false),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@7fb95e9c). The input RDD has 8 partitions.
[2025-05-12T19:50:15.622+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO SparkContext: Starting job: save at <unknown>:0
[2025-05-12T19:50:15.626+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO DAGScheduler: Got job 2 (save at <unknown>:0) with 8 output partitions
[2025-05-12T19:50:15.627+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO DAGScheduler: Final stage: ResultStage 2 (save at <unknown>:0)
[2025-05-12T19:50:15.629+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T19:50:15.631+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO DAGScheduler: Missing parents: List()
[2025-05-12T19:50:15.632+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at save at <unknown>:0), which has no missing parents
[2025-05-12T19:50:15.649+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 21.0 KiB, free 434.4 MiB)
[2025-05-12T19:50:15.667+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 434.4 MiB)
[2025-05-12T19:50:15.674+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 407aab94b4b5:36387 (size: 10.5 KiB, free: 434.4 MiB)
[2025-05-12T19:50:15.677+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-05-12T19:50:15.679+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at save at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T19:50:15.679+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 8 tasks resource profile 0
[2025-05-12T19:50:15.688+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (407aab94b4b5, executor driver, partition 0, PROCESS_LOCAL, 13241 bytes)
[2025-05-12T19:50:15.692+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (407aab94b4b5, executor driver, partition 1, PROCESS_LOCAL, 13411 bytes)
[2025-05-12T19:50:15.697+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4) (407aab94b4b5, executor driver, partition 2, PROCESS_LOCAL, 13489 bytes)
[2025-05-12T19:50:15.706+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5) (407aab94b4b5, executor driver, partition 3, PROCESS_LOCAL, 14019 bytes)
[2025-05-12T19:50:15.709+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 6) (407aab94b4b5, executor driver, partition 4, PROCESS_LOCAL, 13473 bytes)
[2025-05-12T19:50:15.710+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 7) (407aab94b4b5, executor driver, partition 5, PROCESS_LOCAL, 13351 bytes)
[2025-05-12T19:50:15.711+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 8) (407aab94b4b5, executor driver, partition 6, PROCESS_LOCAL, 13358 bytes)
[2025-05-12T19:50:15.713+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 9) (407aab94b4b5, executor driver, partition 7, PROCESS_LOCAL, 13728 bytes)
[2025-05-12T19:50:15.714+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-05-12T19:50:15.735+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
[2025-05-12T19:50:15.739+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO Executor: Running task 2.0 in stage 2.0 (TID 4)
[2025-05-12T19:50:15.754+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO Executor: Running task 3.0 in stage 2.0 (TID 5)
[2025-05-12T19:50:15.755+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO Executor: Running task 4.0 in stage 2.0 (TID 6)
[2025-05-12T19:50:15.814+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO Executor: Running task 6.0 in stage 2.0 (TID 8)
[2025-05-12T19:50:15.823+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO Executor: Running task 5.0 in stage 2.0 (TID 7)
[2025-05-12T19:50:15.835+0000] {subprocess.py:93} INFO - 25/05/12 19:50:15 INFO Executor: Running task 7.0 in stage 2.0 (TID 9)
[2025-05-12T19:50:17.000+0000] {subprocess.py:93} INFO - 25/05/12 19:50:16 INFO CodeGenerator: Code generated in 20.377498 ms
[2025-05-12T19:50:17.693+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO PythonRunner: Times: total = 1106, boot = 1033, init = 73, finish = 0
[2025-05-12T19:50:17.696+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO PythonRunner: Times: total = 1096, boot = 1046, init = 50, finish = 0
[2025-05-12T19:50:17.697+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO PythonRunner: Times: total = 1107, boot = 1051, init = 55, finish = 1
[2025-05-12T19:50:17.699+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO PythonRunner: Times: total = 1065, boot = 1001, init = 64, finish = 0
[2025-05-12T19:50:17.704+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO PythonRunner: Times: total = 1096, boot = 1029, init = 67, finish = 0
[2025-05-12T19:50:17.705+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO PythonRunner: Times: total = 1076, boot = 1015, init = 61, finish = 0
[2025-05-12T19:50:17.706+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO PythonRunner: Times: total = 1065, boot = 1007, init = 58, finish = 0
[2025-05-12T19:50:17.712+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO PythonRunner: Times: total = 1065, boot = 996, init = 69, finish = 0
[2025-05-12T19:50:17.719+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Commit authorized for partition 6 (task 8, attempt 0, stage 2.0)
[2025-05-12T19:50:17.720+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Commit authorized for partition 7 (task 9, attempt 0, stage 2.0)
[2025-05-12T19:50:17.720+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Commit authorized for partition 4 (task 6, attempt 0, stage 2.0)
[2025-05-12T19:50:17.721+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 2, attempt 0, stage 2.0)
[2025-05-12T19:50:17.722+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Commit authorized for partition 5 (task 7, attempt 0, stage 2.0)
[2025-05-12T19:50:17.723+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Commit authorized for partition 1 (task 3, attempt 0, stage 2.0)
[2025-05-12T19:50:17.723+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Commit authorized for partition 2 (task 4, attempt 0, stage 2.0)
[2025-05-12T19:50:17.724+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Commit authorized for partition 3 (task 5, attempt 0, stage 2.0)
[2025-05-12T19:50:17.801+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Committed partition 1 (task 3, attempt 0, stage 2.0)
[2025-05-12T19:50:17.948+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1939 bytes result sent to driver
[2025-05-12T19:50:17.950+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 2189 ms on 407aab94b4b5 (executor driver) (1/8)
[2025-05-12T19:50:17.950+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2025-05-12T19:50:17.954+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 53025
[2025-05-12T19:50:17.955+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Committed partition 7 (task 9, attempt 0, stage 2.0)
[2025-05-12T19:50:17.976+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Committed partition 5 (task 7, attempt 0, stage 2.0)
[2025-05-12T19:50:17.982+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Committed partition 6 (task 8, attempt 0, stage 2.0)
[2025-05-12T19:50:17.983+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO Executor: Finished task 5.0 in stage 2.0 (TID 7). 1896 bytes result sent to driver
[2025-05-12T19:50:17.985+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Committed partition 3 (task 5, attempt 0, stage 2.0)
[2025-05-12T19:50:17.985+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1896 bytes result sent to driver
[2025-05-12T19:50:17.986+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO Executor: Finished task 7.0 in stage 2.0 (TID 9). 1896 bytes result sent to driver
[2025-05-12T19:50:17.989+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Committed partition 2 (task 4, attempt 0, stage 2.0)
[2025-05-12T19:50:17.993+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO Executor: Finished task 3.0 in stage 2.0 (TID 5). 1896 bytes result sent to driver
[2025-05-12T19:50:17.994+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO Executor: Finished task 2.0 in stage 2.0 (TID 4). 1896 bytes result sent to driver
[2025-05-12T19:50:17.995+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO DataWritingSparkTask: Committed partition 4 (task 6, attempt 0, stage 2.0)
[2025-05-12T19:50:17.995+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 9) in 2248 ms on 407aab94b4b5 (executor driver) (2/8)
[2025-05-12T19:50:17.996+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2273 ms on 407aab94b4b5 (executor driver) (3/8)
[2025-05-12T19:50:17.996+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 5) in 2261 ms on 407aab94b4b5 (executor driver) (4/8)
[2025-05-12T19:50:17.997+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO Executor: Finished task 4.0 in stage 2.0 (TID 6). 1896 bytes result sent to driver
[2025-05-12T19:50:17.998+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO Executor: Finished task 6.0 in stage 2.0 (TID 8). 1896 bytes result sent to driver
[2025-05-12T19:50:17.999+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 6) in 2268 ms on 407aab94b4b5 (executor driver) (5/8)
[2025-05-12T19:50:17.999+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 7) in 2267 ms on 407aab94b4b5 (executor driver) (6/8)
[2025-05-12T19:50:18.000+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 2279 ms on 407aab94b4b5 (executor driver) (7/8)
[2025-05-12T19:50:18.001+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 8) in 2274 ms on 407aab94b4b5 (executor driver) (8/8)
[2025-05-12T19:50:18.003+0000] {subprocess.py:93} INFO - 25/05/12 19:50:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-05-12T19:50:18.010+0000] {subprocess.py:93} INFO - 25/05/12 19:50:18 INFO DAGScheduler: ResultStage 2 (save at <unknown>:0) finished in 2.372 s
[2025-05-12T19:50:18.012+0000] {subprocess.py:93} INFO - 25/05/12 19:50:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T19:50:18.013+0000] {subprocess.py:93} INFO - 25/05/12 19:50:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-05-12T19:50:18.013+0000] {subprocess.py:93} INFO - 25/05/12 19:50:18 INFO DAGScheduler: Job 2 finished: save at <unknown>:0, took 2.387785 s
[2025-05-12T19:50:18.017+0000] {subprocess.py:93} INFO - 25/05/12 19:50:18 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@f2d2bff,com.datastax.spark.connector.cql.CassandraConnector@5c8a5e7,TableDef(gold_news,raw_news,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,false),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@7fb95e9c) is committing.
[2025-05-12T19:50:18.024+0000] {subprocess.py:93} INFO - 25/05/12 19:50:18 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@f2d2bff,com.datastax.spark.connector.cql.CassandraConnector@5c8a5e7,TableDef(gold_news,raw_news,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,false),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@7fb95e9c) committed.
[2025-05-12T19:50:18.036+0000] {subprocess.py:93} INFO - Batch 3 : 10 news brutes écrites dans gold_news.raw_news
[2025-05-12T19:50:18.037+0000] {subprocess.py:93} INFO - Batch 3 : 10 descriptions valides pour analyse
[2025-05-12T19:50:20.517+0000] {subprocess.py:93} INFO - Analyse Gemini pour batch 3 : {
[2025-05-12T19:50:20.518+0000] {subprocess.py:93} INFO -   "impact": 1,
[2025-05-12T19:50:20.518+0000] {subprocess.py:93} INFO -   "confidence": 0.8,
[2025-05-12T19:50:20.519+0000] {subprocess.py:93} INFO -   "explanation": "Plusieurs articles mentionnent la volatilit\u00e9 des march\u00e9s et la hausse potentielle de l'or en cas de fuite de capitaux des actifs am\u00e9ricains.  L'article \"In Volatile Markets, RWAs Like Gold Are A Lifeline\" souligne explicitement l'or comme valeur refuge en p\u00e9riode d'incertitude.  Bien que d'autres news soient hors sujet, la pr\u00e9sence de ces \u00e9l\u00e9ments sugg\u00e8re une influence potentielle sur le prix de l'or."
[2025-05-12T19:50:20.520+0000] {subprocess.py:93} INFO - }
[2025-05-12T19:50:20.686+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO CodeGenerator: Code generated in 13.610295 ms
[2025-05-12T19:50:20.693+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO AppendDataExec: Start processing data source write support: CassandraBulkWrite(org.apache.spark.sql.SparkSession@f2d2bff,com.datastax.spark.connector.cql.CassandraConnector@5fbafb02,TableDef(gold_news,impact_analysis,ArrayBuffer(ColumnDef(window_timestamp,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(confidence,RegularColumn,DoubleType), ColumnDef(explanation,RegularColumn,VarCharType), ColumnDef(impact,RegularColumn,IntType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(confidence,DoubleType,true),StructField(explanation,StringType,true),StructField(impact,LongType,true),StructField(window_timestamp,StringType,true)),org.apache.spark.SparkConf@77fbad03). The input RDD has 8 partitions.
[2025-05-12T19:50:20.698+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO SparkContext: Starting job: save at <unknown>:0
[2025-05-12T19:50:20.700+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DAGScheduler: Got job 3 (save at <unknown>:0) with 8 output partitions
[2025-05-12T19:50:20.701+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DAGScheduler: Final stage: ResultStage 3 (save at <unknown>:0)
[2025-05-12T19:50:20.702+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T19:50:20.704+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DAGScheduler: Missing parents: List()
[2025-05-12T19:50:20.705+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[20] at save at <unknown>:0), which has no missing parents
[2025-05-12T19:50:20.717+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 20.4 KiB, free 434.3 MiB)
[2025-05-12T19:50:20.762+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 434.3 MiB)
[2025-05-12T19:50:20.763+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 407aab94b4b5:36387 (size: 10.5 KiB, free: 434.4 MiB)
[2025-05-12T19:50:20.764+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 407aab94b4b5:36387 in memory (size: 10.5 KiB, free: 434.4 MiB)
[2025-05-12T19:50:20.765+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-05-12T19:50:20.768+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 3 (MapPartitionsRDD[20] at save at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T19:50:20.770+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 8 tasks resource profile 0
[2025-05-12T19:50:20.773+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 10) (407aab94b4b5, executor driver, partition 0, PROCESS_LOCAL, 12854 bytes)
[2025-05-12T19:50:20.775+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 11) (407aab94b4b5, executor driver, partition 1, PROCESS_LOCAL, 12854 bytes)
[2025-05-12T19:50:20.776+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 12) (407aab94b4b5, executor driver, partition 2, PROCESS_LOCAL, 12854 bytes)
[2025-05-12T19:50:20.777+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 13) (407aab94b4b5, executor driver, partition 3, PROCESS_LOCAL, 12854 bytes)
[2025-05-12T19:50:20.779+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 14) (407aab94b4b5, executor driver, partition 4, PROCESS_LOCAL, 12854 bytes)
[2025-05-12T19:50:20.780+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 15) (407aab94b4b5, executor driver, partition 5, PROCESS_LOCAL, 12854 bytes)
[2025-05-12T19:50:20.785+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 16) (407aab94b4b5, executor driver, partition 6, PROCESS_LOCAL, 12854 bytes)
[2025-05-12T19:50:20.788+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 17) (407aab94b4b5, executor driver, partition 7, PROCESS_LOCAL, 13333 bytes)
[2025-05-12T19:50:20.800+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 10)
[2025-05-12T19:50:20.815+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Running task 1.0 in stage 3.0 (TID 11)
[2025-05-12T19:50:20.819+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Running task 3.0 in stage 3.0 (TID 13)
[2025-05-12T19:50:20.819+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Running task 6.0 in stage 3.0 (TID 16)
[2025-05-12T19:50:20.820+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Running task 5.0 in stage 3.0 (TID 15)
[2025-05-12T19:50:20.821+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Running task 7.0 in stage 3.0 (TID 17)
[2025-05-12T19:50:20.824+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Running task 4.0 in stage 3.0 (TID 14)
[2025-05-12T19:50:20.826+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Running task 2.0 in stage 3.0 (TID 12)
[2025-05-12T19:50:20.877+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO CodeGenerator: Code generated in 47.198498 ms
[2025-05-12T19:50:20.931+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO PythonRunner: Times: total = 77, boot = -3772, init = 3849, finish = 0
[2025-05-12T19:50:20.932+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Commit authorized for partition 6 (task 16, attempt 0, stage 3.0)
[2025-05-12T19:50:20.934+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO PythonRunner: Times: total = 56, boot = -3740, init = 3796, finish = 0
[2025-05-12T19:50:20.938+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 10, attempt 0, stage 3.0)
[2025-05-12T19:50:20.939+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 3.0)
[2025-05-12T19:50:20.940+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO PythonRunner: Times: total = 68, boot = -3749, init = 3816, finish = 1
[2025-05-12T19:50:20.941+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Commit authorized for partition 3 (task 13, attempt 0, stage 3.0)
[2025-05-12T19:50:20.941+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO PythonRunner: Times: total = 70, boot = -3801, init = 3871, finish = 0
[2025-05-12T19:50:20.942+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 10). 1853 bytes result sent to driver
[2025-05-12T19:50:20.943+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO PythonRunner: Times: total = 67, boot = -3790, init = 3857, finish = 0
[2025-05-12T19:50:20.964+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Committed partition 6 (task 16, attempt 0, stage 3.0)
[2025-05-12T19:50:20.968+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Commit authorized for partition 2 (task 12, attempt 0, stage 3.0)
[2025-05-12T19:50:20.969+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Committed partition 2 (task 12, attempt 0, stage 3.0)
[2025-05-12T19:50:20.970+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO PythonRunner: Times: total = 58, boot = -3748, init = 3806, finish = 0
[2025-05-12T19:50:20.976+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Commit authorized for partition 5 (task 15, attempt 0, stage 3.0)
[2025-05-12T19:50:20.977+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Finished task 6.0 in stage 3.0 (TID 16). 1896 bytes result sent to driver
[2025-05-12T19:50:20.977+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Committed partition 5 (task 15, attempt 0, stage 3.0)
[2025-05-12T19:50:20.978+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO PythonRunner: Times: total = 48, boot = -3784, init = 3832, finish = 0
[2025-05-12T19:50:20.979+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Committed partition 3 (task 13, attempt 0, stage 3.0)
[2025-05-12T19:50:20.979+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 10) in 176 ms on 407aab94b4b5 (executor driver) (1/8)
[2025-05-12T19:50:20.983+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Commit authorized for partition 1 (task 11, attempt 0, stage 3.0)
[2025-05-12T19:50:20.984+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Commit authorized for partition 4 (task 14, attempt 0, stage 3.0)
[2025-05-12T19:50:20.985+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Committed partition 1 (task 11, attempt 0, stage 3.0)
[2025-05-12T19:50:20.986+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Committed partition 4 (task 14, attempt 0, stage 3.0)
[2025-05-12T19:50:20.987+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Finished task 2.0 in stage 3.0 (TID 12). 1896 bytes result sent to driver
[2025-05-12T19:50:20.988+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Finished task 3.0 in stage 3.0 (TID 13). 1853 bytes result sent to driver
[2025-05-12T19:50:20.989+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Finished task 1.0 in stage 3.0 (TID 11). 1853 bytes result sent to driver
[2025-05-12T19:50:20.993+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 16) in 195 ms on 407aab94b4b5 (executor driver) (2/8)
[2025-05-12T19:50:20.995+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 12) in 212 ms on 407aab94b4b5 (executor driver) (3/8)
[2025-05-12T19:50:20.998+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Finished task 4.0 in stage 3.0 (TID 14). 1853 bytes result sent to driver
[2025-05-12T19:50:21.002+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 11) in 216 ms on 407aab94b4b5 (executor driver) (4/8)
[2025-05-12T19:50:21.003+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 13) in 213 ms on 407aab94b4b5 (executor driver) (5/8)
[2025-05-12T19:50:21.004+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO PythonRunner: Times: total = 72, boot = -3739, init = 3810, finish = 1
[2025-05-12T19:50:21.005+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 14) in 215 ms on 407aab94b4b5 (executor driver) (6/8)
[2025-05-12T19:50:21.007+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO DataWritingSparkTask: Commit authorized for partition 7 (task 17, attempt 0, stage 3.0)
[2025-05-12T19:50:21.014+0000] {subprocess.py:93} INFO - 25/05/12 19:50:20 INFO Executor: Finished task 5.0 in stage 3.0 (TID 15). 1853 bytes result sent to driver
[2025-05-12T19:50:21.015+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 15) in 223 ms on 407aab94b4b5 (executor driver) (7/8)
[2025-05-12T19:50:21.015+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO DataWritingSparkTask: Committed partition 7 (task 17, attempt 0, stage 3.0)
[2025-05-12T19:50:21.016+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO Executor: Finished task 7.0 in stage 3.0 (TID 17). 1853 bytes result sent to driver
[2025-05-12T19:50:21.017+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 17) in 228 ms on 407aab94b4b5 (executor driver) (8/8)
[2025-05-12T19:50:21.018+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-05-12T19:50:21.018+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO DAGScheduler: ResultStage 3 (save at <unknown>:0) finished in 0.305 s
[2025-05-12T19:50:21.019+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T19:50:21.019+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-05-12T19:50:21.020+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO DAGScheduler: Job 3 finished: save at <unknown>:0, took 0.319892 s
[2025-05-12T19:50:21.021+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@f2d2bff,com.datastax.spark.connector.cql.CassandraConnector@5fbafb02,TableDef(gold_news,impact_analysis,ArrayBuffer(ColumnDef(window_timestamp,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(confidence,RegularColumn,DoubleType), ColumnDef(explanation,RegularColumn,VarCharType), ColumnDef(impact,RegularColumn,IntType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(confidence,DoubleType,true),StructField(explanation,StringType,true),StructField(impact,LongType,true),StructField(window_timestamp,StringType,true)),org.apache.spark.SparkConf@77fbad03) is committing.
[2025-05-12T19:50:21.022+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@f2d2bff,com.datastax.spark.connector.cql.CassandraConnector@5fbafb02,TableDef(gold_news,impact_analysis,ArrayBuffer(ColumnDef(window_timestamp,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(confidence,RegularColumn,DoubleType), ColumnDef(explanation,RegularColumn,VarCharType), ColumnDef(impact,RegularColumn,IntType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(confidence,DoubleType,true),StructField(explanation,StringType,true),StructField(impact,LongType,true),StructField(window_timestamp,StringType,true)),org.apache.spark.SparkConf@77fbad03) committed.
[2025-05-12T19:50:21.023+0000] {subprocess.py:93} INFO - Batch 3 : Résultat d'impact écrit dans gold_news.impact_analysis
[2025-05-12T19:50:21.024+0000] {subprocess.py:93} INFO - Batch 3 processing completed
[2025-05-12T19:50:21.046+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/impact_analysis/commits/3 using temp file file:/tmp/spark-checkpoint/impact_analysis/commits/.3.5ddad620-af8d-4e45-8678-83ffe4b415df.tmp
[2025-05-12T19:50:21.079+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/impact_analysis/commits/.3.5ddad620-af8d-4e45-8678-83ffe4b415df.tmp to file:/tmp/spark-checkpoint/impact_analysis/commits/3
[2025-05-12T19:50:21.130+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO MicroBatchExecution: Streaming query made progress: {
[2025-05-12T19:50:21.131+0000] {subprocess.py:93} INFO -   "id" : "456a207a-18f1-456a-87bc-ab7592296d01",
[2025-05-12T19:50:21.132+0000] {subprocess.py:93} INFO -   "runId" : "0155082a-f1bc-4e0a-8ffa-c260073d080b",
[2025-05-12T19:50:21.137+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-05-12T19:50:21.138+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-05-12T19:50:01.656Z",
[2025-05-12T19:50:21.139+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-05-12T19:50:21.140+0000] {subprocess.py:93} INFO -   "numInputRows" : 11,
[2025-05-12T19:50:21.140+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-05-12T19:50:21.141+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.5663680362475543,
[2025-05-12T19:50:21.142+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-05-12T19:50:21.142+0000] {subprocess.py:93} INFO -     "addBatch" : 17572,
[2025-05-12T19:50:21.143+0000] {subprocess.py:93} INFO -     "commitOffsets" : 45,
[2025-05-12T19:50:21.144+0000] {subprocess.py:93} INFO -     "getBatch" : 4,
[2025-05-12T19:50:21.144+0000] {subprocess.py:93} INFO -     "latestOffset" : 781,
[2025-05-12T19:50:21.144+0000] {subprocess.py:93} INFO -     "queryPlanning" : 821,
[2025-05-12T19:50:21.145+0000] {subprocess.py:93} INFO -     "triggerExecution" : 19421,
[2025-05-12T19:50:21.146+0000] {subprocess.py:93} INFO -     "walCommit" : 54
[2025-05-12T19:50:21.146+0000] {subprocess.py:93} INFO -   },
[2025-05-12T19:50:21.147+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-05-12T19:50:21.147+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-05-12T19:50:21.148+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[gold-news]]",
[2025-05-12T19:50:21.150+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-05-12T19:50:21.151+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T19:50:21.152+0000] {subprocess.py:93} INFO -         "0" : 40
[2025-05-12T19:50:21.152+0000] {subprocess.py:93} INFO -       }
[2025-05-12T19:50:21.153+0000] {subprocess.py:93} INFO -     },
[2025-05-12T19:50:21.154+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-05-12T19:50:21.154+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T19:50:21.155+0000] {subprocess.py:93} INFO -         "0" : 50
[2025-05-12T19:50:21.156+0000] {subprocess.py:93} INFO -       }
[2025-05-12T19:50:21.156+0000] {subprocess.py:93} INFO -     },
[2025-05-12T19:50:21.157+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-05-12T19:50:21.157+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T19:50:21.158+0000] {subprocess.py:93} INFO -         "0" : 50
[2025-05-12T19:50:21.160+0000] {subprocess.py:93} INFO -       }
[2025-05-12T19:50:21.160+0000] {subprocess.py:93} INFO -     },
[2025-05-12T19:50:21.161+0000] {subprocess.py:93} INFO -     "numInputRows" : 11,
[2025-05-12T19:50:21.165+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-05-12T19:50:21.166+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.5663680362475543,
[2025-05-12T19:50:21.167+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-05-12T19:50:21.168+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-05-12T19:50:21.168+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-05-12T19:50:21.169+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-05-12T19:50:21.169+0000] {subprocess.py:93} INFO -     }
[2025-05-12T19:50:21.170+0000] {subprocess.py:93} INFO -   } ],
[2025-05-12T19:50:21.170+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-05-12T19:50:21.171+0000] {subprocess.py:93} INFO -     "description" : "ForeachBatchSink",
[2025-05-12T19:50:21.171+0000] {subprocess.py:93} INFO -     "numOutputRows" : -1
[2025-05-12T19:50:21.172+0000] {subprocess.py:93} INFO -   }
[2025-05-12T19:50:21.172+0000] {subprocess.py:93} INFO - }
[2025-05-12T19:50:21.173+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2025-05-12T19:50:21.173+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO Metrics: Metrics scheduler closed
[2025-05-12T19:50:21.174+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T19:50:21.174+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO Metrics: Metrics reporters closed
[2025-05-12T19:50:21.175+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO MicroBatchExecution: Async log purge executor pool for query [id = 456a207a-18f1-456a-87bc-ab7592296d01, runId = 0155082a-f1bc-4e0a-8ffa-c260073d080b] has been shutdown
[2025-05-12T19:50:21.182+0000] {subprocess.py:93} INFO - Streaming query completed successfully
[2025-05-12T19:50:21.183+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-05-12T19:50:21.208+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO SparkUI: Stopped Spark web UI at http://407aab94b4b5:4040
[2025-05-12T19:50:21.267+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-05-12T19:50:21.295+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO MemoryStore: MemoryStore cleared
[2025-05-12T19:50:21.296+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO BlockManager: BlockManager stopped
[2025-05-12T19:50:21.300+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-05-12T19:50:21.307+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-05-12T19:50:21.324+0000] {subprocess.py:93} INFO - 25/05/12 19:50:21 INFO SparkContext: Successfully stopped SparkContext
[2025-05-12T19:50:22.022+0000] {subprocess.py:93} INFO - Spark session stopped
[2025-05-12T19:50:22.122+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-05-12T19:50:22.124+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1, groupId=spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-05-12T19:50:22.124+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO Metrics: Metrics scheduler closed
[2025-05-12T19:50:22.127+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T19:50:22.128+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO Metrics: Metrics reporters closed
[2025-05-12T19:50:22.130+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-7b652519-c048-4c95-8b1b-717695b21f89--977810735-executor-1 unregistered
[2025-05-12T19:50:22.135+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO ShutdownHookManager: Shutdown hook called
[2025-05-12T19:50:22.136+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858
[2025-05-12T19:50:22.138+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-f6489d7d-c9d9-4825-a889-c1eb63586858/pyspark-6ac98374-5c01-411b-bb45-248a58cfad31
[2025-05-12T19:50:22.155+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-3118be71-7b3e-492f-b434-6264bb96daaa
[2025-05-12T19:50:22.184+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO CassandraConnector: Disconnected from Cassandra cluster.
[2025-05-12T19:50:22.190+0000] {subprocess.py:93} INFO - 25/05/12 19:50:22 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
[2025-05-12T19:50:22.274+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2025-05-12T19:50:22.311+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=gold_news_pipeline, task_id=run_consumer, execution_date=20250512T194934, start_date=20250512T194943, end_date=20250512T195022
[2025-05-12T19:50:22.387+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-05-12T19:50:22.410+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
