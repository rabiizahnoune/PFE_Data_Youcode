[2025-05-12T17:31:09.626+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: gold_news_pipeline.run_consumer scheduled__2025-05-12T17:21:03.789689+00:00 [queued]>
[2025-05-12T17:31:09.638+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: gold_news_pipeline.run_consumer scheduled__2025-05-12T17:21:03.789689+00:00 [queued]>
[2025-05-12T17:31:09.639+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-05-12T17:31:09.655+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): run_consumer> on 2025-05-12 17:21:03.789689+00:00
[2025-05-12T17:31:09.663+0000] {standard_task_runner.py:57} INFO - Started process 3386 to run task
[2025-05-12T17:31:09.668+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'gold_news_pipeline', 'run_consumer', 'scheduled__2025-05-12T17:21:03.789689+00:00', '--job-id', '85', '--raw', '--subdir', 'DAGS_FOLDER/dag_news.py', '--cfg-path', '/tmp/tmppwxqjis4']
[2025-05-12T17:31:09.671+0000] {standard_task_runner.py:85} INFO - Job 85: Subtask run_consumer
[2025-05-12T17:31:09.735+0000] {task_command.py:415} INFO - Running <TaskInstance: gold_news_pipeline.run_consumer scheduled__2025-05-12T17:21:03.789689+00:00 [running]> on host 51d6065cf9f5
[2025-05-12T17:31:09.829+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='gold_news_pipeline' AIRFLOW_CTX_TASK_ID='run_consumer' AIRFLOW_CTX_EXECUTION_DATE='2025-05-12T17:21:03.789689+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-05-12T17:21:03.789689+00:00'
[2025-05-12T17:31:09.832+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-05-12T17:31:09.835+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec gold_price_project-spark-1 spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 /scripts/spark_news.py']
[2025-05-12T17:31:09.848+0000] {subprocess.py:86} INFO - Output:
[2025-05-12T17:31:12.521+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-05-12T17:31:12.665+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /root/.ivy2/cache
[2025-05-12T17:31:12.666+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /root/.ivy2/jars
[2025-05-12T17:31:12.676+0000] {subprocess.py:93} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-05-12T17:31:12.677+0000] {subprocess.py:93} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2025-05-12T17:31:12.678+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-0f38a4b8-f9b8-4ccc-8b3e-c03ff8a42943;1.0
[2025-05-12T17:31:12.678+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-05-12T17:31:12.951+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
[2025-05-12T17:31:13.056+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2025-05-12T17:31:13.114+0000] {subprocess.py:93} INFO - 	found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-05-12T17:31:13.161+0000] {subprocess.py:93} INFO - 	found org.lz4#lz4-java;1.8.0 in central
[2025-05-12T17:31:13.194+0000] {subprocess.py:93} INFO - 	found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2025-05-12T17:31:13.235+0000] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;2.0.7 in central
[2025-05-12T17:31:13.279+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-05-12T17:31:13.333+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-05-12T17:31:13.375+0000] {subprocess.py:93} INFO - 	found commons-logging#commons-logging;1.1.3 in central
[2025-05-12T17:31:13.396+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-05-12T17:31:13.426+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-pool2;2.11.1 in central
[2025-05-12T17:31:13.449+0000] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central
[2025-05-12T17:31:13.468+0000] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central
[2025-05-12T17:31:13.487+0000] {subprocess.py:93} INFO - 	found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2025-05-12T17:31:13.524+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2025-05-12T17:31:13.544+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#native-protocol;1.5.0 in central
[2025-05-12T17:31:13.561+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2025-05-12T17:31:13.577+0000] {subprocess.py:93} INFO - 	found com.typesafe#config;1.4.1 in central
[2025-05-12T17:31:13.604+0000] {subprocess.py:93} INFO - 	found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2025-05-12T17:31:13.620+0000] {subprocess.py:93} INFO - 	found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2025-05-12T17:31:13.640+0000] {subprocess.py:93} INFO - 	found org.reactivestreams#reactive-streams;1.0.3 in central
[2025-05-12T17:31:13.665+0000] {subprocess.py:93} INFO - 	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-05-12T17:31:13.687+0000] {subprocess.py:93} INFO - 	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2025-05-12T17:31:13.706+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.2 in central
[2025-05-12T17:31:13.726+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2025-05-12T17:31:13.744+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2025-05-12T17:31:13.761+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-lang3;3.10 in central
[2025-05-12T17:31:13.777+0000] {subprocess.py:93} INFO - 	found com.thoughtworks.paranamer#paranamer;2.8 in central
[2025-05-12T17:31:13.792+0000] {subprocess.py:93} INFO - 	found org.scala-lang#scala-reflect;2.12.11 in central
[2025-05-12T17:31:13.865+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 1143ms :: artifacts dl 43ms
[2025-05-12T17:31:13.869+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-05-12T17:31:13.876+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2025-05-12T17:31:13.877+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2025-05-12T17:31:13.878+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2025-05-12T17:31:13.879+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2025-05-12T17:31:13.880+0000] {subprocess.py:93} INFO - 	com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2025-05-12T17:31:13.880+0000] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]
[2025-05-12T17:31:13.881+0000] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]
[2025-05-12T17:31:13.882+0000] {subprocess.py:93} INFO - 	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2025-05-12T17:31:13.883+0000] {subprocess.py:93} INFO - 	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-05-12T17:31:13.883+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2025-05-12T17:31:13.884+0000] {subprocess.py:93} INFO - 	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2025-05-12T17:31:13.885+0000] {subprocess.py:93} INFO - 	com.typesafe#config;1.4.1 from central in [default]
[2025-05-12T17:31:13.887+0000] {subprocess.py:93} INFO - 	commons-logging#commons-logging;1.1.3 from central in [default]
[2025-05-12T17:31:13.888+0000] {subprocess.py:93} INFO - 	io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2025-05-12T17:31:13.890+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-lang3;3.10 from central in [default]
[2025-05-12T17:31:13.891+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2025-05-12T17:31:13.894+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2025-05-12T17:31:13.895+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-05-12T17:31:13.897+0000] {subprocess.py:93} INFO - 	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
[2025-05-12T17:31:13.898+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
[2025-05-12T17:31:13.906+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
[2025-05-12T17:31:13.907+0000] {subprocess.py:93} INFO - 	org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2025-05-12T17:31:13.908+0000] {subprocess.py:93} INFO - 	org.lz4#lz4-java;1.8.0 from central in [default]
[2025-05-12T17:31:13.909+0000] {subprocess.py:93} INFO - 	org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2025-05-12T17:31:13.910+0000] {subprocess.py:93} INFO - 	org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2025-05-12T17:31:13.911+0000] {subprocess.py:93} INFO - 	org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2025-05-12T17:31:13.911+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;2.0.7 from central in [default]
[2025-05-12T17:31:13.912+0000] {subprocess.py:93} INFO - 	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2025-05-12T17:31:13.913+0000] {subprocess.py:93} INFO - 	:: evicted modules:
[2025-05-12T17:31:13.913+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]
[2025-05-12T17:31:13.914+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.7] in [default]
[2025-05-12T17:31:13.915+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T17:31:13.915+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-05-12T17:31:13.920+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-12T17:31:13.922+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T17:31:13.924+0000] {subprocess.py:93} INFO - 	|      default     |   30  |   0   |   0   |   2   ||   28  |   0   |
[2025-05-12T17:31:13.925+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T17:31:13.926+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-0f38a4b8-f9b8-4ccc-8b3e-c03ff8a42943
[2025-05-12T17:31:13.928+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-05-12T17:31:14.105+0000] {subprocess.py:93} INFO - 	28 artifacts copied, 0 already retrieved (75061kB/211ms)
[2025-05-12T17:31:14.613+0000] {subprocess.py:93} INFO - 25/05/12 17:31:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-12T17:31:16.983+0000] {subprocess.py:93} INFO - 25/05/12 17:31:16 INFO SparkContext: Running Spark version 3.5.1
[2025-05-12T17:31:16.989+0000] {subprocess.py:93} INFO - 25/05/12 17:31:16 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-12T17:31:16.991+0000] {subprocess.py:93} INFO - 25/05/12 17:31:16 INFO SparkContext: Java version 11.0.22
[2025-05-12T17:31:17.075+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO ResourceUtils: ==============================================================
[2025-05-12T17:31:17.082+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-05-12T17:31:17.084+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO ResourceUtils: ==============================================================
[2025-05-12T17:31:17.085+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO SparkContext: Submitted application: GoldNewsProcessing
[2025-05-12T17:31:17.207+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-05-12T17:31:17.258+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO ResourceProfile: Limiting resource is cpu
[2025-05-12T17:31:17.263+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-05-12T17:31:17.516+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO SecurityManager: Changing view acls to: root
[2025-05-12T17:31:17.524+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO SecurityManager: Changing modify acls to: root
[2025-05-12T17:31:17.525+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO SecurityManager: Changing view acls groups to:
[2025-05-12T17:31:17.530+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO SecurityManager: Changing modify acls groups to:
[2025-05-12T17:31:17.538+0000] {subprocess.py:93} INFO - 25/05/12 17:31:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
[2025-05-12T17:31:18.354+0000] {subprocess.py:93} INFO - 25/05/12 17:31:18 INFO Utils: Successfully started service 'sparkDriver' on port 39617.
[2025-05-12T17:31:18.446+0000] {subprocess.py:93} INFO - 25/05/12 17:31:18 INFO SparkEnv: Registering MapOutputTracker
[2025-05-12T17:31:18.554+0000] {subprocess.py:93} INFO - 25/05/12 17:31:18 INFO SparkEnv: Registering BlockManagerMaster
[2025-05-12T17:31:18.611+0000] {subprocess.py:93} INFO - 25/05/12 17:31:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-05-12T17:31:18.613+0000] {subprocess.py:93} INFO - 25/05/12 17:31:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-05-12T17:31:18.628+0000] {subprocess.py:93} INFO - 25/05/12 17:31:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-05-12T17:31:18.739+0000] {subprocess.py:93} INFO - 25/05/12 17:31:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-215e2fd5-5b29-4e1f-8f93-18bd4ce2c8dd
[2025-05-12T17:31:18.794+0000] {subprocess.py:93} INFO - 25/05/12 17:31:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-05-12T17:31:18.836+0000] {subprocess.py:93} INFO - 25/05/12 17:31:18 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-05-12T17:31:19.264+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-05-12T17:31:19.410+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-05-12T17:31:19.486+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://d4e9ca837c07:39617/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.487+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://d4e9ca837c07:39617/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.488+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://d4e9ca837c07:39617/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.494+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://d4e9ca837c07:39617/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.497+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://d4e9ca837c07:39617/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.498+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://d4e9ca837c07:39617/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747071076952
[2025-05-12T17:31:19.499+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://d4e9ca837c07:39617/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.499+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://d4e9ca837c07:39617/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747071076952
[2025-05-12T17:31:19.500+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://d4e9ca837c07:39617/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747071076952
[2025-05-12T17:31:19.500+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://d4e9ca837c07:39617/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747071076952
[2025-05-12T17:31:19.501+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://d4e9ca837c07:39617/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747071076952
[2025-05-12T17:31:19.501+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://d4e9ca837c07:39617/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.502+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://d4e9ca837c07:39617/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.503+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.505+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.508+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://d4e9ca837c07:39617/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747071076952
[2025-05-12T17:31:19.510+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://d4e9ca837c07:39617/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747071076952
[2025-05-12T17:31:19.510+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://d4e9ca837c07:39617/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747071076952
[2025-05-12T17:31:19.511+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://d4e9ca837c07:39617/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.512+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.512+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://d4e9ca837c07:39617/jars/com.typesafe_config-1.4.1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.513+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://d4e9ca837c07:39617/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747071076952
[2025-05-12T17:31:19.514+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://d4e9ca837c07:39617/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747071076952
[2025-05-12T17:31:19.514+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://d4e9ca837c07:39617/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747071076952
[2025-05-12T17:31:19.515+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://d4e9ca837c07:39617/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.515+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://d4e9ca837c07:39617/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747071076952
[2025-05-12T17:31:19.516+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://d4e9ca837c07:39617/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747071076952
[2025-05-12T17:31:19.516+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.517+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.517+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:31:19.535+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.537+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T17:31:19.546+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.547+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:31:19.555+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.555+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T17:31:19.573+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.574+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T17:31:19.580+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747071076952
[2025-05-12T17:31:19.582+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T17:31:19.624+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.626+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T17:31:19.635+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747071076952
[2025-05-12T17:31:19.636+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T17:31:19.647+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747071076952
[2025-05-12T17:31:19.648+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T17:31:19.654+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747071076952
[2025-05-12T17:31:19.656+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T17:31:19.694+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747071076952
[2025-05-12T17:31:19.695+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T17:31:19.701+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.703+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T17:31:19.713+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.714+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T17:31:19.723+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.725+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T17:31:19.769+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.771+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T17:31:19.786+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747071076952
[2025-05-12T17:31:19.787+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T17:31:19.803+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747071076952
[2025-05-12T17:31:19.804+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T17:31:19.828+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747071076952
[2025-05-12T17:31:19.829+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T17:31:19.845+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.846+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T17:31:19.853+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.855+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T17:31:19.865+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar at file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.866+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.typesafe_config-1.4.1.jar
[2025-05-12T17:31:19.874+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747071076952
[2025-05-12T17:31:19.875+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T17:31:19.881+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747071076952
[2025-05-12T17:31:19.882+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T17:31:19.890+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747071076952
[2025-05-12T17:31:19.892+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T17:31:19.898+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747071076952
[2025-05-12T17:31:19.899+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T17:31:19.910+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747071076952
[2025-05-12T17:31:19.911+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T17:31:19.923+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747071076952
[2025-05-12T17:31:19.924+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T17:31:19.939+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:19.940+0000] {subprocess.py:93} INFO - 25/05/12 17:31:19 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T17:31:20.169+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Starting executor ID driver on host d4e9ca837c07
[2025-05-12T17:31:20.172+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-12T17:31:20.175+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Java version 11.0.22
[2025-05-12T17:31:20.202+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-05-12T17:31:20.206+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@57e120aa for default.
[2025-05-12T17:31:20.249+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.307+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:31:20.317+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747071076952
[2025-05-12T17:31:20.320+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T17:31:20.327+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747071076952
[2025-05-12T17:31:20.348+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T17:31:20.354+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747071076952
[2025-05-12T17:31:20.392+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T17:31:20.398+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747071076952
[2025-05-12T17:31:20.400+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T17:31:20.409+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747071076952
[2025-05-12T17:31:20.410+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T17:31:20.414+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.415+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T17:31:20.420+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.426+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T17:31:20.430+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1747071076952
[2025-05-12T17:31:20.431+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.typesafe_config-1.4.1.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.typesafe_config-1.4.1.jar
[2025-05-12T17:31:20.437+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747071076952
[2025-05-12T17:31:20.439+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T17:31:20.444+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747071076952
[2025-05-12T17:31:20.473+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T17:31:20.479+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.483+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:31:20.498+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747071076952
[2025-05-12T17:31:20.500+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T17:31:20.504+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747071076952
[2025-05-12T17:31:20.505+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T17:31:20.508+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.509+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T17:31:20.517+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747071076952
[2025-05-12T17:31:20.518+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T17:31:20.521+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.523+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T17:31:20.528+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747071076952
[2025-05-12T17:31:20.532+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T17:31:20.535+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747071076952
[2025-05-12T17:31:20.536+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T17:31:20.540+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747071076952
[2025-05-12T17:31:20.543+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T17:31:20.548+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747071076952
[2025-05-12T17:31:20.553+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T17:31:20.560+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.561+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T17:31:20.565+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747071076952
[2025-05-12T17:31:20.566+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T17:31:20.570+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747071076952
[2025-05-12T17:31:20.570+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T17:31:20.577+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.578+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T17:31:20.585+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.591+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T17:31:20.597+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.599+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T17:31:20.606+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747071076952
[2025-05-12T17:31:20.613+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T17:31:20.627+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747071076952
[2025-05-12T17:31:20.765+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO TransportClientFactory: Successfully created connection to d4e9ca837c07/172.18.0.9:39617 after 101 ms (0 ms spent in bootstraps)
[2025-05-12T17:31:20.784+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp15230879079943409516.tmp
[2025-05-12T17:31:20.875+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp15230879079943409516.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T17:31:20.882+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.thoughtworks.paranamer_paranamer-2.8.jar to class loader default
[2025-05-12T17:31:20.884+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.885+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp8028506475745758039.tmp
[2025-05-12T17:31:20.891+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp8028506475745758039.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T17:31:20.898+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_native-protocol-1.5.0.jar to class loader default
[2025-05-12T17:31:20.899+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747071076952
[2025-05-12T17:31:20.900+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp13588945610608350556.tmp
[2025-05-12T17:31:20.914+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp13588945610608350556.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T17:31:20.918+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.commons_commons-lang3-3.10.jar to class loader default
[2025-05-12T17:31:20.919+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747071076952
[2025-05-12T17:31:20.920+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp9203324194887575342.tmp
[2025-05-12T17:31:20.934+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp9203324194887575342.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T17:31:20.945+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.lz4_lz4-java-1.8.0.jar to class loader default
[2025-05-12T17:31:20.946+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747071076952
[2025-05-12T17:31:20.947+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17745795129575938114.tmp
[2025-05-12T17:31:20.950+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17745795129575938114.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T17:31:20.958+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to class loader default
[2025-05-12T17:31:20.959+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747071076952
[2025-05-12T17:31:20.962+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp7943540361707086880.tmp
[2025-05-12T17:31:20.963+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp7943540361707086880.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T17:31:20.968+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to class loader default
[2025-05-12T17:31:20.972+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747071076952
[2025-05-12T17:31:20.974+0000] {subprocess.py:93} INFO - 25/05/12 17:31:20 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17399480885392496190.tmp
[2025-05-12T17:31:21.033+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17399480885392496190.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T17:31:21.038+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.scala-lang_scala-reflect-2.12.11.jar to class loader default
[2025-05-12T17:31:21.039+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747071076952
[2025-05-12T17:31:21.040+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp12519953789951210477.tmp
[2025-05-12T17:31:21.042+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp12519953789951210477.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T17:31:21.050+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/commons-logging_commons-logging-1.1.3.jar to class loader default
[2025-05-12T17:31:21.051+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:21.053+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17729284032122773390.tmp
[2025-05-12T17:31:21.137+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17729284032122773390.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T17:31:21.147+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to class loader default
[2025-05-12T17:31:21.147+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.typesafe_config-1.4.1.jar with timestamp 1747071076952
[2025-05-12T17:31:21.148+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp9871135417261076733.tmp
[2025-05-12T17:31:21.151+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp9871135417261076733.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.typesafe_config-1.4.1.jar
[2025-05-12T17:31:21.156+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.typesafe_config-1.4.1.jar to class loader default
[2025-05-12T17:31:21.157+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747071076952
[2025-05-12T17:31:21.159+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp3761895492941274765.tmp
[2025-05-12T17:31:21.163+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp3761895492941274765.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T17:31:21.170+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2025-05-12T17:31:21.174+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:21.176+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17874605711634712416.tmp
[2025-05-12T17:31:21.184+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17874605711634712416.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T17:31:21.190+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to class loader default
[2025-05-12T17:31:21.191+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747071076952
[2025-05-12T17:31:21.195+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp10715065012602774777.tmp
[2025-05-12T17:31:21.237+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp10715065012602774777.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T17:31:21.241+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
[2025-05-12T17:31:21.242+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747071076952
[2025-05-12T17:31:21.244+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp12840094846949805074.tmp
[2025-05-12T17:31:21.247+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp12840094846949805074.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T17:31:21.249+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.reactivestreams_reactive-streams-1.0.3.jar to class loader default
[2025-05-12T17:31:21.250+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747071076952
[2025-05-12T17:31:21.251+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp3715592781570953593.tmp
[2025-05-12T17:31:21.254+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp3715592781570953593.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T17:31:21.263+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to class loader default
[2025-05-12T17:31:21.265+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:21.266+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp11899943954543617975.tmp
[2025-05-12T17:31:21.269+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp11899943954543617975.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:31:21.278+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
[2025-05-12T17:31:21.279+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747071076952
[2025-05-12T17:31:21.281+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp15667591167709159526.tmp
[2025-05-12T17:31:21.324+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp15667591167709159526.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T17:31:21.331+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to class loader default
[2025-05-12T17:31:21.332+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:21.333+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp3374639590025380571.tmp
[2025-05-12T17:31:21.339+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp3374639590025380571.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T17:31:21.347+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-query-builder-4.13.0.jar to class loader default
[2025-05-12T17:31:21.348+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747071076952
[2025-05-12T17:31:21.349+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp14331100060720145853.tmp
[2025-05-12T17:31:21.663+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp14331100060720145853.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T17:31:21.675+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
[2025-05-12T17:31:21.676+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747071076952
[2025-05-12T17:31:21.677+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp15691676761508545882.tmp
[2025-05-12T17:31:21.680+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp15691676761508545882.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T17:31:21.684+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.hdrhistogram_HdrHistogram-2.1.12.jar to class loader default
[2025-05-12T17:31:21.685+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747071076952
[2025-05-12T17:31:21.689+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp12906830236816967852.tmp
[2025-05-12T17:31:21.690+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp12906830236816967852.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T17:31:21.695+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.slf4j_slf4j-api-2.0.7.jar to class loader default
[2025-05-12T17:31:21.696+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747071076952
[2025-05-12T17:31:21.697+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp8930088130064755262.tmp
[2025-05-12T17:31:21.698+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp8930088130064755262.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T17:31:21.707+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.google.code.findbugs_jsr305-3.0.2.jar to class loader default
[2025-05-12T17:31:21.708+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747071076952
[2025-05-12T17:31:21.709+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp8244252715688076179.tmp
[2025-05-12T17:31:21.721+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp8244252715688076179.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T17:31:21.725+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
[2025-05-12T17:31:21.726+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747071076952
[2025-05-12T17:31:21.727+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp11429103652963609157.tmp
[2025-05-12T17:31:21.728+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp11429103652963609157.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T17:31:21.735+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to class loader default
[2025-05-12T17:31:21.736+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:21.737+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp7012037286327039178.tmp
[2025-05-12T17:31:21.738+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp7012037286327039178.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:31:21.740+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
[2025-05-12T17:31:21.742+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747071076952
[2025-05-12T17:31:21.742+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp448532150689626257.tmp
[2025-05-12T17:31:21.744+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp448532150689626257.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T17:31:21.751+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/io.dropwizard.metrics_metrics-core-4.1.18.jar to class loader default
[2025-05-12T17:31:21.752+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747071076952
[2025-05-12T17:31:21.753+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17490346192916940208.tmp
[2025-05-12T17:31:21.877+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp17490346192916940208.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T17:31:21.890+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
[2025-05-12T17:31:21.891+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747071076952
[2025-05-12T17:31:21.892+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Fetching spark://d4e9ca837c07:39617/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp2466319573929151358.tmp
[2025-05-12T17:31:21.902+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/fetchFileTemp2466319573929151358.tmp has been previously copied to /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T17:31:21.905+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Executor: Adding file:/tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/userFiles-35674778-7cd1-4b58-9174-b815ac9395cc/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to class loader default
[2025-05-12T17:31:21.924+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43209.
[2025-05-12T17:31:21.925+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO NettyBlockTransferService: Server created on d4e9ca837c07:43209
[2025-05-12T17:31:21.929+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-05-12T17:31:21.942+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d4e9ca837c07, 43209, None)
[2025-05-12T17:31:21.949+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO BlockManagerMasterEndpoint: Registering block manager d4e9ca837c07:43209 with 434.4 MiB RAM, BlockManagerId(driver, d4e9ca837c07, 43209, None)
[2025-05-12T17:31:21.954+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d4e9ca837c07, 43209, None)
[2025-05-12T17:31:21.956+0000] {subprocess.py:93} INFO - 25/05/12 17:31:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d4e9ca837c07, 43209, None)
[2025-05-12T17:31:25.644+0000] {subprocess.py:93} INFO - 25/05/12 17:31:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-05-12T17:31:25.652+0000] {subprocess.py:93} INFO - 25/05/12 17:31:25 INFO SharedState: Warehouse path is 'file:/opt/spark-apps/spark-warehouse'.
[2025-05-12T17:31:34.215+0000] {subprocess.py:93} INFO - 25/05/12 17:31:34 INFO DefaultMavenCoordinates: DataStax Java driver for Apache Cassandra(R) (com.datastax.oss:java-driver-core-shaded) version 4.13.0
[2025-05-12T17:31:34.589+0000] {subprocess.py:93} INFO - 25/05/12 17:31:34 INFO Native: Unable to load JNR native implementation. This could be normal if JNR is excluded from the classpath
[2025-05-12T17:31:34.589+0000] {subprocess.py:93} INFO - java.lang.NoClassDefFoundError: jnr/posix/POSIXHandler
[2025-05-12T17:31:34.590+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native$LibcLoader.load(Native.java:42)
[2025-05-12T17:31:34.590+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native.<clinit>(Native.java:59)
[2025-05-12T17:31:34.591+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.Clock.getInstance(Clock.java:41)
[2025-05-12T17:31:34.591+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.buildClock(MonotonicTimestampGenerator.java:109)
[2025-05-12T17:31:34.592+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.<init>(MonotonicTimestampGenerator.java:43)
[2025-05-12T17:31:34.592+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.AtomicTimestampGenerator.<init>(AtomicTimestampGenerator.java:52)
[2025-05-12T17:31:34.593+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-12T17:31:34.593+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-12T17:31:34.594+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-12T17:31:34.594+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-12T17:31:34.596+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.resolveClass(Reflection.java:329)
[2025-05-12T17:31:34.597+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:235)
[2025-05-12T17:31:34.597+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:110)
[2025-05-12T17:31:34.598+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.buildTimestampGenerator(DefaultDriverContext.java:377)
[2025-05-12T17:31:34.598+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.concurrent.LazyReference.get(LazyReference.java:55)
[2025-05-12T17:31:34.598+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.getTimestampGenerator(DefaultDriverContext.java:773)
[2025-05-12T17:31:34.599+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.init(DefaultSession.java:349)
[2025-05-12T17:31:34.599+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.access$1100(DefaultSession.java:300)
[2025-05-12T17:31:34.599+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession.lambda$init$0(DefaultSession.java:146)
[2025-05-12T17:31:34.600+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
[2025-05-12T17:31:34.600+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
[2025-05-12T17:31:34.600+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)
[2025-05-12T17:31:34.601+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2025-05-12T17:31:34.601+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-05-12T17:31:34.602+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-05-12T17:31:34.602+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-12T17:31:34.602+0000] {subprocess.py:93} INFO - Caused by: java.lang.ClassNotFoundException: jnr.posix.POSIXHandler
[2025-05-12T17:31:34.603+0000] {subprocess.py:93} INFO - 	at java.base/java.net.URLClassLoader.findClass(Unknown Source)
[2025-05-12T17:31:34.603+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
[2025-05-12T17:31:34.603+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
[2025-05-12T17:31:34.604+0000] {subprocess.py:93} INFO - 	... 26 more
[2025-05-12T17:31:34.604+0000] {subprocess.py:93} INFO - 25/05/12 17:31:34 INFO Clock: Could not access native clock (see debug logs for details), falling back to Java system clock
[2025-05-12T17:31:35.722+0000] {subprocess.py:93} INFO - 25/05/12 17:31:35 INFO CassandraConnector: Connected to Cassandra cluster.
[2025-05-12T17:31:35.784+0000] {subprocess.py:93} INFO - 25/05/12 17:31:35 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-05-12T17:31:35.884+0000] {subprocess.py:93} INFO - 25/05/12 17:31:35 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint/cassandra resolved to file:/tmp/spark-checkpoint/cassandra.
[2025-05-12T17:31:35.884+0000] {subprocess.py:93} INFO - 25/05/12 17:31:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-05-12T17:31:36.004+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/cassandra/metadata using temp file file:/tmp/spark-checkpoint/cassandra/.metadata.53297424-78b7-4d09-95b9-88a0e748976e.tmp
[2025-05-12T17:31:36.110+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/cassandra/.metadata.53297424-78b7-4d09-95b9-88a0e748976e.tmp to file:/tmp/spark-checkpoint/cassandra/metadata
[2025-05-12T17:31:36.164+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO MicroBatchExecution: Starting [id = 71460171-bd4d-4562-8003-9df0ff8d9d3e, runId = 194a7c71-7598-4a27-a96b-8390c203544a]. Use file:/tmp/spark-checkpoint/cassandra to store the query checkpoint.
[2025-05-12T17:31:36.202+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2175affc] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4b6fc201]
[2025-05-12T17:31:36.249+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258 resolved to file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258.
[2025-05-12T17:31:36.250+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-05-12T17:31:36.270+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/metadata using temp file file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/.metadata.5d5628fe-f5d5-46d3-9979-037f28850c3e.tmp
[2025-05-12T17:31:36.311+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 WARN MicroBatchExecution: The read limit MaxRows: 100 for KafkaV2[Subscribe[gold-news]] is ignored when Trigger.Once is used.
[2025-05-12T17:31:36.330+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO OffsetSeqLog: BatchIds found from listing:
[2025-05-12T17:31:36.340+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO OffsetSeqLog: BatchIds found from listing:
[2025-05-12T17:31:36.342+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO MicroBatchExecution: Starting new streaming query.
[2025-05-12T17:31:36.345+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO MicroBatchExecution: Stream started from {}
[2025-05-12T17:31:36.373+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/.metadata.5d5628fe-f5d5-46d3-9979-037f28850c3e.tmp to file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/metadata
[2025-05-12T17:31:36.414+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO MicroBatchExecution: Starting [id = 06a77141-55f5-4c1a-aba2-499a68b82bf8, runId = df69921a-42ab-41b7-ab6f-b0e839dc5024]. Use file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258 to store the query checkpoint.
[2025-05-12T17:31:36.423+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2175affc] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4b6fc201]
[2025-05-12T17:31:36.430+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 WARN MicroBatchExecution: The read limit MaxRows: 100 for KafkaV2[Subscribe[gold-news]] is ignored when Trigger.Once is used.
[2025-05-12T17:31:36.435+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO OffsetSeqLog: BatchIds found from listing:
[2025-05-12T17:31:36.436+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO OffsetSeqLog: BatchIds found from listing:
[2025-05-12T17:31:36.437+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO MicroBatchExecution: Starting new streaming query.
[2025-05-12T17:31:36.438+0000] {subprocess.py:93} INFO - 25/05/12 17:31:36 INFO MicroBatchExecution: Stream started from {}
[2025-05-12T17:31:37.211+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 INFO AdminClientConfig: AdminClientConfig values:
[2025-05-12T17:31:37.212+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:31:37.213+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:31:37.214+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:31:37.214+0000] {subprocess.py:93} INFO - 	client.id =
[2025-05-12T17:31:37.216+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-05-12T17:31:37.217+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:31:37.223+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:31:37.225+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:31:37.228+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:31:37.229+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:31:37.230+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:31:37.232+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:31:37.233+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:31:37.238+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:31:37.240+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:31:37.241+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-05-12T17:31:37.241+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:31:37.242+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:31:37.242+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:31:37.243+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:31:37.243+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:31:37.244+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:31:37.244+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:31:37.245+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:31:37.245+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:31:37.246+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:31:37.246+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:31:37.246+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:31:37.247+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:31:37.254+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:31:37.255+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:31:37.256+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:31:37.257+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:31:37.257+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:31:37.258+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:31:37.258+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:31:37.259+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:31:37.259+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:31:37.260+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:31:37.260+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:31:37.260+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:31:37.261+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:31:37.261+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:31:37.262+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:31:37.262+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:31:37.263+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:31:37.263+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:31:37.264+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:31:37.265+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:31:37.268+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:31:37.269+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:31:37.270+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:31:37.270+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:31:37.271+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:31:37.271+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:31:37.271+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:31:37.272+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:31:37.272+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:31:37.272+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:31:37.273+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:31:37.273+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:31:37.273+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:31:37.274+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:31:37.274+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:31:37.274+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:31:37.275+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:31:37.275+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:31:37.276+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:31:37.276+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:31:37.277+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:37.277+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 INFO AdminClientConfig: AdminClientConfig values:
[2025-05-12T17:31:37.278+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:31:37.279+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:31:37.279+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:31:37.280+0000] {subprocess.py:93} INFO - 	client.id =
[2025-05-12T17:31:37.282+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-05-12T17:31:37.283+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:31:37.284+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:31:37.285+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:31:37.285+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:31:37.287+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:31:37.287+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:31:37.288+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:31:37.288+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:31:37.288+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:31:37.289+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:31:37.289+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-05-12T17:31:37.290+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:31:37.290+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:31:37.291+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:31:37.291+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:31:37.292+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:31:37.292+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:31:37.293+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:31:37.293+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:31:37.294+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:31:37.294+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:31:37.295+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:31:37.298+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:31:37.298+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:31:37.300+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:31:37.301+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:31:37.301+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:31:37.301+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:31:37.302+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:31:37.302+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:31:37.303+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:31:37.303+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:31:37.304+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:31:37.305+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:31:37.305+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:31:37.306+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:31:37.307+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:31:37.309+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:31:37.315+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:31:37.316+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:31:37.316+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:31:37.317+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:31:37.318+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:31:37.319+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:31:37.319+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:31:37.320+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:31:37.320+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:31:37.321+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:31:37.321+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:31:37.322+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:31:37.322+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:31:37.323+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:31:37.323+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:31:37.324+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:31:37.324+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:31:37.325+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:31:37.329+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:31:37.331+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:31:37.332+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:31:37.333+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:31:37.334+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:31:37.335+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:31:37.336+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:31:37.337+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:31:37.337+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:37.435+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-05-12T17:31:37.436+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-05-12T17:31:37.439+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:31:37.440+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:31:37.441+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 INFO AppInfoParser: Kafka startTimeMs: 1747071097432
[2025-05-12T17:31:37.443+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:31:37.444+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:31:37.445+0000] {subprocess.py:93} INFO - 25/05/12 17:31:37 INFO AppInfoParser: Kafka startTimeMs: 1747071097432
[2025-05-12T17:31:38.102+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/sources/0/0 using temp file file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/sources/0/.0.18bf3fb8-1115-4d56-8de8-ba8bd9a246e0.tmp
[2025-05-12T17:31:38.103+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/cassandra/sources/0/0 using temp file file:/tmp/spark-checkpoint/cassandra/sources/0/.0.5d803973-4508-4404-bcc7-a404211a98b5.tmp
[2025-05-12T17:31:38.132+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/sources/0/.0.18bf3fb8-1115-4d56-8de8-ba8bd9a246e0.tmp to file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/sources/0/0
[2025-05-12T17:31:38.133+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/cassandra/sources/0/.0.5d803973-4508-4404-bcc7-a404211a98b5.tmp to file:/tmp/spark-checkpoint/cassandra/sources/0/0
[2025-05-12T17:31:38.134+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO KafkaMicroBatchStream: Initial offsets: {"gold-news":{"0":0}}
[2025-05-12T17:31:38.135+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO KafkaMicroBatchStream: Initial offsets: {"gold-news":{"0":0}}
[2025-05-12T17:31:38.175+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/cassandra/offsets/0 using temp file file:/tmp/spark-checkpoint/cassandra/offsets/.0.bcf796b3-9047-4dcd-b011-4cc702a6a7e9.tmp
[2025-05-12T17:31:38.176+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/offsets/0 using temp file file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/offsets/.0.3c57e747-937f-438b-a94e-188d4ca7628d.tmp
[2025-05-12T17:31:38.247+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/offsets/.0.3c57e747-937f-438b-a94e-188d4ca7628d.tmp to file:/tmp/spark-checkpoint/c573c85c-0940-4246-b4a1-6075de90c258/offsets/0
[2025-05-12T17:31:38.248+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/cassandra/offsets/.0.bcf796b3-9047-4dcd-b011-4cc702a6a7e9.tmp to file:/tmp/spark-checkpoint/cassandra/offsets/0
[2025-05-12T17:31:38.249+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1747071098159,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-05-12T17:31:38.251+0000] {subprocess.py:93} INFO - 25/05/12 17:31:38 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1747071098159,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-05-12T17:31:39.221+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:39.222+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:39.391+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:39.399+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:39.504+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:39.506+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:39.518+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:39.522+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:39.658+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:39.665+0000] {subprocess.py:93} INFO - 25/05/12 17:31:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:31:40.202+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO CodeGenerator: Code generated in 346.76944 ms
[2025-05-12T17:31:40.370+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@53ee4181,com.datastax.spark.connector.cql.CassandraConnector@776e743d,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@8b7a565)]. The input RDD has 1 partitions.
[2025-05-12T17:31:40.475+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO CodeGenerator: Code generated in 14.477481 ms
[2025-05-12T17:31:40.489+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO SparkContext: Starting job: start at <unknown>:0
[2025-05-12T17:31:40.497+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO SparkContext: Starting job: start at <unknown>:0
[2025-05-12T17:31:40.513+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-05-12T17:31:40.514+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-05-12T17:31:40.515+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:31:40.519+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:31:40.525+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0), which has no missing parents
[2025-05-12T17:31:40.688+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KiB, free 434.4 MiB)
[2025-05-12T17:31:40.732+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 434.4 MiB)
[2025-05-12T17:31:40.736+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on d4e9ca837c07:43209 (size: 13.5 KiB, free: 434.4 MiB)
[2025-05-12T17:31:40.742+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:31:40.765+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:31:40.767+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-05-12T17:31:40.792+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-05-12T17:31:40.794+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-05-12T17:31:40.795+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:31:40.798+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:31:40.800+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-05-12T17:31:40.827+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 434.3 MiB)
[2025-05-12T17:31:40.848+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 15.7 KiB, free 434.3 MiB)
[2025-05-12T17:31:40.849+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on d4e9ca837c07:43209 (size: 15.7 KiB, free: 434.4 MiB)
[2025-05-12T17:31:40.852+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:31:40.857+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:31:40.859+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-05-12T17:31:40.873+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 13811 bytes)
[2025-05-12T17:31:40.891+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 13808 bytes)
[2025-05-12T17:31:40.905+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-05-12T17:31:40.906+0000] {subprocess.py:93} INFO - 25/05/12 17:31:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-05-12T17:31:41.260+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO CodeGenerator: Code generated in 93.267633 ms
[2025-05-12T17:31:41.318+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO CodeGenerator: Code generated in 52.854723 ms
[2025-05-12T17:31:41.342+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO CodeGenerator: Code generated in 14.723967 ms
[2025-05-12T17:31:41.348+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=0 untilOffset=30, for query queryId=06a77141-55f5-4c1a-aba2-499a68b82bf8 batchId=0 taskId=1 partitionId=0
[2025-05-12T17:31:41.456+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO CodeGenerator: Code generated in 27.944129 ms
[2025-05-12T17:31:41.499+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO CodeGenerator: Code generated in 27.070196 ms
[2025-05-12T17:31:41.547+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO ConsumerConfig: ConsumerConfig values:
[2025-05-12T17:31:41.547+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-05-12T17:31:41.548+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-05-12T17:31:41.549+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:31:41.549+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-05-12T17:31:41.549+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:31:41.550+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-05-12T17:31:41.550+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:31:41.550+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1
[2025-05-12T17:31:41.551+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-05-12T17:31:41.551+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-05-12T17:31:41.552+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:31:41.552+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-05-12T17:31:41.552+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-05-12T17:31:41.553+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-05-12T17:31:41.553+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-05-12T17:31:41.554+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-05-12T17:31:41.555+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor
[2025-05-12T17:31:41.556+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-05-12T17:31:41.557+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-05-12T17:31:41.558+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-05-12T17:31:41.558+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-05-12T17:31:41.559+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-05-12T17:31:41.559+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-05-12T17:31:41.559+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:31:41.560+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-05-12T17:31:41.560+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-05-12T17:31:41.561+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-05-12T17:31:41.561+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:31:41.562+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:31:41.562+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:31:41.563+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:31:41.563+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:31:41.564+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-05-12T17:31:41.564+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:31:41.565+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:31:41.565+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:31:41.566+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:31:41.566+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:31:41.567+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:31:41.567+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:31:41.568+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:31:41.568+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:31:41.568+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:31:41.569+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:31:41.571+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:31:41.573+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:31:41.574+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:31:41.574+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:31:41.575+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:31:41.576+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:31:41.576+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:31:41.577+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:31:41.577+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:31:41.577+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:31:41.578+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:31:41.578+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:31:41.579+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:31:41.579+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:31:41.579+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:31:41.580+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:31:41.580+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:31:41.581+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:31:41.581+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:31:41.582+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:31:41.582+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:31:41.583+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:31:41.583+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:31:41.585+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:31:41.586+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:31:41.587+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-05-12T17:31:41.588+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:31:41.589+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:31:41.589+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:31:41.590+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:31:41.590+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:31:41.590+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:31:41.591+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:31:41.591+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:31:41.592+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:31:41.592+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:31:41.592+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:31:41.593+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:31:41.593+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:31:41.593+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:31:41.594+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:31:41.594+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:31:41.594+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:31:41.595+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:31:41.595+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:31:41.596+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:31:41.596+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:31:41.596+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:31:41.597+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:41.641+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:31:41.642+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:31:41.642+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO AppInfoParser: Kafka startTimeMs: 1747071101639
[2025-05-12T17:31:41.644+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Assigned to partition(s): gold-news-0
[2025-05-12T17:31:41.661+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Seeking to offset 0 for partition gold-news-0
[2025-05-12T17:31:41.674+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Resetting the last seen epoch of partition gold-news-0 to 0 since the associated topicId changed from null to EyFovrTXQQOm3GPuq8IGNA
[2025-05-12T17:31:41.675+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Cluster ID: HTSY0p8VS7eCaEFoyzBH3g
[2025-05-12T17:31:41.784+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=0 untilOffset=30, for query queryId=71460171-bd4d-4562-8003-9df0ff8d9d3e batchId=0 taskId=0 partitionId=0
[2025-05-12T17:31:41.806+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO ConsumerConfig: ConsumerConfig values:
[2025-05-12T17:31:41.807+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-05-12T17:31:41.808+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-05-12T17:31:41.808+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:31:41.809+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-05-12T17:31:41.809+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:31:41.810+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-05-12T17:31:41.810+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:31:41.811+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2
[2025-05-12T17:31:41.812+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-05-12T17:31:41.812+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-05-12T17:31:41.813+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:31:41.814+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-05-12T17:31:41.817+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-05-12T17:31:41.817+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-05-12T17:31:41.818+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-05-12T17:31:41.818+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-05-12T17:31:41.818+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor
[2025-05-12T17:31:41.819+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-05-12T17:31:41.819+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-05-12T17:31:41.820+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-05-12T17:31:41.820+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-05-12T17:31:41.821+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-05-12T17:31:41.821+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-05-12T17:31:41.821+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:31:41.822+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-05-12T17:31:41.822+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-05-12T17:31:41.823+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-05-12T17:31:41.823+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:31:41.823+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:31:41.824+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:31:41.824+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:31:41.824+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:31:41.825+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-05-12T17:31:41.825+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:31:41.825+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:31:41.826+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:31:41.827+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:31:41.827+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:31:41.828+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:31:41.828+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:31:41.829+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:31:41.832+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:31:41.833+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:31:41.834+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:31:41.834+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:31:41.835+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:31:41.835+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:31:41.836+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:31:41.836+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:31:41.837+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:31:41.837+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:31:41.838+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:31:41.838+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:31:41.839+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:31:41.839+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:31:41.840+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:31:41.840+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:31:41.841+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:31:41.841+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:31:41.842+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:31:41.842+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:31:41.842+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:31:41.843+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:31:41.844+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:31:41.846+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:31:41.847+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:31:41.847+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:31:41.848+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:31:41.848+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:31:41.849+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-05-12T17:31:41.849+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:31:41.849+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:31:41.849+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:31:41.850+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:31:41.850+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:31:41.851+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:31:41.851+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:31:41.851+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:31:41.852+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:31:41.852+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:31:41.853+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:31:41.853+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:31:41.853+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:31:41.854+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:31:41.854+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:31:41.855+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:31:41.855+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:31:41.856+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:31:41.856+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:31:41.857+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:31:41.857+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:31:41.858+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:31:41.858+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:41.888+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:31:41.889+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:31:41.894+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO AppInfoParser: Kafka startTimeMs: 1747071101886
[2025-05-12T17:31:41.897+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Assigned to partition(s): gold-news-0
[2025-05-12T17:31:41.897+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Seeking to offset 0 for partition gold-news-0
[2025-05-12T17:31:41.909+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Resetting the last seen epoch of partition gold-news-0 to 0 since the associated topicId changed from null to EyFovrTXQQOm3GPuq8IGNA
[2025-05-12T17:31:41.910+0000] {subprocess.py:93} INFO - 25/05/12 17:31:41 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Cluster ID: HTSY0p8VS7eCaEFoyzBH3g
[2025-05-12T17:31:42.098+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T17:31:42.100+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T17:31:42.662+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:31:42.663+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T17:31:42.666+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:31:42.675+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:31:42.676+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T17:31:42.679+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:31:42.858+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor read 1 records through 1 polls (polled  out 30 records), taking 1019075578 nanos, during time span of 1202611683 nanos.
[2025-05-12T17:31:42.895+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1957 bytes result sent to driver
[2025-05-12T17:31:42.912+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2021 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:31:42.948+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-05-12T17:31:42.962+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 2.155 s
[2025-05-12T17:31:42.965+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-05-12T17:31:42.969+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:31:42.970+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-05-12T17:31:42.978+0000] {subprocess.py:93} INFO - 25/05/12 17:31:42 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 2.476192 s
[2025-05-12T17:31:43.071+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO CodeGenerator: Code generated in 25.78661 ms
[2025-05-12T17:31:43.118+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T17:31:43.121+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Got job 2 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T17:31:43.122+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Final stage: ResultStage 2 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:31:43.123+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:31:43.124+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:31:43.125+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:31:43.129+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 42.3 KiB, free 434.3 MiB)
[2025-05-12T17:31:43.163+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 434.2 MiB)
[2025-05-12T17:31:43.167+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on d4e9ca837c07:43209 (size: 16.1 KiB, free: 434.4 MiB)
[2025-05-12T17:31:43.178+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:31:43.179+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:31:43.190+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-05-12T17:31:43.194+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 13808 bytes)
[2025-05-12T17:31:43.195+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-05-12T17:31:43.283+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO CodeGenerator: Code generated in 37.439813 ms
[2025-05-12T17:31:43.299+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=0 untilOffset=30, for query queryId=06a77141-55f5-4c1a-aba2-499a68b82bf8 batchId=0 taskId=2 partitionId=0
[2025-05-12T17:31:43.365+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Seeking to offset 0 for partition gold-news-0
[2025-05-12T17:31:43.377+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-05-12T17:31:43.378+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor read 30 records through 1 polls (polled  out 30 records), taking 774818844 nanos, during time span of 1487610272 nanos.
[2025-05-12T17:31:43.388+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T17:31:43.397+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1851 bytes result sent to driver
[2025-05-12T17:31:43.413+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2593 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:31:43.415+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-05-12T17:31:43.415+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 2.874 s
[2025-05-12T17:31:43.416+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:31:43.420+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-05-12T17:31:43.422+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 2.933536 s
[2025-05-12T17:31:43.423+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@53ee4181,com.datastax.spark.connector.cql.CassandraConnector@776e743d,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@8b7a565)] is committing.
[2025-05-12T17:31:43.424+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@53ee4181,com.datastax.spark.connector.cql.CassandraConnector@776e743d,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@8b7a565)] committed.
[2025-05-12T17:31:43.435+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on d4e9ca837c07:43209 in memory (size: 15.7 KiB, free: 434.4 MiB)
[2025-05-12T17:31:43.440+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/cassandra/commits/0 using temp file file:/tmp/spark-checkpoint/cassandra/commits/.0.be80fd1f-4ced-4468-ab60-cac9fc8f8b20.tmp
[2025-05-12T17:31:43.480+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/cassandra/commits/.0.be80fd1f-4ced-4468-ab60-cac9fc8f8b20.tmp to file:/tmp/spark-checkpoint/cassandra/commits/0
[2025-05-12T17:31:43.529+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO MicroBatchExecution: Streaming query made progress: {
[2025-05-12T17:31:43.530+0000] {subprocess.py:93} INFO -   "id" : "71460171-bd4d-4562-8003-9df0ff8d9d3e",
[2025-05-12T17:31:43.531+0000] {subprocess.py:93} INFO -   "runId" : "194a7c71-7598-4a27-a96b-8390c203544a",
[2025-05-12T17:31:43.531+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-05-12T17:31:43.532+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-05-12T17:31:36.318Z",
[2025-05-12T17:31:43.533+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-05-12T17:31:43.533+0000] {subprocess.py:93} INFO -   "numInputRows" : 30,
[2025-05-12T17:31:43.534+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-05-12T17:31:43.534+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 4.188774085450992,
[2025-05-12T17:31:43.535+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-05-12T17:31:43.536+0000] {subprocess.py:93} INFO -     "addBatch" : 3979,
[2025-05-12T17:31:43.537+0000] {subprocess.py:93} INFO -     "commitOffsets" : 49,
[2025-05-12T17:31:43.538+0000] {subprocess.py:93} INFO -     "getBatch" : 53,
[2025-05-12T17:31:43.543+0000] {subprocess.py:93} INFO -     "latestOffset" : 1810,
[2025-05-12T17:31:43.544+0000] {subprocess.py:93} INFO -     "queryPlanning" : 1132,
[2025-05-12T17:31:43.545+0000] {subprocess.py:93} INFO -     "triggerExecution" : 7160,
[2025-05-12T17:31:43.545+0000] {subprocess.py:93} INFO -     "walCommit" : 80
[2025-05-12T17:31:43.546+0000] {subprocess.py:93} INFO -   },
[2025-05-12T17:31:43.547+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-05-12T17:31:43.547+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-05-12T17:31:43.548+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[gold-news]]",
[2025-05-12T17:31:43.548+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-05-12T17:31:43.549+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-05-12T17:31:43.549+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T17:31:43.550+0000] {subprocess.py:93} INFO -         "0" : 30
[2025-05-12T17:31:43.551+0000] {subprocess.py:93} INFO -       }
[2025-05-12T17:31:43.551+0000] {subprocess.py:93} INFO -     },
[2025-05-12T17:31:43.552+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-05-12T17:31:43.552+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T17:31:43.553+0000] {subprocess.py:93} INFO -         "0" : 30
[2025-05-12T17:31:43.556+0000] {subprocess.py:93} INFO -       }
[2025-05-12T17:31:43.557+0000] {subprocess.py:93} INFO -     },
[2025-05-12T17:31:43.558+0000] {subprocess.py:93} INFO -     "numInputRows" : 30,
[2025-05-12T17:31:43.558+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-05-12T17:31:43.559+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 4.188774085450992,
[2025-05-12T17:31:43.559+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-05-12T17:31:43.559+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-05-12T17:31:43.560+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-05-12T17:31:43.560+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-05-12T17:31:43.561+0000] {subprocess.py:93} INFO -     }
[2025-05-12T17:31:43.561+0000] {subprocess.py:93} INFO -   } ],
[2025-05-12T17:31:43.562+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-05-12T17:31:43.562+0000] {subprocess.py:93} INFO -     "description" : "CassandraTable(org.apache.spark.sql.SparkSession@53ee4181,org.apache.spark.sql.util.CaseInsensitiveStringMap@6e619b4c,com.datastax.spark.connector.cql.CassandraConnector@169881ab,default,DefaultTableMetadata@88aa72be(gold_news.articles),None)",
[2025-05-12T17:31:43.563+0000] {subprocess.py:93} INFO -     "numOutputRows" : 30
[2025-05-12T17:31:43.563+0000] {subprocess.py:93} INFO -   }
[2025-05-12T17:31:43.564+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:43.564+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2025-05-12T17:31:43.564+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:31:43.565+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:31:43.565+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO Metrics: Metrics reporters closed
[2025-05-12T17:31:43.566+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO MicroBatchExecution: Async log purge executor pool for query [id = 71460171-bd4d-4562-8003-9df0ff8d9d3e, runId = 194a7c71-7598-4a27-a96b-8390c203544a] has been shutdown
[2025-05-12T17:31:43.884+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:31:43.884+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T17:31:43.887+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:31:43.922+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor read 30 records through 1 polls (polled  out 30 records), taking 522802829 nanos, during time span of 555187666 nanos.
[2025-05-12T17:31:43.929+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 7106 bytes result sent to driver
[2025-05-12T17:31:43.933+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 755 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:31:43.934+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-05-12T17:31:43.940+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: ResultStage 2 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.811 s
[2025-05-12T17:31:43.941+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:31:43.944+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-05-12T17:31:43.945+0000] {subprocess.py:93} INFO - 25/05/12 17:31:43 INFO DAGScheduler: Job 2 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.822751 s
[2025-05-12T17:31:43.995+0000] {subprocess.py:93} INFO - Traitement du batch 0 avec 30 descriptions
[2025-05-12T17:31:44.034+0000] {subprocess.py:93} INFO - 25/05/12 17:31:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on d4e9ca837c07:43209 in memory (size: 13.5 KiB, free: 434.4 MiB)
[2025-05-12T17:31:57.712+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.714+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:31:57.716+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.718+0000] {subprocess.py:93} INFO -   "description_number": "1",
[2025-05-12T17:31:57.719+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.720+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase due to easing US-China tensions could reduce safe-haven demand for gold, potentially impacting its price negatively. However, this impact is indirect and uncertain.",
[2025-05-12T17:31:57.722+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.723+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.724+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.725+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:31:57.725+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.726+0000] {subprocess.py:93} INFO -   "description_number": "2",
[2025-05-12T17:31:57.727+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.727+0000] {subprocess.py:93} INFO -   "impact_explanation": "A legal settlement between Google and Texas is unlikely to have a direct impact on the gold market.",
[2025-05-12T17:31:57.732+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.734+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.735+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.735+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:31:57.736+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.737+0000] {subprocess.py:93} INFO -   "description_number": "3",
[2025-05-12T17:31:57.737+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:31:57.737+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold as a safe haven asset during volatile market conditions, suggesting increased demand and potentially higher prices.",
[2025-05-12T17:31:57.738+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:31:57.738+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.739+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.739+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:31:57.739+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.740+0000] {subprocess.py:93} INFO -   "description_number": "4",
[2025-05-12T17:31:57.740+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.741+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description is irrelevant to the gold market.",
[2025-05-12T17:31:57.742+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.743+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.744+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.747+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:31:57.748+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.748+0000] {subprocess.py:93} INFO -   "description_number": "5",
[2025-05-12T17:31:57.749+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:31:57.749+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar typically strengthens the price of dollar-denominated assets like gold, as it becomes cheaper for investors using other currencies.",
[2025-05-12T17:31:57.750+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:31:57.750+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.750+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.750+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:31:57.751+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.751+0000] {subprocess.py:93} INFO -   "description_number": "6",
[2025-05-12T17:31:57.751+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.752+0000] {subprocess.py:93} INFO -   "impact_explanation": "Political news might indirectly affect market sentiment, but this specific description lacks a clear impact on gold.",
[2025-05-12T17:31:57.752+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.753+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.753+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.753+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:31:57.754+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.754+0000] {subprocess.py:93} INFO -   "description_number": "7",
[2025-05-12T17:31:57.755+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.755+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description about Web3 investment is irrelevant to the gold market.",
[2025-05-12T17:31:57.756+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.756+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.757+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.757+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:31:57.758+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.758+0000] {subprocess.py:93} INFO -   "description_number": "8",
[2025-05-12T17:31:57.759+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.760+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description is irrelevant to the gold market.",
[2025-05-12T17:31:57.762+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.762+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.763+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.764+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:31:57.764+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.765+0000] {subprocess.py:93} INFO -   "description_number": "9",
[2025-05-12T17:31:57.765+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.766+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description is irrelevant to the gold market.",
[2025-05-12T17:31:57.766+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.767+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.767+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.768+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:31:57.769+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.769+0000] {subprocess.py:93} INFO -   "description_number": "10",
[2025-05-12T17:31:57.770+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.771+0000] {subprocess.py:93} INFO -   "impact_explanation": "The news about an XRP spot ETF is unlikely to directly affect gold prices.",
[2025-05-12T17:31:57.772+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.772+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.773+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.773+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:31:57.774+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.776+0000] {subprocess.py:93} INFO -   "description_number": "11",
[2025-05-12T17:31:57.778+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.779+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase due to easing US-China tensions could reduce safe-haven demand for gold, potentially impacting its price negatively. However, this impact is indirect and uncertain.",
[2025-05-12T17:31:57.780+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.780+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.781+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.781+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:31:57.782+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.782+0000] {subprocess.py:93} INFO -   "description_number": "12",
[2025-05-12T17:31:57.783+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.783+0000] {subprocess.py:93} INFO -   "impact_explanation": "A legal settlement between Google and Texas is unlikely to have a direct impact on the gold market.",
[2025-05-12T17:31:57.783+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.784+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.784+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.785+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:31:57.785+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.786+0000] {subprocess.py:93} INFO -   "description_number": "13",
[2025-05-12T17:31:57.786+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:31:57.787+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold as a safe haven asset during volatile market conditions, suggesting increased demand and potentially higher prices.",
[2025-05-12T17:31:57.787+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:31:57.788+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.788+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.788+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:31:57.789+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.790+0000] {subprocess.py:93} INFO -   "description_number": "14",
[2025-05-12T17:31:57.791+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.792+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description is irrelevant to the gold market.",
[2025-05-12T17:31:57.793+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.794+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.794+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.795+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:31:57.796+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.796+0000] {subprocess.py:93} INFO -   "description_number": "15",
[2025-05-12T17:31:57.797+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:31:57.797+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar typically strengthens the price of dollar-denominated assets like gold, as it becomes cheaper for investors using other currencies.",
[2025-05-12T17:31:57.798+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:31:57.798+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.799+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.799+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:31:57.799+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.800+0000] {subprocess.py:93} INFO -   "description_number": "16",
[2025-05-12T17:31:57.800+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.800+0000] {subprocess.py:93} INFO -   "impact_explanation": "Political news might indirectly affect market sentiment, but this specific description lacks a clear impact on gold.",
[2025-05-12T17:31:57.801+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.801+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.802+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.802+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:31:57.802+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.803+0000] {subprocess.py:93} INFO -   "description_number": "17",
[2025-05-12T17:31:57.803+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.804+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description about Web3 investment is irrelevant to the gold market.",
[2025-05-12T17:31:57.805+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.806+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.810+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.810+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:31:57.811+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.812+0000] {subprocess.py:93} INFO -   "description_number": "18",
[2025-05-12T17:31:57.813+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.813+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description is irrelevant to the gold market.",
[2025-05-12T17:31:57.814+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.814+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.815+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.815+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:31:57.816+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.817+0000] {subprocess.py:93} INFO -   "description_number": "19",
[2025-05-12T17:31:57.818+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.819+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description is irrelevant to the gold market.",
[2025-05-12T17:31:57.823+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.825+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.827+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.828+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:31:57.829+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.830+0000] {subprocess.py:93} INFO -   "description_number": "20",
[2025-05-12T17:31:57.831+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.832+0000] {subprocess.py:93} INFO -   "impact_explanation": "The news about an XRP spot ETF is unlikely to directly affect gold prices.",
[2025-05-12T17:31:57.833+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.833+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.834+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.834+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:31:57.837+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.840+0000] {subprocess.py:93} INFO -   "description_number": "21",
[2025-05-12T17:31:57.841+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.843+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase due to easing US-China tensions could reduce safe-haven demand for gold, potentially impacting its price negatively. However, this impact is indirect and uncertain.",
[2025-05-12T17:31:57.844+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.844+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.845+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.846+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:31:57.847+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.848+0000] {subprocess.py:93} INFO -   "description_number": "22",
[2025-05-12T17:31:57.848+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.849+0000] {subprocess.py:93} INFO -   "impact_explanation": "A legal settlement between Google and Texas is unlikely to have a direct impact on the gold market.",
[2025-05-12T17:31:57.854+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.855+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.856+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.857+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:31:57.859+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.859+0000] {subprocess.py:93} INFO -   "description_number": "23",
[2025-05-12T17:31:57.860+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:31:57.861+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold as a safe haven asset during volatile market conditions, suggesting increased demand and potentially higher prices.",
[2025-05-12T17:31:57.862+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:31:57.863+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.864+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.865+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:31:57.869+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.870+0000] {subprocess.py:93} INFO -   "description_number": "24",
[2025-05-12T17:31:57.871+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.871+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description is irrelevant to the gold market.",
[2025-05-12T17:31:57.872+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.872+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.873+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.874+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:31:57.874+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.875+0000] {subprocess.py:93} INFO -   "description_number": "25",
[2025-05-12T17:31:57.875+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:31:57.876+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar typically strengthens the price of dollar-denominated assets like gold, as it becomes cheaper for investors using other currencies.",
[2025-05-12T17:31:57.876+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:31:57.877+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.877+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.878+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:31:57.878+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.879+0000] {subprocess.py:93} INFO -   "description_number": "26",
[2025-05-12T17:31:57.880+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.881+0000] {subprocess.py:93} INFO -   "impact_explanation": "Political news might indirectly affect market sentiment, but this specific description lacks a clear impact on gold.",
[2025-05-12T17:31:57.882+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.883+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.885+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.886+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:31:57.887+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.887+0000] {subprocess.py:93} INFO -   "description_number": "27",
[2025-05-12T17:31:57.888+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.889+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description about Web3 investment is irrelevant to the gold market.",
[2025-05-12T17:31:57.890+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.890+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.891+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.892+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:31:57.893+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.893+0000] {subprocess.py:93} INFO -   "description_number": "28",
[2025-05-12T17:31:57.894+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.895+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description is irrelevant to the gold market.",
[2025-05-12T17:31:57.897+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.902+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.903+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.904+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:31:57.905+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.905+0000] {subprocess.py:93} INFO -   "description_number": "29",
[2025-05-12T17:31:57.906+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.906+0000] {subprocess.py:93} INFO -   "impact_explanation": "This description is irrelevant to the gold market.",
[2025-05-12T17:31:57.907+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.908+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:57.908+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:31:57.909+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:31:57.910+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:31:57.910+0000] {subprocess.py:93} INFO -   "description_number": "30",
[2025-05-12T17:31:57.915+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:31:57.916+0000] {subprocess.py:93} INFO -   "impact_explanation": "The news about an XRP spot ETF is unlikely to directly affect gold prices.",
[2025-05-12T17:31:57.918+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:31:57.919+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:31:58.214+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO CodeGenerator: Code generated in 26.755945 ms
[2025-05-12T17:31:58.226+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO AppendDataExec: Start processing data source write support: CassandraBulkWrite(org.apache.spark.sql.SparkSession@53ee4181,com.datastax.spark.connector.cql.CassandraConnector@3382bd49,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(description,StringType,true),StructField(id,StringType,true),StructField(ingestion_time,StringType,true),StructField(published_at,StringType,true),StructField(recommendation,StringType,true),StructField(sentiment,StringType,true),StructField(source,StringType,true),StructField(title,StringType,true),StructField(url,StringType,true)),org.apache.spark.SparkConf@4549d4b5). The input RDD has 8 partitions.
[2025-05-12T17:31:58.237+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO SparkContext: Starting job: save at <unknown>:0
[2025-05-12T17:31:58.239+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO DAGScheduler: Got job 3 (save at <unknown>:0) with 8 output partitions
[2025-05-12T17:31:58.240+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO DAGScheduler: Final stage: ResultStage 3 (save at <unknown>:0)
[2025-05-12T17:31:58.240+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:31:58.257+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:31:58.258+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at save at <unknown>:0), which has no missing parents
[2025-05-12T17:31:58.282+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.8 KiB, free 434.3 MiB)
[2025-05-12T17:31:58.350+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 434.3 MiB)
[2025-05-12T17:31:58.359+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on d4e9ca837c07:43209 (size: 10.7 KiB, free: 434.4 MiB)
[2025-05-12T17:31:58.361+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:31:58.363+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on d4e9ca837c07:43209 in memory (size: 16.1 KiB, free: 434.4 MiB)
[2025-05-12T17:31:58.364+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at save at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T17:31:58.365+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO TaskSchedulerImpl: Adding task set 3.0 with 8 tasks resource profile 0
[2025-05-12T17:31:58.368+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 14584 bytes)
[2025-05-12T17:31:58.377+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (d4e9ca837c07, executor driver, partition 1, PROCESS_LOCAL, 14234 bytes)
[2025-05-12T17:31:58.378+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5) (d4e9ca837c07, executor driver, partition 2, PROCESS_LOCAL, 14247 bytes)
[2025-05-12T17:31:58.380+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 6) (d4e9ca837c07, executor driver, partition 3, PROCESS_LOCAL, 15813 bytes)
[2025-05-12T17:31:58.382+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 7) (d4e9ca837c07, executor driver, partition 4, PROCESS_LOCAL, 14315 bytes)
[2025-05-12T17:31:58.383+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 8) (d4e9ca837c07, executor driver, partition 5, PROCESS_LOCAL, 14265 bytes)
[2025-05-12T17:31:58.383+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 9) (d4e9ca837c07, executor driver, partition 6, PROCESS_LOCAL, 14476 bytes)
[2025-05-12T17:31:58.386+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 10) (d4e9ca837c07, executor driver, partition 7, PROCESS_LOCAL, 15480 bytes)
[2025-05-12T17:31:58.395+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-05-12T17:31:58.408+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
[2025-05-12T17:31:58.427+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO Executor: Running task 2.0 in stage 3.0 (TID 5)
[2025-05-12T17:31:58.434+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO Executor: Running task 3.0 in stage 3.0 (TID 6)
[2025-05-12T17:31:58.435+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO Executor: Running task 4.0 in stage 3.0 (TID 7)
[2025-05-12T17:31:58.456+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO Executor: Running task 6.0 in stage 3.0 (TID 9)
[2025-05-12T17:31:58.465+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO Executor: Running task 5.0 in stage 3.0 (TID 8)
[2025-05-12T17:31:58.471+0000] {subprocess.py:93} INFO - 25/05/12 17:31:58 INFO Executor: Running task 7.0 in stage 3.0 (TID 10)
[2025-05-12T17:31:59.434+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO CodeGenerator: Code generated in 28.508486 ms
[2025-05-12T17:31:59.523+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO PythonRunner: Times: total = 957, boot = 898, init = 58, finish = 1
[2025-05-12T17:31:59.533+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO PythonRunner: Times: total = 883, boot = 824, init = 59, finish = 0
[2025-05-12T17:31:59.536+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Commit authorized for partition 2 (task 5, attempt 0, stage 3.0)
[2025-05-12T17:31:59.538+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Commit authorized for partition 6 (task 9, attempt 0, stage 3.0)
[2025-05-12T17:31:59.568+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO PythonRunner: Times: total = 956, boot = 860, init = 96, finish = 0
[2025-05-12T17:31:59.569+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Committed partition 6 (task 9, attempt 0, stage 3.0)
[2025-05-12T17:31:59.570+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Commit authorized for partition 5 (task 8, attempt 0, stage 3.0)
[2025-05-12T17:31:59.611+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Committed partition 2 (task 5, attempt 0, stage 3.0)
[2025-05-12T17:31:59.613+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO PythonRunner: Times: total = 946, boot = 876, init = 70, finish = 0
[2025-05-12T17:31:59.614+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-05-12T17:31:59.618+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO Executor: Finished task 2.0 in stage 3.0 (TID 5). 1939 bytes result sent to driver
[2025-05-12T17:31:59.636+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO Executor: Finished task 6.0 in stage 3.0 (TID 9). 1939 bytes result sent to driver
[2025-05-12T17:31:59.641+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 1266 ms on d4e9ca837c07 (executor driver) (1/8)
[2025-05-12T17:31:59.644+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 42191
[2025-05-12T17:31:59.645+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Committed partition 5 (task 8, attempt 0, stage 3.0)
[2025-05-12T17:31:59.658+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO PythonRunner: Times: total = 945, boot = 868, init = 76, finish = 1
[2025-05-12T17:31:59.659+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Commit authorized for partition 7 (task 10, attempt 0, stage 3.0)
[2025-05-12T17:31:59.661+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 9) in 1266 ms on d4e9ca837c07 (executor driver) (2/8)
[2025-05-12T17:31:59.662+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO Executor: Finished task 5.0 in stage 3.0 (TID 8). 1896 bytes result sent to driver
[2025-05-12T17:31:59.672+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-05-12T17:31:59.676+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1896 bytes result sent to driver
[2025-05-12T17:31:59.679+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 8) in 1288 ms on d4e9ca837c07 (executor driver) (3/8)
[2025-05-12T17:31:59.681+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1315 ms on d4e9ca837c07 (executor driver) (4/8)
[2025-05-12T17:31:59.696+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO PythonRunner: Times: total = 953, boot = 906, init = 46, finish = 1
[2025-05-12T17:31:59.704+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Commit authorized for partition 1 (task 4, attempt 0, stage 3.0)
[2025-05-12T17:31:59.711+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Committed partition 7 (task 10, attempt 0, stage 3.0)
[2025-05-12T17:31:59.717+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO Executor: Finished task 7.0 in stage 3.0 (TID 10). 1896 bytes result sent to driver
[2025-05-12T17:31:59.725+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO PythonRunner: Times: total = 951, boot = 890, init = 61, finish = 0
[2025-05-12T17:31:59.726+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Committed partition 1 (task 4, attempt 0, stage 3.0)
[2025-05-12T17:31:59.733+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Commit authorized for partition 4 (task 7, attempt 0, stage 3.0)
[2025-05-12T17:31:59.734+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 10) in 1344 ms on d4e9ca837c07 (executor driver) (5/8)
[2025-05-12T17:31:59.735+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 1896 bytes result sent to driver
[2025-05-12T17:31:59.736+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 1360 ms on d4e9ca837c07 (executor driver) (6/8)
[2025-05-12T17:31:59.737+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO PythonRunner: Times: total = 893, boot = 813, init = 79, finish = 1
[2025-05-12T17:31:59.738+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Commit authorized for partition 3 (task 6, attempt 0, stage 3.0)
[2025-05-12T17:31:59.742+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Committed partition 4 (task 7, attempt 0, stage 3.0)
[2025-05-12T17:31:59.750+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO Executor: Finished task 4.0 in stage 3.0 (TID 7). 1939 bytes result sent to driver
[2025-05-12T17:31:59.776+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 7) in 1391 ms on d4e9ca837c07 (executor driver) (7/8)
[2025-05-12T17:31:59.800+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DataWritingSparkTask: Committed partition 3 (task 6, attempt 0, stage 3.0)
[2025-05-12T17:31:59.810+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO Executor: Finished task 3.0 in stage 3.0 (TID 6). 1939 bytes result sent to driver
[2025-05-12T17:31:59.814+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 6) in 1437 ms on d4e9ca837c07 (executor driver) (8/8)
[2025-05-12T17:31:59.817+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-05-12T17:31:59.849+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DAGScheduler: ResultStage 3 (save at <unknown>:0) finished in 1.595 s
[2025-05-12T17:31:59.856+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:31:59.859+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-05-12T17:31:59.860+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO DAGScheduler: Job 3 finished: save at <unknown>:0, took 1.615735 s
[2025-05-12T17:31:59.861+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@53ee4181,com.datastax.spark.connector.cql.CassandraConnector@3382bd49,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(description,StringType,true),StructField(id,StringType,true),StructField(ingestion_time,StringType,true),StructField(published_at,StringType,true),StructField(recommendation,StringType,true),StructField(sentiment,StringType,true),StructField(source,StringType,true),StructField(title,StringType,true),StructField(url,StringType,true)),org.apache.spark.SparkConf@4549d4b5) is committing.
[2025-05-12T17:31:59.862+0000] {subprocess.py:93} INFO - 25/05/12 17:31:59 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@53ee4181,com.datastax.spark.connector.cql.CassandraConnector@3382bd49,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(description,StringType,true),StructField(id,StringType,true),StructField(ingestion_time,StringType,true),StructField(published_at,StringType,true),StructField(recommendation,StringType,true),StructField(sentiment,StringType,true),StructField(source,StringType,true),StructField(title,StringType,true),StructField(url,StringType,true)),org.apache.spark.SparkConf@4549d4b5) committed.
[2025-05-12T17:32:00.627+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO CodeGenerator: Code generated in 63.962857 ms
[2025-05-12T17:32:00.714+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO DAGScheduler: Registering RDD 20 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) as input to shuffle 0
[2025-05-12T17:32:00.729+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO DAGScheduler: Got map stage job 4 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 8 output partitions
[2025-05-12T17:32:00.731+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:32:00.732+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:32:00.739+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:32:00.756+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:32:00.814+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 25.9 KiB, free 434.3 MiB)
[2025-05-12T17:32:00.850+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 434.3 MiB)
[2025-05-12T17:32:00.855+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on d4e9ca837c07:43209 (size: 11.6 KiB, free: 434.4 MiB)
[2025-05-12T17:32:00.856+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:32:00.863+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T17:32:00.864+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO TaskSchedulerImpl: Adding task set 4.0 with 8 tasks resource profile 0
[2025-05-12T17:32:00.865+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on d4e9ca837c07:43209 in memory (size: 10.7 KiB, free: 434.4 MiB)
[2025-05-12T17:32:00.869+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 14573 bytes)
[2025-05-12T17:32:00.869+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 12) (d4e9ca837c07, executor driver, partition 1, PROCESS_LOCAL, 14223 bytes)
[2025-05-12T17:32:00.876+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 13) (d4e9ca837c07, executor driver, partition 2, PROCESS_LOCAL, 14236 bytes)
[2025-05-12T17:32:00.877+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 14) (d4e9ca837c07, executor driver, partition 3, PROCESS_LOCAL, 15802 bytes)
[2025-05-12T17:32:00.880+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 15) (d4e9ca837c07, executor driver, partition 4, PROCESS_LOCAL, 14304 bytes)
[2025-05-12T17:32:00.882+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 16) (d4e9ca837c07, executor driver, partition 5, PROCESS_LOCAL, 14254 bytes)
[2025-05-12T17:32:00.885+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 17) (d4e9ca837c07, executor driver, partition 6, PROCESS_LOCAL, 14465 bytes)
[2025-05-12T17:32:00.887+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 18) (d4e9ca837c07, executor driver, partition 7, PROCESS_LOCAL, 15469 bytes)
[2025-05-12T17:32:00.893+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO Executor: Running task 1.0 in stage 4.0 (TID 12)
[2025-05-12T17:32:00.895+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO Executor: Running task 3.0 in stage 4.0 (TID 14)
[2025-05-12T17:32:00.897+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO Executor: Running task 0.0 in stage 4.0 (TID 11)
[2025-05-12T17:32:00.898+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO Executor: Running task 2.0 in stage 4.0 (TID 13)
[2025-05-12T17:32:00.899+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO Executor: Running task 4.0 in stage 4.0 (TID 15)
[2025-05-12T17:32:00.901+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO Executor: Running task 5.0 in stage 4.0 (TID 16)
[2025-05-12T17:32:00.912+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO Executor: Running task 6.0 in stage 4.0 (TID 17)
[2025-05-12T17:32:00.914+0000] {subprocess.py:93} INFO - 25/05/12 17:32:00 INFO Executor: Running task 7.0 in stage 4.0 (TID 18)
[2025-05-12T17:32:01.096+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO CodeGenerator: Code generated in 61.527446 ms
[2025-05-12T17:32:01.189+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO PythonRunner: Times: total = 119, boot = -1451, init = 1569, finish = 1
[2025-05-12T17:32:01.189+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO PythonRunner: Times: total = 80, boot = -1526, init = 1605, finish = 1
[2025-05-12T17:32:01.191+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO PythonRunner: Times: total = 138, boot = -1442, init = 1580, finish = 0
[2025-05-12T17:32:01.192+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO PythonRunner: Times: total = 110, boot = -1518, init = 1628, finish = 0
[2025-05-12T17:32:01.194+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO PythonRunner: Times: total = 107, boot = -1500, init = 1607, finish = 0
[2025-05-12T17:32:01.198+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO PythonRunner: Times: total = 87, boot = -1433, init = 1520, finish = 0
[2025-05-12T17:32:01.199+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO PythonRunner: Times: total = 134, boot = -1548, init = 1682, finish = 0
[2025-05-12T17:32:01.200+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO PythonRunner: Times: total = 53, boot = -1422, init = 1474, finish = 1
[2025-05-12T17:32:01.291+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Finished task 1.0 in stage 4.0 (TID 12). 2362 bytes result sent to driver
[2025-05-12T17:32:01.292+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Finished task 6.0 in stage 4.0 (TID 17). 2362 bytes result sent to driver
[2025-05-12T17:32:01.293+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Finished task 7.0 in stage 4.0 (TID 18). 2362 bytes result sent to driver
[2025-05-12T17:32:01.294+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 17) in 409 ms on d4e9ca837c07 (executor driver) (1/8)
[2025-05-12T17:32:01.296+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 12) in 426 ms on d4e9ca837c07 (executor driver) (2/8)
[2025-05-12T17:32:01.309+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 18) in 412 ms on d4e9ca837c07 (executor driver) (3/8)
[2025-05-12T17:32:01.310+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Finished task 2.0 in stage 4.0 (TID 13). 2362 bytes result sent to driver
[2025-05-12T17:32:01.312+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Finished task 4.0 in stage 4.0 (TID 15). 2362 bytes result sent to driver
[2025-05-12T17:32:01.312+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Finished task 3.0 in stage 4.0 (TID 14). 2362 bytes result sent to driver
[2025-05-12T17:32:01.314+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Finished task 5.0 in stage 4.0 (TID 16). 2362 bytes result sent to driver
[2025-05-12T17:32:01.322+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Finished task 0.0 in stage 4.0 (TID 11). 2362 bytes result sent to driver
[2025-05-12T17:32:01.325+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 13) in 438 ms on d4e9ca837c07 (executor driver) (4/8)
[2025-05-12T17:32:01.326+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 15) in 437 ms on d4e9ca837c07 (executor driver) (5/8)
[2025-05-12T17:32:01.327+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 452 ms on d4e9ca837c07 (executor driver) (6/8)
[2025-05-12T17:32:01.327+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 16) in 443 ms on d4e9ca837c07 (executor driver) (7/8)
[2025-05-12T17:32:01.329+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 14) in 451 ms on d4e9ca837c07 (executor driver) (8/8)
[2025-05-12T17:32:01.331+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-05-12T17:32:01.338+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: ShuffleMapStage 4 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.567 s
[2025-05-12T17:32:01.340+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: looking for newly runnable stages
[2025-05-12T17:32:01.342+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: running: Set()
[2025-05-12T17:32:01.343+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: waiting: Set()
[2025-05-12T17:32:01.343+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: failed: Set()
[2025-05-12T17:32:01.512+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO CodeGenerator: Code generated in 48.018154 ms
[2025-05-12T17:32:01.583+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T17:32:01.587+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: Got job 5 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T17:32:01.588+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: Final stage: ResultStage 6 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:32:01.589+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2025-05-12T17:32:01.594+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:32:01.597+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:32:01.630+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.1 KiB, free 434.3 MiB)
[2025-05-12T17:32:01.657+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.3 MiB)
[2025-05-12T17:32:01.670+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on d4e9ca837c07:43209 (size: 7.4 KiB, free: 434.4 MiB)
[2025-05-12T17:32:01.674+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:32:01.676+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:32:01.678+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-05-12T17:32:01.681+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO BlockManagerInfo: Removed broadcast_4_piece0 on d4e9ca837c07:43209 in memory (size: 11.6 KiB, free: 434.4 MiB)
[2025-05-12T17:32:01.732+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 19) (d4e9ca837c07, executor driver, partition 0, NODE_LOCAL, 12874 bytes)
[2025-05-12T17:32:01.734+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Running task 0.0 in stage 6.0 (TID 19)
[2025-05-12T17:32:01.864+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO ShuffleBlockFetcherIterator: Getting 8 (624.0 B) non-empty blocks including 8 (624.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-12T17:32:01.870+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms
[2025-05-12T17:32:01.906+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO CodeGenerator: Code generated in 28.664864 ms
[2025-05-12T17:32:01.962+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 19). 4056 bytes result sent to driver
[2025-05-12T17:32:01.967+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 19) in 267 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:32:01.976+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-05-12T17:32:01.979+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: ResultStage 6 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.351 s
[2025-05-12T17:32:01.980+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:32:01.982+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-05-12T17:32:01.983+0000] {subprocess.py:93} INFO - 25/05/12 17:32:01 INFO DAGScheduler: Job 5 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.394497 s
[2025-05-12T17:32:02.276+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO CodeGenerator: Code generated in 92.114519 ms
[2025-05-12T17:32:02.303+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: Registering RDD 25 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) as input to shuffle 1
[2025-05-12T17:32:02.304+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: Got map stage job 6 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 8 output partitions
[2025-05-12T17:32:02.305+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:32:02.306+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:32:02.306+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:32:02.309+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[25] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:32:02.320+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.3 KiB, free 434.3 MiB)
[2025-05-12T17:32:02.350+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 434.3 MiB)
[2025-05-12T17:32:02.357+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on d4e9ca837c07:43209 (size: 18.4 KiB, free: 434.4 MiB)
[2025-05-12T17:32:02.359+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO BlockManagerInfo: Removed broadcast_5_piece0 on d4e9ca837c07:43209 in memory (size: 7.4 KiB, free: 434.4 MiB)
[2025-05-12T17:32:02.360+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:32:02.363+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[25] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T17:32:02.364+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSchedulerImpl: Adding task set 7.0 with 8 tasks resource profile 0
[2025-05-12T17:32:02.368+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 20) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 14573 bytes)
[2025-05-12T17:32:02.374+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 21) (d4e9ca837c07, executor driver, partition 1, PROCESS_LOCAL, 14223 bytes)
[2025-05-12T17:32:02.379+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 22) (d4e9ca837c07, executor driver, partition 2, PROCESS_LOCAL, 14236 bytes)
[2025-05-12T17:32:02.380+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 23) (d4e9ca837c07, executor driver, partition 3, PROCESS_LOCAL, 15802 bytes)
[2025-05-12T17:32:02.382+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 24) (d4e9ca837c07, executor driver, partition 4, PROCESS_LOCAL, 14304 bytes)
[2025-05-12T17:32:02.387+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 25) (d4e9ca837c07, executor driver, partition 5, PROCESS_LOCAL, 14254 bytes)
[2025-05-12T17:32:02.388+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 26) (d4e9ca837c07, executor driver, partition 6, PROCESS_LOCAL, 14465 bytes)
[2025-05-12T17:32:02.391+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 27) (d4e9ca837c07, executor driver, partition 7, PROCESS_LOCAL, 15469 bytes)
[2025-05-12T17:32:02.392+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Running task 0.0 in stage 7.0 (TID 20)
[2025-05-12T17:32:02.393+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Running task 2.0 in stage 7.0 (TID 22)
[2025-05-12T17:32:02.394+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Running task 3.0 in stage 7.0 (TID 23)
[2025-05-12T17:32:02.394+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Running task 1.0 in stage 7.0 (TID 21)
[2025-05-12T17:32:02.423+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Running task 4.0 in stage 7.0 (TID 24)
[2025-05-12T17:32:02.424+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Running task 5.0 in stage 7.0 (TID 25)
[2025-05-12T17:32:02.425+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Running task 6.0 in stage 7.0 (TID 26)
[2025-05-12T17:32:02.425+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Running task 7.0 in stage 7.0 (TID 27)
[2025-05-12T17:32:02.583+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO CodeGenerator: Code generated in 129.613261 ms
[2025-05-12T17:32:02.611+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO CodeGenerator: Code generated in 13.612019 ms
[2025-05-12T17:32:02.630+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO CodeGenerator: Code generated in 7.181484 ms
[2025-05-12T17:32:02.673+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO CodeGenerator: Code generated in 14.226171 ms
[2025-05-12T17:32:02.700+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO CodeGenerator: Code generated in 15.728553 ms
[2025-05-12T17:32:02.722+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO PythonRunner: Times: total = 68, boot = -1303, init = 1371, finish = 0
[2025-05-12T17:32:02.723+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO PythonRunner: Times: total = 68, boot = -1293, init = 1361, finish = 0
[2025-05-12T17:32:02.726+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO PythonRunner: Times: total = 47, boot = -1282, init = 1329, finish = 0
[2025-05-12T17:32:02.727+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO PythonRunner: Times: total = 77, boot = -1259, init = 1336, finish = 0
[2025-05-12T17:32:02.732+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO PythonRunner: Times: total = 59, boot = -1330, init = 1389, finish = 0
[2025-05-12T17:32:02.733+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO PythonRunner: Times: total = 90, boot = -1412, init = 1502, finish = 0
[2025-05-12T17:32:02.737+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO PythonRunner: Times: total = 59, boot = -1276, init = 1335, finish = 0
[2025-05-12T17:32:02.741+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO PythonRunner: Times: total = 68, boot = -1324, init = 1392, finish = 0
[2025-05-12T17:32:02.838+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Finished task 1.0 in stage 7.0 (TID 21). 2925 bytes result sent to driver
[2025-05-12T17:32:02.845+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Finished task 6.0 in stage 7.0 (TID 26). 2925 bytes result sent to driver
[2025-05-12T17:32:02.846+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Finished task 4.0 in stage 7.0 (TID 24). 2925 bytes result sent to driver
[2025-05-12T17:32:02.848+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 21) in 477 ms on d4e9ca837c07 (executor driver) (1/8)
[2025-05-12T17:32:02.849+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 26) in 465 ms on d4e9ca837c07 (executor driver) (2/8)
[2025-05-12T17:32:02.850+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Finished task 0.0 in stage 7.0 (TID 20). 2882 bytes result sent to driver
[2025-05-12T17:32:02.852+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 24) in 470 ms on d4e9ca837c07 (executor driver) (3/8)
[2025-05-12T17:32:02.853+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 20) in 488 ms on d4e9ca837c07 (executor driver) (4/8)
[2025-05-12T17:32:02.856+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Finished task 5.0 in stage 7.0 (TID 25). 2882 bytes result sent to driver
[2025-05-12T17:32:02.861+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 25) in 478 ms on d4e9ca837c07 (executor driver) (5/8)
[2025-05-12T17:32:02.862+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Finished task 2.0 in stage 7.0 (TID 22). 2882 bytes result sent to driver
[2025-05-12T17:32:02.875+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 22) in 493 ms on d4e9ca837c07 (executor driver) (6/8)
[2025-05-12T17:32:02.877+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Finished task 7.0 in stage 7.0 (TID 27). 2882 bytes result sent to driver
[2025-05-12T17:32:02.883+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO Executor: Finished task 3.0 in stage 7.0 (TID 23). 2882 bytes result sent to driver
[2025-05-12T17:32:02.885+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 23) in 503 ms on d4e9ca837c07 (executor driver) (7/8)
[2025-05-12T17:32:02.893+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 27) in 501 ms on d4e9ca837c07 (executor driver) (8/8)
[2025-05-12T17:32:02.895+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-05-12T17:32:02.896+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: ShuffleMapStage 7 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.580 s
[2025-05-12T17:32:02.896+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: looking for newly runnable stages
[2025-05-12T17:32:02.897+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: running: Set()
[2025-05-12T17:32:02.897+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: waiting: Set()
[2025-05-12T17:32:02.898+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO DAGScheduler: failed: Set()
[2025-05-12T17:32:02.961+0000] {subprocess.py:93} INFO - 25/05/12 17:32:02 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-05-12T17:32:03.004+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2025-05-12T17:32:03.030+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO CodeGenerator: Code generated in 13.584517 ms
[2025-05-12T17:32:03.076+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T17:32:03.078+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO DAGScheduler: Got job 7 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T17:32:03.078+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO DAGScheduler: Final stage: ResultStage 9 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:32:03.079+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[2025-05-12T17:32:03.080+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:32:03.081+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[28] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:32:03.087+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 40.5 KiB, free 434.3 MiB)
[2025-05-12T17:32:03.107+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 434.3 MiB)
[2025-05-12T17:32:03.109+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on d4e9ca837c07:43209 (size: 19.5 KiB, free: 434.4 MiB)
[2025-05-12T17:32:03.110+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:32:03.111+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[28] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:32:03.111+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-05-12T17:32:03.113+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 28) (d4e9ca837c07, executor driver, partition 0, NODE_LOCAL, 12874 bytes)
[2025-05-12T17:32:03.114+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Executor: Running task 0.0 in stage 9.0 (TID 28)
[2025-05-12T17:32:03.141+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ShuffleBlockFetcherIterator: Getting 8 (936.0 B) non-empty blocks including 8 (936.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-12T17:32:03.141+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2025-05-12T17:32:03.166+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO CodeGenerator: Code generated in 20.46087 ms
[2025-05-12T17:32:03.182+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Executor: Finished task 0.0 in stage 9.0 (TID 28). 5263 bytes result sent to driver
[2025-05-12T17:32:03.183+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 28) in 67 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:32:03.184+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-05-12T17:32:03.185+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO DAGScheduler: ResultStage 9 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.100 s
[2025-05-12T17:32:03.185+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:32:03.186+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-05-12T17:32:03.186+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO DAGScheduler: Job 7 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.109795 s
[2025-05-12T17:32:03.233+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on d4e9ca837c07:43209 in memory (size: 18.4 KiB, free: 434.4 MiB)
[2025-05-12T17:32:03.248+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 ERROR MicroBatchExecution: Query [id = 06a77141-55f5-4c1a-aba2-499a68b82bf8, runId = df69921a-42ab-41b7-ab6f-b0e839dc5024] terminated with error
[2025-05-12T17:32:03.248+0000] {subprocess.py:93} INFO - py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
[2025-05-12T17:32:03.250+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
[2025-05-12T17:32:03.250+0000] {subprocess.py:93} INFO -     return_value = getattr(self.pool[obj_id], method)(*params)
[2025-05-12T17:32:03.250+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
[2025-05-12T17:32:03.251+0000] {subprocess.py:93} INFO -     raise e
[2025-05-12T17:32:03.251+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
[2025-05-12T17:32:03.252+0000] {subprocess.py:93} INFO -     self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
[2025-05-12T17:32:03.252+0000] {subprocess.py:93} INFO -   File "/scripts/spark_news.py", line 175, in process_batch
[2025-05-12T17:32:03.252+0000] {subprocess.py:93} INFO -     dominant_recommendation = max(recommendation_counts, key=lambda x: x["count"])["recommendation"]
[2025-05-12T17:32:03.253+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 174, in wrapped
[2025-05-12T17:32:03.253+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-12T17:32:03.259+0000] {subprocess.py:93} INFO - TypeError: max() got an unexpected keyword argument 'key'
[2025-05-12T17:32:03.260+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:32:03.261+0000] {subprocess.py:93} INFO - 	at py4j.Protocol.getReturnValue(Protocol.java:476)
[2025-05-12T17:32:03.261+0000] {subprocess.py:93} INFO - 	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
[2025-05-12T17:32:03.262+0000] {subprocess.py:93} INFO - 	at com.sun.proxy.$Proxy37.call(Unknown Source)
[2025-05-12T17:32:03.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
[2025-05-12T17:32:03.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
[2025-05-12T17:32:03.264+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
[2025-05-12T17:32:03.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
[2025-05-12T17:32:03.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-05-12T17:32:03.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-05-12T17:32:03.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-05-12T17:32:03.267+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-05-12T17:32:03.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-05-12T17:32:03.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-05-12T17:32:03.274+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-05-12T17:32:03.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-05-12T17:32:03.276+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-05-12T17:32:03.276+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-05-12T17:32:03.278+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-05-12T17:32:03.278+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-05-12T17:32:03.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-05-12T17:32:03.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-05-12T17:32:03.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-05-12T17:32:03.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-05-12T17:32:03.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
[2025-05-12T17:32:03.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-05-12T17:32:03.282+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-05-12T17:32:03.282+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-05-12T17:32:03.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-05-12T17:32:03.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-05-12T17:32:03.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-05-12T17:32:03.288+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-05-12T17:32:03.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-05-12T17:32:03.289+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-05-12T17:32:03.289+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
[2025-05-12T17:32:03.290+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:32:03.290+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:32:03.290+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Metrics: Metrics reporters closed
[2025-05-12T17:32:03.291+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO MicroBatchExecution: Async log purge executor pool for query [id = 06a77141-55f5-4c1a-aba2-499a68b82bf8, runId = df69921a-42ab-41b7-ab6f-b0e839dc5024] has been shutdown
[2025-05-12T17:32:03.365+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-05-12T17:32:03.366+0000] {subprocess.py:93} INFO -   File "/scripts/spark_news.py", line 228, in <module>
[2025-05-12T17:32:03.368+0000] {subprocess.py:93} INFO -     analysis_query.awaitTermination()
[2025-05-12T17:32:03.369+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
[2025-05-12T17:32:03.370+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-05-12T17:32:03.370+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-05-12T17:32:03.381+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 06a77141-55f5-4c1a-aba2-499a68b82bf8, runId = df69921a-42ab-41b7-ab6f-b0e839dc5024] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
[2025-05-12T17:32:03.382+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
[2025-05-12T17:32:03.383+0000] {subprocess.py:93} INFO -     return_value = getattr(self.pool[obj_id], method)(*params)
[2025-05-12T17:32:03.384+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
[2025-05-12T17:32:03.384+0000] {subprocess.py:93} INFO -     raise e
[2025-05-12T17:32:03.385+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
[2025-05-12T17:32:03.385+0000] {subprocess.py:93} INFO -     self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
[2025-05-12T17:32:03.386+0000] {subprocess.py:93} INFO -   File "/scripts/spark_news.py", line 175, in process_batch
[2025-05-12T17:32:03.386+0000] {subprocess.py:93} INFO -     dominant_recommendation = max(recommendation_counts, key=lambda x: x["count"])["recommendation"]
[2025-05-12T17:32:03.387+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 174, in wrapped
[2025-05-12T17:32:03.387+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-12T17:32:03.387+0000] {subprocess.py:93} INFO - TypeError: max() got an unexpected keyword argument 'key'
[2025-05-12T17:32:03.388+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:32:03.488+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-05-12T17:32:03.490+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2, groupId=spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-05-12T17:32:03.491+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:32:03.492+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:32:03.492+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Metrics: Metrics reporters closed
[2025-05-12T17:32:03.493+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-20be3df1-6ea4-433d-a770-ff464bc2268d-1979046052-executor-2 unregistered
[2025-05-12T17:32:03.495+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-05-12T17:32:03.496+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1, groupId=spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-05-12T17:32:03.496+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:32:03.498+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:32:03.501+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO Metrics: Metrics reporters closed
[2025-05-12T17:32:03.507+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-2d2f4a33-fcf4-427f-bedd-f90df46ac4e2-7807588-executor-1 unregistered
[2025-05-12T17:32:03.509+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO SparkContext: Invoking stop() from shutdown hook
[2025-05-12T17:32:03.510+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-05-12T17:32:03.536+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO CassandraConnector: Disconnected from Cassandra cluster.
[2025-05-12T17:32:03.537+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
[2025-05-12T17:32:03.555+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO SparkUI: Stopped Spark web UI at http://d4e9ca837c07:4040
[2025-05-12T17:32:03.601+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-05-12T17:32:03.645+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO MemoryStore: MemoryStore cleared
[2025-05-12T17:32:03.646+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO BlockManager: BlockManager stopped
[2025-05-12T17:32:03.657+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-05-12T17:32:03.671+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-05-12T17:32:03.690+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO SparkContext: Successfully stopped SparkContext
[2025-05-12T17:32:03.691+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ShutdownHookManager: Shutdown hook called
[2025-05-12T17:32:03.693+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3/pyspark-be639d65-dbfe-4fc0-9811-78e846233721
[2025-05-12T17:32:03.695+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4b04853-0727-42f4-b0df-4483e0d6fc99
[2025-05-12T17:32:03.702+0000] {subprocess.py:93} INFO - 25/05/12 17:32:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-f38e7a2f-0837-4cf3-bc99-f6a42a2f98a3
[2025-05-12T17:32:03.802+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-05-12T17:32:03.815+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-05-12T17:32:03.821+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=gold_news_pipeline, task_id=run_consumer, execution_date=20250512T172103, start_date=20250512T173109, end_date=20250512T173203
[2025-05-12T17:32:03.838+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 85 for task run_consumer (Bash command failed. The command returned a non-zero exit code 1.; 3386)
[2025-05-12T17:32:03.874+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-05-12T17:32:03.906+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
