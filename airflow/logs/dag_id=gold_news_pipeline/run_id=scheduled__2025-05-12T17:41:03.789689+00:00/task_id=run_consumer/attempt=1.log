[2025-05-12T17:51:09.040+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: gold_news_pipeline.run_consumer scheduled__2025-05-12T17:41:03.789689+00:00 [queued]>
[2025-05-12T17:51:09.051+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: gold_news_pipeline.run_consumer scheduled__2025-05-12T17:41:03.789689+00:00 [queued]>
[2025-05-12T17:51:09.051+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-05-12T17:51:09.067+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): run_consumer> on 2025-05-12 17:41:03.789689+00:00
[2025-05-12T17:51:09.072+0000] {standard_task_runner.py:57} INFO - Started process 5451 to run task
[2025-05-12T17:51:09.075+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'gold_news_pipeline', 'run_consumer', 'scheduled__2025-05-12T17:41:03.789689+00:00', '--job-id', '91', '--raw', '--subdir', 'DAGS_FOLDER/dag_news.py', '--cfg-path', '/tmp/tmpf13tayii']
[2025-05-12T17:51:09.079+0000] {standard_task_runner.py:85} INFO - Job 91: Subtask run_consumer
[2025-05-12T17:51:09.130+0000] {task_command.py:415} INFO - Running <TaskInstance: gold_news_pipeline.run_consumer scheduled__2025-05-12T17:41:03.789689+00:00 [running]> on host 51d6065cf9f5
[2025-05-12T17:51:09.212+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='gold_news_pipeline' AIRFLOW_CTX_TASK_ID='run_consumer' AIRFLOW_CTX_EXECUTION_DATE='2025-05-12T17:41:03.789689+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-05-12T17:41:03.789689+00:00'
[2025-05-12T17:51:09.216+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-05-12T17:51:09.218+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec gold_price_project-spark-1 spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 /scripts/spark_news.py']
[2025-05-12T17:51:09.231+0000] {subprocess.py:86} INFO - Output:
[2025-05-12T17:51:11.839+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-05-12T17:51:11.999+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /root/.ivy2/cache
[2025-05-12T17:51:12.000+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /root/.ivy2/jars
[2025-05-12T17:51:12.013+0000] {subprocess.py:93} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-05-12T17:51:12.015+0000] {subprocess.py:93} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2025-05-12T17:51:12.016+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-6887b19d-b2a2-45d5-8e2f-ad34fc35d008;1.0
[2025-05-12T17:51:12.016+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-05-12T17:51:12.349+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in local-m2-cache
[2025-05-12T17:51:12.498+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2025-05-12T17:51:12.546+0000] {subprocess.py:93} INFO - 	found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-05-12T17:51:12.597+0000] {subprocess.py:93} INFO - 	found org.lz4#lz4-java;1.8.0 in central
[2025-05-12T17:51:12.640+0000] {subprocess.py:93} INFO - 	found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2025-05-12T17:51:12.681+0000] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;2.0.7 in central
[2025-05-12T17:51:12.720+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-05-12T17:51:12.774+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-05-12T17:51:12.826+0000] {subprocess.py:93} INFO - 	found commons-logging#commons-logging;1.1.3 in central
[2025-05-12T17:51:12.847+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-05-12T17:51:12.872+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-pool2;2.11.1 in central
[2025-05-12T17:51:12.898+0000] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central
[2025-05-12T17:51:12.917+0000] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central
[2025-05-12T17:51:12.952+0000] {subprocess.py:93} INFO - 	found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2025-05-12T17:51:12.977+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2025-05-12T17:51:13.005+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#native-protocol;1.5.0 in central
[2025-05-12T17:51:13.025+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2025-05-12T17:51:13.044+0000] {subprocess.py:93} INFO - 	found com.typesafe#config;1.4.1 in central
[2025-05-12T17:51:13.070+0000] {subprocess.py:93} INFO - 	found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2025-05-12T17:51:13.091+0000] {subprocess.py:93} INFO - 	found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2025-05-12T17:51:13.102+0000] {subprocess.py:93} INFO - 	found org.reactivestreams#reactive-streams;1.0.3 in central
[2025-05-12T17:51:13.120+0000] {subprocess.py:93} INFO - 	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-05-12T17:51:13.140+0000] {subprocess.py:93} INFO - 	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2025-05-12T17:51:13.160+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.2 in central
[2025-05-12T17:51:13.185+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2025-05-12T17:51:13.216+0000] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2025-05-12T17:51:13.251+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-lang3;3.10 in central
[2025-05-12T17:51:13.267+0000] {subprocess.py:93} INFO - 	found com.thoughtworks.paranamer#paranamer;2.8 in central
[2025-05-12T17:51:13.285+0000] {subprocess.py:93} INFO - 	found org.scala-lang#scala-reflect;2.12.11 in central
[2025-05-12T17:51:13.343+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 1297ms :: artifacts dl 30ms
[2025-05-12T17:51:13.345+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-05-12T17:51:13.346+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2025-05-12T17:51:13.347+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2025-05-12T17:51:13.351+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2025-05-12T17:51:13.352+0000] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2025-05-12T17:51:13.352+0000] {subprocess.py:93} INFO - 	com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2025-05-12T17:51:13.353+0000] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]
[2025-05-12T17:51:13.354+0000] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]
[2025-05-12T17:51:13.354+0000] {subprocess.py:93} INFO - 	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2025-05-12T17:51:13.355+0000] {subprocess.py:93} INFO - 	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-05-12T17:51:13.356+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2025-05-12T17:51:13.356+0000] {subprocess.py:93} INFO - 	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2025-05-12T17:51:13.357+0000] {subprocess.py:93} INFO - 	com.typesafe#config;1.4.1 from central in [default]
[2025-05-12T17:51:13.358+0000] {subprocess.py:93} INFO - 	commons-logging#commons-logging;1.1.3 from central in [default]
[2025-05-12T17:51:13.358+0000] {subprocess.py:93} INFO - 	io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2025-05-12T17:51:13.359+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-lang3;3.10 from central in [default]
[2025-05-12T17:51:13.359+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2025-05-12T17:51:13.360+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2025-05-12T17:51:13.360+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-05-12T17:51:13.361+0000] {subprocess.py:93} INFO - 	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
[2025-05-12T17:51:13.361+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from local-m2-cache in [default]
[2025-05-12T17:51:13.362+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
[2025-05-12T17:51:13.363+0000] {subprocess.py:93} INFO - 	org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2025-05-12T17:51:13.363+0000] {subprocess.py:93} INFO - 	org.lz4#lz4-java;1.8.0 from central in [default]
[2025-05-12T17:51:13.364+0000] {subprocess.py:93} INFO - 	org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2025-05-12T17:51:13.365+0000] {subprocess.py:93} INFO - 	org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2025-05-12T17:51:13.366+0000] {subprocess.py:93} INFO - 	org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2025-05-12T17:51:13.367+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;2.0.7 from central in [default]
[2025-05-12T17:51:13.367+0000] {subprocess.py:93} INFO - 	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2025-05-12T17:51:13.368+0000] {subprocess.py:93} INFO - 	:: evicted modules:
[2025-05-12T17:51:13.369+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]
[2025-05-12T17:51:13.369+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.7] in [default]
[2025-05-12T17:51:13.370+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T17:51:13.371+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-05-12T17:51:13.371+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-12T17:51:13.372+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T17:51:13.372+0000] {subprocess.py:93} INFO - 	|      default     |   30  |   0   |   0   |   2   ||   28  |   0   |
[2025-05-12T17:51:13.372+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-05-12T17:51:13.373+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-6887b19d-b2a2-45d5-8e2f-ad34fc35d008
[2025-05-12T17:51:13.373+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-05-12T17:51:13.383+0000] {subprocess.py:93} INFO - 	0 artifacts copied, 28 already retrieved (0kB/22ms)
[2025-05-12T17:51:13.696+0000] {subprocess.py:93} INFO - 25/05/12 17:51:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-12T17:51:15.116+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SparkContext: Running Spark version 3.5.1
[2025-05-12T17:51:15.117+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-12T17:51:15.122+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SparkContext: Java version 11.0.22
[2025-05-12T17:51:15.184+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO ResourceUtils: ==============================================================
[2025-05-12T17:51:15.185+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-05-12T17:51:15.187+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO ResourceUtils: ==============================================================
[2025-05-12T17:51:15.189+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SparkContext: Submitted application: GoldNewsStreaming
[2025-05-12T17:51:15.232+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-05-12T17:51:15.247+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO ResourceProfile: Limiting resource is cpu
[2025-05-12T17:51:15.247+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-05-12T17:51:15.315+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SecurityManager: Changing view acls to: root
[2025-05-12T17:51:15.316+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SecurityManager: Changing modify acls to: root
[2025-05-12T17:51:15.317+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SecurityManager: Changing view acls groups to:
[2025-05-12T17:51:15.318+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SecurityManager: Changing modify acls groups to:
[2025-05-12T17:51:15.319+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
[2025-05-12T17:51:15.653+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO Utils: Successfully started service 'sparkDriver' on port 42333.
[2025-05-12T17:51:15.716+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SparkEnv: Registering MapOutputTracker
[2025-05-12T17:51:15.785+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SparkEnv: Registering BlockManagerMaster
[2025-05-12T17:51:15.819+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-05-12T17:51:15.819+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-05-12T17:51:15.827+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-05-12T17:51:15.862+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-42db1aaf-6cd0-4737-b508-d4b716649ed9
[2025-05-12T17:51:15.882+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-05-12T17:51:15.908+0000] {subprocess.py:93} INFO - 25/05/12 17:51:15 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-05-12T17:51:16.078+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-05-12T17:51:16.159+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-05-12T17:51:16.208+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://d4e9ca837c07:42333/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.209+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://d4e9ca837c07:42333/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.210+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://d4e9ca837c07:42333/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.211+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://d4e9ca837c07:42333/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.211+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://d4e9ca837c07:42333/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.212+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://d4e9ca837c07:42333/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747072275098
[2025-05-12T17:51:16.213+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://d4e9ca837c07:42333/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.213+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://d4e9ca837c07:42333/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747072275098
[2025-05-12T17:51:16.214+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://d4e9ca837c07:42333/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747072275098
[2025-05-12T17:51:16.215+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://d4e9ca837c07:42333/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747072275098
[2025-05-12T17:51:16.215+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://d4e9ca837c07:42333/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747072275098
[2025-05-12T17:51:16.216+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://d4e9ca837c07:42333/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.218+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://d4e9ca837c07:42333/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.219+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.220+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.221+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://d4e9ca837c07:42333/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747072275098
[2025-05-12T17:51:16.222+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://d4e9ca837c07:42333/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747072275098
[2025-05-12T17:51:16.222+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://d4e9ca837c07:42333/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747072275098
[2025-05-12T17:51:16.223+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://d4e9ca837c07:42333/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.224+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.224+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://d4e9ca837c07:42333/jars/com.typesafe_config-1.4.1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.225+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://d4e9ca837c07:42333/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747072275098
[2025-05-12T17:51:16.225+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://d4e9ca837c07:42333/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747072275098
[2025-05-12T17:51:16.226+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://d4e9ca837c07:42333/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747072275098
[2025-05-12T17:51:16.226+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://d4e9ca837c07:42333/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.227+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://d4e9ca837c07:42333/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747072275098
[2025-05-12T17:51:16.227+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://d4e9ca837c07:42333/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747072275098
[2025-05-12T17:51:16.228+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.229+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.229+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:51:16.245+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.246+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T17:51:16.257+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.257+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:51:16.263+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.265+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T17:51:16.282+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.283+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T17:51:16.288+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747072275098
[2025-05-12T17:51:16.288+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T17:51:16.363+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.363+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T17:51:16.367+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747072275098
[2025-05-12T17:51:16.367+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T17:51:16.376+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747072275098
[2025-05-12T17:51:16.376+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T17:51:16.380+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747072275098
[2025-05-12T17:51:16.381+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T17:51:16.427+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747072275098
[2025-05-12T17:51:16.428+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T17:51:16.433+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.434+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T17:51:16.442+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.443+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T17:51:16.449+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.450+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T17:51:16.468+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.469+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T17:51:16.473+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747072275098
[2025-05-12T17:51:16.474+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T17:51:16.481+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747072275098
[2025-05-12T17:51:16.482+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T17:51:16.486+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747072275098
[2025-05-12T17:51:16.486+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T17:51:16.502+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.503+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T17:51:16.510+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.511+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T17:51:16.522+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar at file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.524+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.typesafe_config-1.4.1.jar
[2025-05-12T17:51:16.529+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747072275098
[2025-05-12T17:51:16.530+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T17:51:16.534+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747072275098
[2025-05-12T17:51:16.535+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T17:51:16.542+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747072275098
[2025-05-12T17:51:16.543+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T17:51:16.547+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.548+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T17:51:16.551+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747072275098
[2025-05-12T17:51:16.552+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T17:51:16.558+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747072275098
[2025-05-12T17:51:16.559+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T17:51:16.563+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.564+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: Copying /root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T17:51:16.653+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Starting executor ID driver on host d4e9ca837c07
[2025-05-12T17:51:16.654+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-12T17:51:16.654+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Java version 11.0.22
[2025-05-12T17:51:16.663+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-05-12T17:51:16.664+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6f044988 for default.
[2025-05-12T17:51:16.678+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.703+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:51:16.708+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747072275098
[2025-05-12T17:51:16.711+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T17:51:16.715+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747072275098
[2025-05-12T17:51:16.731+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T17:51:16.734+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747072275098
[2025-05-12T17:51:16.752+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T17:51:16.756+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.760+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T17:51:16.789+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747072275098
[2025-05-12T17:51:16.790+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T17:51:16.796+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.800+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T17:51:16.804+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.818+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T17:51:16.821+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.822+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.typesafe_config-1.4.1.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.typesafe_config-1.4.1.jar
[2025-05-12T17:51:16.826+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747072275098
[2025-05-12T17:51:16.829+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T17:51:16.833+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747072275098
[2025-05-12T17:51:16.834+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T17:51:16.838+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.840+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:51:16.843+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747072275098
[2025-05-12T17:51:16.845+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T17:51:16.850+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.851+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T17:51:16.855+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.856+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T17:51:16.861+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.862+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T17:51:16.866+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.867+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T17:51:16.871+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747072275098
[2025-05-12T17:51:16.871+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T17:51:16.876+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747072275098
[2025-05-12T17:51:16.877+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T17:51:16.880+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747072275098
[2025-05-12T17:51:16.881+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T17:51:16.884+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747072275098
[2025-05-12T17:51:16.888+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T17:51:16.894+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.896+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T17:51:16.899+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747072275098
[2025-05-12T17:51:16.900+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T17:51:16.903+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747072275098
[2025-05-12T17:51:16.904+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T17:51:16.909+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.910+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T17:51:16.914+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.915+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T17:51:16.922+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747072275098
[2025-05-12T17:51:16.924+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T17:51:16.928+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747072275098
[2025-05-12T17:51:16.933+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T17:51:16.945+0000] {subprocess.py:93} INFO - 25/05/12 17:51:16 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.006+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO TransportClientFactory: Successfully created connection to d4e9ca837c07/172.18.0.9:42333 after 46 ms (0 ms spent in bootstraps)
[2025-05-12T17:51:17.021+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp124532992987639950.tmp
[2025-05-12T17:51:17.056+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp124532992987639950.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-05-12T17:51:17.063+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to class loader default
[2025-05-12T17:51:17.064+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.065+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp8166066183681193197.tmp
[2025-05-12T17:51:17.070+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp8166066183681193197.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-05-12T17:51:17.076+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to class loader default
[2025-05-12T17:51:17.077+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.077+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp12908407251895268577.tmp
[2025-05-12T17:51:17.092+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp12908407251895268577.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.lz4_lz4-java-1.8.0.jar
[2025-05-12T17:51:17.098+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.lz4_lz4-java-1.8.0.jar to class loader default
[2025-05-12T17:51:17.100+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1747072275098
[2025-05-12T17:51:17.101+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp6304102031277155891.tmp
[2025-05-12T17:51:17.105+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp6304102031277155891.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.slf4j_slf4j-api-2.0.7.jar
[2025-05-12T17:51:17.115+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.slf4j_slf4j-api-2.0.7.jar to class loader default
[2025-05-12T17:51:17.116+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1747072275098
[2025-05-12T17:51:17.121+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp2924839617509239746.tmp
[2025-05-12T17:51:17.122+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp2924839617509239746.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-05-12T17:51:17.126+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.reactivestreams_reactive-streams-1.0.3.jar to class loader default
[2025-05-12T17:51:17.127+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1747072275098
[2025-05-12T17:51:17.130+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp2907980304046616037.tmp
[2025-05-12T17:51:17.273+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp2907980304046616037.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-05-12T17:51:17.279+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
[2025-05-12T17:51:17.280+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.281+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp5985572741927798477.tmp
[2025-05-12T17:51:17.284+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp5985572741927798477.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_native-protocol-1.5.0.jar
[2025-05-12T17:51:17.289+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_native-protocol-1.5.0.jar to class loader default
[2025-05-12T17:51:17.290+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1747072275098
[2025-05-12T17:51:17.291+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp3999235748923011791.tmp
[2025-05-12T17:51:17.293+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp3999235748923011791.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-05-12T17:51:17.296+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.hdrhistogram_HdrHistogram-2.1.12.jar to class loader default
[2025-05-12T17:51:17.296+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1747072275098
[2025-05-12T17:51:17.297+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp12087921262767244356.tmp
[2025-05-12T17:51:17.301+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp12087921262767244356.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.commons_commons-lang3-3.10.jar
[2025-05-12T17:51:17.305+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.commons_commons-lang3-3.10.jar to class loader default
[2025-05-12T17:51:17.306+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1747072275098
[2025-05-12T17:51:17.307+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp12674272735691887377.tmp
[2025-05-12T17:51:17.310+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp12674272735691887377.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-05-12T17:51:17.313+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/io.dropwizard.metrics_metrics-core-4.1.18.jar to class loader default
[2025-05-12T17:51:17.314+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1747072275098
[2025-05-12T17:51:17.315+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp17205660713764674472.tmp
[2025-05-12T17:51:17.339+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp17205660713764674472.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-05-12T17:51:17.346+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to class loader default
[2025-05-12T17:51:17.352+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1747072275098
[2025-05-12T17:51:17.353+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp14724376266698835790.tmp
[2025-05-12T17:51:17.360+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp14724376266698835790.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.commons_commons-pool2-2.11.1.jar
[2025-05-12T17:51:17.369+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2025-05-12T17:51:17.370+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.372+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp11786294771871094943.tmp
[2025-05-12T17:51:17.381+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp11786294771871094943.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-05-12T17:51:17.386+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-query-builder-4.13.0.jar to class loader default
[2025-05-12T17:51:17.387+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.388+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp5965544499911829780.tmp
[2025-05-12T17:51:17.446+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp5965544499911829780.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-05-12T17:51:17.464+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to class loader default
[2025-05-12T17:51:17.465+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1747072275098
[2025-05-12T17:51:17.466+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp17762623054550312629.tmp
[2025-05-12T17:51:17.498+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp17762623054550312629.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-05-12T17:51:17.508+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
[2025-05-12T17:51:17.509+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.510+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp10671414120130539274.tmp
[2025-05-12T17:51:17.514+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp10671414120130539274.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:51:17.519+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
[2025-05-12T17:51:17.520+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1747072275098
[2025-05-12T17:51:17.522+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp8892437450508899164.tmp
[2025-05-12T17:51:17.523+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp8892437450508899164.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-05-12T17:51:17.527+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.google.code.findbugs_jsr305-3.0.2.jar to class loader default
[2025-05-12T17:51:17.528+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.529+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp13196709842962291629.tmp
[2025-05-12T17:51:17.529+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp13196709842962291629.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2025-05-12T17:51:17.535+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
[2025-05-12T17:51:17.536+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1747072275098
[2025-05-12T17:51:17.538+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp14009027898968178602.tmp
[2025-05-12T17:51:17.539+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp14009027898968178602.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-05-12T17:51:17.542+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to class loader default
[2025-05-12T17:51:17.543+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.544+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp10793848233589611111.tmp
[2025-05-12T17:51:17.554+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp10793848233589611111.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-05-12T17:51:17.564+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to class loader default
[2025-05-12T17:51:17.565+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.typesafe_config-1.4.1.jar with timestamp 1747072275098
[2025-05-12T17:51:17.566+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp5817650771048149484.tmp
[2025-05-12T17:51:17.574+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp5817650771048149484.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.typesafe_config-1.4.1.jar
[2025-05-12T17:51:17.583+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.typesafe_config-1.4.1.jar to class loader default
[2025-05-12T17:51:17.584+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1747072275098
[2025-05-12T17:51:17.585+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp5342816777596656528.tmp
[2025-05-12T17:51:17.612+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp5342816777596656528.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.scala-lang_scala-reflect-2.12.11.jar
[2025-05-12T17:51:17.617+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.scala-lang_scala-reflect-2.12.11.jar to class loader default
[2025-05-12T17:51:17.621+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1747072275098
[2025-05-12T17:51:17.624+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp7798974318053350599.tmp
[2025-05-12T17:51:17.625+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp7798974318053350599.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/commons-logging_commons-logging-1.1.3.jar
[2025-05-12T17:51:17.645+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/commons-logging_commons-logging-1.1.3.jar to class loader default
[2025-05-12T17:51:17.647+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1747072275098
[2025-05-12T17:51:17.648+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp8234028190258865373.tmp
[2025-05-12T17:51:17.892+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp8234028190258865373.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-05-12T17:51:17.903+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
[2025-05-12T17:51:17.904+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1747072275098
[2025-05-12T17:51:17.908+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp2057868172088198274.tmp
[2025-05-12T17:51:17.909+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp2057868172088198274.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-05-12T17:51:17.915+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.thoughtworks.paranamer_paranamer-2.8.jar to class loader default
[2025-05-12T17:51:17.916+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1747072275098
[2025-05-12T17:51:17.916+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp8772672393334179306.tmp
[2025-05-12T17:51:17.933+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp8772672393334179306.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-05-12T17:51:17.936+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
[2025-05-12T17:51:17.937+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1747072275098
[2025-05-12T17:51:17.938+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp4985168421570474841.tmp
[2025-05-12T17:51:17.941+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp4985168421570474841.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-05-12T17:51:17.949+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to class loader default
[2025-05-12T17:51:17.950+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1747072275098
[2025-05-12T17:51:17.951+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Fetching spark://d4e9ca837c07:42333/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp6293499732655234796.tmp
[2025-05-12T17:51:17.960+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/fetchFileTemp6293499732655234796.tmp has been previously copied to /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-05-12T17:51:17.977+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Executor: Adding file:/tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/userFiles-62076872-e2ac-4729-9b7d-6f4d6b74cb8d/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to class loader default
[2025-05-12T17:51:18.001+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38409.
[2025-05-12T17:51:18.002+0000] {subprocess.py:93} INFO - 25/05/12 17:51:17 INFO NettyBlockTransferService: Server created on d4e9ca837c07:38409
[2025-05-12T17:51:18.007+0000] {subprocess.py:93} INFO - 25/05/12 17:51:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-05-12T17:51:18.034+0000] {subprocess.py:93} INFO - 25/05/12 17:51:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d4e9ca837c07, 38409, None)
[2025-05-12T17:51:18.038+0000] {subprocess.py:93} INFO - 25/05/12 17:51:18 INFO BlockManagerMasterEndpoint: Registering block manager d4e9ca837c07:38409 with 434.4 MiB RAM, BlockManagerId(driver, d4e9ca837c07, 38409, None)
[2025-05-12T17:51:18.046+0000] {subprocess.py:93} INFO - 25/05/12 17:51:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d4e9ca837c07, 38409, None)
[2025-05-12T17:51:18.053+0000] {subprocess.py:93} INFO - 25/05/12 17:51:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d4e9ca837c07, 38409, None)
[2025-05-12T17:51:18.654+0000] {subprocess.py:93} INFO - 25/05/12 17:51:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-05-12T17:51:18.656+0000] {subprocess.py:93} INFO - 25/05/12 17:51:18 INFO SharedState: Warehouse path is 'file:/opt/spark-apps/spark-warehouse'.
[2025-05-12T17:51:22.591+0000] {subprocess.py:93} INFO - 25/05/12 17:51:22 INFO DefaultMavenCoordinates: DataStax Java driver for Apache Cassandra(R) (com.datastax.oss:java-driver-core-shaded) version 4.13.0
[2025-05-12T17:51:22.781+0000] {subprocess.py:93} INFO - 25/05/12 17:51:22 INFO Native: Unable to load JNR native implementation. This could be normal if JNR is excluded from the classpath
[2025-05-12T17:51:22.782+0000] {subprocess.py:93} INFO - java.lang.NoClassDefFoundError: jnr/posix/POSIXHandler
[2025-05-12T17:51:22.783+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native$LibcLoader.load(Native.java:42)
[2025-05-12T17:51:22.784+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native.<clinit>(Native.java:59)
[2025-05-12T17:51:22.785+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.Clock.getInstance(Clock.java:41)
[2025-05-12T17:51:22.785+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.buildClock(MonotonicTimestampGenerator.java:109)
[2025-05-12T17:51:22.786+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.<init>(MonotonicTimestampGenerator.java:43)
[2025-05-12T17:51:22.786+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.AtomicTimestampGenerator.<init>(AtomicTimestampGenerator.java:52)
[2025-05-12T17:51:22.787+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-12T17:51:22.787+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-12T17:51:22.788+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-12T17:51:22.788+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-12T17:51:22.789+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.resolveClass(Reflection.java:329)
[2025-05-12T17:51:22.790+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:235)
[2025-05-12T17:51:22.790+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:110)
[2025-05-12T17:51:22.791+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.buildTimestampGenerator(DefaultDriverContext.java:377)
[2025-05-12T17:51:22.791+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.concurrent.LazyReference.get(LazyReference.java:55)
[2025-05-12T17:51:22.792+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.getTimestampGenerator(DefaultDriverContext.java:773)
[2025-05-12T17:51:22.792+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.init(DefaultSession.java:349)
[2025-05-12T17:51:22.793+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.access$1100(DefaultSession.java:300)
[2025-05-12T17:51:22.794+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession.lambda$init$0(DefaultSession.java:146)
[2025-05-12T17:51:22.794+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
[2025-05-12T17:51:22.795+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
[2025-05-12T17:51:22.795+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)
[2025-05-12T17:51:22.796+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2025-05-12T17:51:22.796+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-05-12T17:51:22.798+0000] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-05-12T17:51:22.798+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-12T17:51:22.799+0000] {subprocess.py:93} INFO - Caused by: java.lang.ClassNotFoundException: jnr.posix.POSIXHandler
[2025-05-12T17:51:22.800+0000] {subprocess.py:93} INFO - 	at java.base/java.net.URLClassLoader.findClass(Unknown Source)
[2025-05-12T17:51:22.800+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
[2025-05-12T17:51:22.801+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
[2025-05-12T17:51:22.802+0000] {subprocess.py:93} INFO - 	... 26 more
[2025-05-12T17:51:22.802+0000] {subprocess.py:93} INFO - 25/05/12 17:51:22 INFO Clock: Could not access native clock (see debug logs for details), falling back to Java system clock
[2025-05-12T17:51:23.422+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO CassandraConnector: Connected to Cassandra cluster.
[2025-05-12T17:51:23.462+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-05-12T17:51:23.511+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint/cassandra resolved to file:/tmp/spark-checkpoint/cassandra.
[2025-05-12T17:51:23.512+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-05-12T17:51:23.660+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO MicroBatchExecution: Starting [id = 71460171-bd4d-4562-8003-9df0ff8d9d3e, runId = 73a48db0-416a-4162-a7b4-27662cb3d32b]. Use file:/tmp/spark-checkpoint/cassandra to store the query checkpoint.
[2025-05-12T17:51:23.680+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@11afc4a7] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@44392672]
[2025-05-12T17:51:23.706+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d resolved to file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d.
[2025-05-12T17:51:23.707+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-05-12T17:51:23.719+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 WARN MicroBatchExecution: The read limit MaxRows: 100 for KafkaV2[Subscribe[gold-news]] is ignored when Trigger.Once is used.
[2025-05-12T17:51:23.728+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/metadata using temp file file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/.metadata.c793c2f6-fce6-4959-822f-31633dd4f02d.tmp
[2025-05-12T17:51:23.731+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2
[2025-05-12T17:51:23.748+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO OffsetSeqLog: Getting latest batch 2
[2025-05-12T17:51:23.767+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/.metadata.c793c2f6-fce6-4959-822f-31633dd4f02d.tmp to file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/metadata
[2025-05-12T17:51:23.783+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO MicroBatchExecution: Starting [id = 7c53f13a-bae6-410c-8999-b3f236a45e54, runId = cbaeb3ee-efcd-4429-81f8-2bcfa7ac9950]. Use file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d to store the query checkpoint.
[2025-05-12T17:51:23.784+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2
[2025-05-12T17:51:23.784+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO OffsetSeqLog: Getting latest batch 2
[2025-05-12T17:51:23.786+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@11afc4a7] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@44392672]
[2025-05-12T17:51:23.789+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 WARN MicroBatchExecution: The read limit MaxRows: 100 for KafkaV2[Subscribe[gold-news]] is ignored when Trigger.Once is used.
[2025-05-12T17:51:23.793+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO OffsetSeqLog: BatchIds found from listing:
[2025-05-12T17:51:23.794+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO OffsetSeqLog: BatchIds found from listing:
[2025-05-12T17:51:23.795+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO MicroBatchExecution: Starting new streaming query.
[2025-05-12T17:51:23.799+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO MicroBatchExecution: Stream started from {}
[2025-05-12T17:51:23.800+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO CommitLog: BatchIds found from listing: 0, 1, 2
[2025-05-12T17:51:23.800+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO CommitLog: Getting latest batch 2
[2025-05-12T17:51:23.808+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO MicroBatchExecution: Resuming at batch 3 with committed offsets {KafkaV2[Subscribe[gold-news]]: {"gold-news":{"0":50}}} and available offsets {KafkaV2[Subscribe[gold-news]]: {"gold-news":{"0":50}}}
[2025-05-12T17:51:23.808+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[gold-news]]: {"gold-news":{"0":50}}}
[2025-05-12T17:51:23.866+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO AdminClientConfig: AdminClientConfig values:
[2025-05-12T17:51:23.867+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:51:23.868+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:51:23.868+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:51:23.869+0000] {subprocess.py:93} INFO - 	client.id =
[2025-05-12T17:51:23.869+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-05-12T17:51:23.869+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:51:23.870+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:51:23.870+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:51:23.870+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:51:23.871+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:51:23.871+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:51:23.871+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:51:23.872+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:51:23.872+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:51:23.873+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:51:23.873+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-05-12T17:51:23.874+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:51:23.874+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:51:23.874+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:51:23.875+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:51:23.875+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:51:23.876+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:51:23.876+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:51:23.877+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:51:23.877+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:51:23.877+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:51:23.878+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:51:23.878+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:51:23.878+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:51:23.879+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:51:23.879+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:51:23.880+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:51:23.880+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:51:23.881+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:51:23.882+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:51:23.883+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:51:23.883+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:51:23.884+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:51:23.884+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:51:23.885+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:51:23.885+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:51:23.886+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:51:23.886+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:51:23.887+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:51:23.887+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:51:23.888+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:51:23.888+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:51:23.889+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:51:23.889+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:51:23.890+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:51:23.890+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:51:23.890+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:51:23.891+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:51:23.891+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:51:23.892+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:51:23.892+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:51:23.893+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:51:23.893+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:51:23.893+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:51:23.894+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:51:23.894+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:51:23.895+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:51:23.895+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:51:23.896+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:51:23.897+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:51:23.897+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:51:23.898+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:51:23.898+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:51:23.899+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:51:23.899+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:23.963+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-05-12T17:51:23.966+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:51:23.967+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:51:23.967+0000] {subprocess.py:93} INFO - 25/05/12 17:51:23 INFO AppInfoParser: Kafka startTimeMs: 1747072283961
[2025-05-12T17:51:24.173+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO AdminClientConfig: AdminClientConfig values:
[2025-05-12T17:51:24.174+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:51:24.174+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:51:24.175+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:51:24.175+0000] {subprocess.py:93} INFO - 	client.id =
[2025-05-12T17:51:24.176+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-05-12T17:51:24.177+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:51:24.178+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:51:24.178+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:51:24.179+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:51:24.179+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:51:24.180+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:51:24.180+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:51:24.181+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:51:24.181+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:51:24.182+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:51:24.182+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-05-12T17:51:24.183+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:51:24.183+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:51:24.183+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:51:24.184+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:51:24.184+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:51:24.184+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:51:24.185+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:51:24.186+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:51:24.187+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:51:24.187+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:51:24.188+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:51:24.188+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:51:24.189+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:51:24.189+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:51:24.189+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:51:24.190+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:51:24.190+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:51:24.191+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:51:24.191+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:51:24.191+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:51:24.192+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:51:24.192+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:51:24.193+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:51:24.193+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:51:24.194+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:51:24.194+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:51:24.194+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:51:24.195+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:51:24.195+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:51:24.196+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:51:24.196+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:51:24.196+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:51:24.197+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:51:24.197+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:51:24.198+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:51:24.198+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:51:24.198+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:51:24.199+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:51:24.199+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:51:24.199+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:51:24.199+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:51:24.200+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:51:24.201+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:51:24.202+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:51:24.202+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:51:24.203+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:51:24.203+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:51:24.203+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:51:24.203+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:51:24.204+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:51:24.204+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:51:24.204+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:51:24.205+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:51:24.205+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:24.205+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-05-12T17:51:24.205+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:51:24.206+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:51:24.206+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO AppInfoParser: Kafka startTimeMs: 1747072284176
[2025-05-12T17:51:24.494+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/sources/0/0 using temp file file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/sources/0/.0.fe877ef0-0713-4e5d-8182-931d49513b42.tmp
[2025-05-12T17:51:24.510+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/cassandra/offsets/3 using temp file file:/tmp/spark-checkpoint/cassandra/offsets/.3.63ea8578-73af-499a-830c-5b21971428a8.tmp
[2025-05-12T17:51:24.525+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/sources/0/.0.fe877ef0-0713-4e5d-8182-931d49513b42.tmp to file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/sources/0/0
[2025-05-12T17:51:24.526+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO KafkaMicroBatchStream: Initial offsets: {"gold-news":{"0":0}}
[2025-05-12T17:51:24.540+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/cassandra/offsets/.3.63ea8578-73af-499a-830c-5b21971428a8.tmp to file:/tmp/spark-checkpoint/cassandra/offsets/3
[2025-05-12T17:51:24.541+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1747072284496,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-05-12T17:51:24.548+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/offsets/0 using temp file file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/offsets/.0.78d52d61-a994-4f0d-87a4-e1268fcbed19.tmp
[2025-05-12T17:51:24.578+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/offsets/.0.78d52d61-a994-4f0d-87a4-e1268fcbed19.tmp to file:/tmp/spark-checkpoint/16ed3b4b-5b8d-4097-8f6e-f30a9ca0e10d/offsets/0
[2025-05-12T17:51:24.579+0000] {subprocess.py:93} INFO - 25/05/12 17:51:24 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1747072284539,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-05-12T17:51:25.259+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:25.260+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:25.430+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:25.441+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:25.548+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:25.550+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:25.552+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:25.552+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:25.643+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:25.645+0000] {subprocess.py:93} INFO - 25/05/12 17:51:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-05-12T17:51:26.357+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO CodeGenerator: Code generated in 531.721727 ms
[2025-05-12T17:51:26.601+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@7ac04918,com.datastax.spark.connector.cql.CassandraConnector@29bbc7f7,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@5fd0c804)]. The input RDD has 1 partitions.
[2025-05-12T17:51:26.808+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO CodeGenerator: Code generated in 16.800646 ms
[2025-05-12T17:51:26.862+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO SparkContext: Starting job: start at <unknown>:0
[2025-05-12T17:51:26.882+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO SparkContext: Starting job: start at <unknown>:0
[2025-05-12T17:51:26.915+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-05-12T17:51:26.920+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-05-12T17:51:26.921+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:51:26.928+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:51:26.943+0000] {subprocess.py:93} INFO - 25/05/12 17:51:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at start at <unknown>:0), which has no missing parents
[2025-05-12T17:51:27.244+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KiB, free 434.4 MiB)
[2025-05-12T17:51:27.306+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 434.4 MiB)
[2025-05-12T17:51:27.311+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on d4e9ca837c07:38409 (size: 13.5 KiB, free: 434.4 MiB)
[2025-05-12T17:51:27.319+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:51:27.347+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:51:27.350+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-05-12T17:51:27.399+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-05-12T17:51:27.400+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-05-12T17:51:27.401+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:51:27.410+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:51:27.413+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-05-12T17:51:27.458+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 434.3 MiB)
[2025-05-12T17:51:27.501+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 15.7 KiB, free 434.3 MiB)
[2025-05-12T17:51:27.503+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on d4e9ca837c07:38409 (size: 15.7 KiB, free: 434.4 MiB)
[2025-05-12T17:51:27.504+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:51:27.505+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:51:27.505+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-05-12T17:51:27.556+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 13811 bytes)
[2025-05-12T17:51:27.643+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 13810 bytes)
[2025-05-12T17:51:27.655+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-05-12T17:51:27.658+0000] {subprocess.py:93} INFO - 25/05/12 17:51:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-05-12T17:51:28.026+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO CodeGenerator: Code generated in 96.406848 ms
[2025-05-12T17:51:28.121+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO CodeGenerator: Code generated in 87.224425 ms
[2025-05-12T17:51:28.145+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO CodeGenerator: Code generated in 12.286063 ms
[2025-05-12T17:51:28.149+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=0 untilOffset=60, for query queryId=7c53f13a-bae6-410c-8999-b3f236a45e54 batchId=0 taskId=1 partitionId=0
[2025-05-12T17:51:28.243+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO CodeGenerator: Code generated in 23.118492 ms
[2025-05-12T17:51:28.291+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO CodeGenerator: Code generated in 33.494921 ms
[2025-05-12T17:51:28.339+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO ConsumerConfig: ConsumerConfig values:
[2025-05-12T17:51:28.340+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-05-12T17:51:28.341+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-05-12T17:51:28.341+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:51:28.342+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-05-12T17:51:28.343+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:51:28.343+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-05-12T17:51:28.344+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:51:28.344+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1
[2025-05-12T17:51:28.345+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-05-12T17:51:28.345+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-05-12T17:51:28.346+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:51:28.346+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-05-12T17:51:28.346+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-05-12T17:51:28.347+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-05-12T17:51:28.347+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-05-12T17:51:28.348+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-05-12T17:51:28.348+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor
[2025-05-12T17:51:28.349+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-05-12T17:51:28.349+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-05-12T17:51:28.349+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-05-12T17:51:28.350+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-05-12T17:51:28.350+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-05-12T17:51:28.351+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-05-12T17:51:28.353+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:51:28.354+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-05-12T17:51:28.355+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-05-12T17:51:28.356+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-05-12T17:51:28.357+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:51:28.357+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:51:28.358+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:51:28.359+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:51:28.359+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:51:28.360+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-05-12T17:51:28.360+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:51:28.361+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:51:28.361+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:51:28.361+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:51:28.362+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:51:28.362+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:51:28.363+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:51:28.363+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:51:28.363+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:51:28.364+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:51:28.364+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:51:28.364+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:51:28.365+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:51:28.365+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:51:28.366+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:51:28.368+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:51:28.370+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:51:28.371+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:51:28.371+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:51:28.372+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:51:28.372+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:51:28.373+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:51:28.374+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:51:28.374+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:51:28.375+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:51:28.375+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:51:28.376+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:51:28.377+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:51:28.377+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:51:28.378+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:51:28.379+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:51:28.379+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:51:28.380+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:51:28.380+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:51:28.381+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:51:28.382+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:51:28.385+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-05-12T17:51:28.385+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:51:28.386+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:51:28.387+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:51:28.387+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:51:28.388+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:51:28.388+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:51:28.389+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:51:28.389+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:51:28.390+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:51:28.390+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:51:28.391+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:51:28.391+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:51:28.392+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:51:28.392+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:51:28.393+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:51:28.393+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:51:28.394+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:51:28.395+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:51:28.395+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:51:28.396+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:51:28.396+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:51:28.397+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:51:28.398+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:28.454+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:51:28.455+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:51:28.459+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO AppInfoParser: Kafka startTimeMs: 1747072288451
[2025-05-12T17:51:28.468+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Assigned to partition(s): gold-news-0
[2025-05-12T17:51:28.529+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Seeking to offset 0 for partition gold-news-0
[2025-05-12T17:51:28.541+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Resetting the last seen epoch of partition gold-news-0 to 0 since the associated topicId changed from null to EyFovrTXQQOm3GPuq8IGNA
[2025-05-12T17:51:28.542+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Cluster ID: HTSY0p8VS7eCaEFoyzBH3g
[2025-05-12T17:51:28.638+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T17:51:28.659+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=50 untilOffset=60, for query queryId=71460171-bd4d-4562-8003-9df0ff8d9d3e batchId=3 taskId=0 partitionId=0
[2025-05-12T17:51:28.675+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO ConsumerConfig: ConsumerConfig values:
[2025-05-12T17:51:28.675+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-05-12T17:51:28.676+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-05-12T17:51:28.677+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-05-12T17:51:28.677+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-05-12T17:51:28.677+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-05-12T17:51:28.678+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-05-12T17:51:28.683+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-05-12T17:51:28.684+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2
[2025-05-12T17:51:28.684+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-05-12T17:51:28.685+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-05-12T17:51:28.685+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-05-12T17:51:28.686+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-05-12T17:51:28.686+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-05-12T17:51:28.686+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-05-12T17:51:28.687+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-05-12T17:51:28.687+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-05-12T17:51:28.688+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor
[2025-05-12T17:51:28.688+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-05-12T17:51:28.690+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-05-12T17:51:28.690+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-05-12T17:51:28.691+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-05-12T17:51:28.691+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-05-12T17:51:28.692+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-05-12T17:51:28.692+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:51:28.692+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-05-12T17:51:28.693+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-05-12T17:51:28.693+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-05-12T17:51:28.693+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-05-12T17:51:28.694+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-05-12T17:51:28.694+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-05-12T17:51:28.694+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-05-12T17:51:28.694+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-05-12T17:51:28.695+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-05-12T17:51:28.695+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-05-12T17:51:28.695+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-05-12T17:51:28.696+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-05-12T17:51:28.696+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-05-12T17:51:28.696+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-05-12T17:51:28.697+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-05-12T17:51:28.697+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-05-12T17:51:28.697+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-05-12T17:51:28.697+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-05-12T17:51:28.698+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-05-12T17:51:28.698+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-05-12T17:51:28.699+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-05-12T17:51:28.699+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-05-12T17:51:28.699+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-05-12T17:51:28.700+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-05-12T17:51:28.700+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-05-12T17:51:28.700+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-05-12T17:51:28.700+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-05-12T17:51:28.701+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-05-12T17:51:28.701+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-05-12T17:51:28.701+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-05-12T17:51:28.702+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-05-12T17:51:28.702+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-05-12T17:51:28.703+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-05-12T17:51:28.703+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-05-12T17:51:28.704+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-05-12T17:51:28.705+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-05-12T17:51:28.706+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-05-12T17:51:28.706+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-05-12T17:51:28.706+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-05-12T17:51:28.707+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-05-12T17:51:28.707+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-05-12T17:51:28.708+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-05-12T17:51:28.708+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-05-12T17:51:28.708+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-05-12T17:51:28.709+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-05-12T17:51:28.709+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-05-12T17:51:28.709+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-05-12T17:51:28.710+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-05-12T17:51:28.710+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-05-12T17:51:28.711+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-05-12T17:51:28.711+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-05-12T17:51:28.711+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-05-12T17:51:28.712+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-05-12T17:51:28.712+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-05-12T17:51:28.712+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-05-12T17:51:28.713+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-05-12T17:51:28.713+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-05-12T17:51:28.713+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-05-12T17:51:28.714+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-05-12T17:51:28.714+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-05-12T17:51:28.714+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-05-12T17:51:28.715+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-05-12T17:51:28.715+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-05-12T17:51:28.715+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-05-12T17:51:28.716+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-05-12T17:51:28.716+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-05-12T17:51:28.716+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-05-12T17:51:28.717+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-05-12T17:51:28.717+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:28.735+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO AppInfoParser: Kafka version: 3.4.1
[2025-05-12T17:51:28.737+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2025-05-12T17:51:28.738+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO AppInfoParser: Kafka startTimeMs: 1747072288732
[2025-05-12T17:51:28.739+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Assigned to partition(s): gold-news-0
[2025-05-12T17:51:28.741+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Seeking to offset 50 for partition gold-news-0
[2025-05-12T17:51:28.748+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Resetting the last seen epoch of partition gold-news-0 to 0 since the associated topicId changed from null to EyFovrTXQQOm3GPuq8IGNA
[2025-05-12T17:51:28.749+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Cluster ID: HTSY0p8VS7eCaEFoyzBH3g
[2025-05-12T17:51:28.762+0000] {subprocess.py:93} INFO - 25/05/12 17:51:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T17:51:29.141+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:51:29.142+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T17:51:29.144+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=60, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:51:29.267+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:51:29.268+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T17:51:29.273+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=60, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:51:29.385+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor read 1 records through 1 polls (polled  out 60 records), taking 620879885 nanos, during time span of 869671083 nanos.
[2025-05-12T17:51:29.449+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1957 bytes result sent to driver
[2025-05-12T17:51:29.507+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-05-12T17:51:29.509+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1920 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:51:29.518+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-05-12T17:51:29.525+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 2.109 s
[2025-05-12T17:51:29.534+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:51:29.535+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-05-12T17:51:29.538+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 2.658563 s
[2025-05-12T17:51:29.610+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-05-12T17:51:29.611+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor read 10 records through 1 polls (polled  out 10 records), taking 532220544 nanos, during time span of 870481136 nanos.
[2025-05-12T17:51:29.642+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1851 bytes result sent to driver
[2025-05-12T17:51:29.653+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2159 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:51:29.654+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-05-12T17:51:29.656+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 2.679 s
[2025-05-12T17:51:29.658+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:51:29.658+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-05-12T17:51:29.659+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 2.812835 s
[2025-05-12T17:51:29.660+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@7ac04918,com.datastax.spark.connector.cql.CassandraConnector@29bbc7f7,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@5fd0c804)] is committing.
[2025-05-12T17:51:29.662+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@7ac04918,com.datastax.spark.connector.cql.CassandraConnector@29bbc7f7,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(title,StringType,true),StructField(source,StringType,true),StructField(published_at,StringType,true),StructField(description,StringType,true),StructField(url,StringType,true),StructField(ingestion_time,StringType,true)),org.apache.spark.SparkConf@5fd0c804)] committed.
[2025-05-12T17:51:29.688+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint/cassandra/commits/3 using temp file file:/tmp/spark-checkpoint/cassandra/commits/.3.7d3c28f6-0d63-405f-944b-647098b52b43.tmp
[2025-05-12T17:51:29.754+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO CodeGenerator: Code generated in 57.673907 ms
[2025-05-12T17:51:29.790+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint/cassandra/commits/.3.7d3c28f6-0d63-405f-944b-647098b52b43.tmp to file:/tmp/spark-checkpoint/cassandra/commits/3
[2025-05-12T17:51:29.804+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on d4e9ca837c07:38409 in memory (size: 15.7 KiB, free: 434.4 MiB)
[2025-05-12T17:51:29.805+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T17:51:29.808+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Got job 2 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T17:51:29.809+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Final stage: ResultStage 2 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:51:29.810+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:51:29.811+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:51:29.812+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:51:29.820+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 42.3 KiB, free 434.3 MiB)
[2025-05-12T17:51:29.838+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 434.3 MiB)
[2025-05-12T17:51:29.841+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on d4e9ca837c07:38409 (size: 16.1 KiB, free: 434.4 MiB)
[2025-05-12T17:51:29.843+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:51:29.848+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:51:29.849+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-05-12T17:51:29.852+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 13810 bytes)
[2025-05-12T17:51:29.855+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-05-12T17:51:29.904+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO MicroBatchExecution: Streaming query made progress: {
[2025-05-12T17:51:29.906+0000] {subprocess.py:93} INFO -   "id" : "71460171-bd4d-4562-8003-9df0ff8d9d3e",
[2025-05-12T17:51:29.907+0000] {subprocess.py:93} INFO -   "runId" : "73a48db0-416a-4162-a7b4-27662cb3d32b",
[2025-05-12T17:51:29.908+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-05-12T17:51:29.909+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-05-12T17:51:23.721Z",
[2025-05-12T17:51:29.910+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-05-12T17:51:29.910+0000] {subprocess.py:93} INFO -   "numInputRows" : 10,
[2025-05-12T17:51:29.911+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-05-12T17:51:29.912+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.6490765171503958,
[2025-05-12T17:51:29.915+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-05-12T17:51:29.918+0000] {subprocess.py:93} INFO -     "addBatch" : 4177,
[2025-05-12T17:51:29.919+0000] {subprocess.py:93} INFO -     "commitOffsets" : 113,
[2025-05-12T17:51:29.920+0000] {subprocess.py:93} INFO -     "getBatch" : 4,
[2025-05-12T17:51:29.920+0000] {subprocess.py:93} INFO -     "latestOffset" : 683,
[2025-05-12T17:51:29.921+0000] {subprocess.py:93} INFO -     "queryPlanning" : 925,
[2025-05-12T17:51:29.922+0000] {subprocess.py:93} INFO -     "triggerExecution" : 6063,
[2025-05-12T17:51:29.923+0000] {subprocess.py:93} INFO -     "walCommit" : 44
[2025-05-12T17:51:29.926+0000] {subprocess.py:93} INFO -   },
[2025-05-12T17:51:29.928+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-05-12T17:51:29.928+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-05-12T17:51:29.929+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[gold-news]]",
[2025-05-12T17:51:29.929+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-05-12T17:51:29.930+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T17:51:29.931+0000] {subprocess.py:93} INFO -         "0" : 50
[2025-05-12T17:51:29.931+0000] {subprocess.py:93} INFO -       }
[2025-05-12T17:51:29.932+0000] {subprocess.py:93} INFO -     },
[2025-05-12T17:51:29.932+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-05-12T17:51:29.933+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T17:51:29.933+0000] {subprocess.py:93} INFO -         "0" : 60
[2025-05-12T17:51:29.934+0000] {subprocess.py:93} INFO -       }
[2025-05-12T17:51:29.934+0000] {subprocess.py:93} INFO -     },
[2025-05-12T17:51:29.935+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-05-12T17:51:29.935+0000] {subprocess.py:93} INFO -       "gold-news" : {
[2025-05-12T17:51:29.936+0000] {subprocess.py:93} INFO -         "0" : 60
[2025-05-12T17:51:29.936+0000] {subprocess.py:93} INFO -       }
[2025-05-12T17:51:29.937+0000] {subprocess.py:93} INFO -     },
[2025-05-12T17:51:29.937+0000] {subprocess.py:93} INFO -     "numInputRows" : 10,
[2025-05-12T17:51:29.938+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-05-12T17:51:29.938+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.6490765171503958,
[2025-05-12T17:51:29.938+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-05-12T17:51:29.940+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-05-12T17:51:29.943+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-05-12T17:51:29.943+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-05-12T17:51:29.944+0000] {subprocess.py:93} INFO -     }
[2025-05-12T17:51:29.944+0000] {subprocess.py:93} INFO -   } ],
[2025-05-12T17:51:29.944+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-05-12T17:51:29.945+0000] {subprocess.py:93} INFO -     "description" : "CassandraTable(org.apache.spark.sql.SparkSession@7ac04918,org.apache.spark.sql.util.CaseInsensitiveStringMap@6e619b4c,com.datastax.spark.connector.cql.CassandraConnector@5ee9c592,default,DefaultTableMetadata@88aa72be(gold_news.articles),None)",
[2025-05-12T17:51:29.946+0000] {subprocess.py:93} INFO -     "numOutputRows" : 10
[2025-05-12T17:51:29.947+0000] {subprocess.py:93} INFO -   }
[2025-05-12T17:51:29.947+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:29.947+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2025-05-12T17:51:29.948+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:51:29.948+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:51:29.949+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO CodeGenerator: Code generated in 32.984109 ms
[2025-05-12T17:51:29.949+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO Metrics: Metrics reporters closed
[2025-05-12T17:51:29.949+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=gold-news-0 fromOffset=0 untilOffset=60, for query queryId=7c53f13a-bae6-410c-8999-b3f236a45e54 batchId=0 taskId=2 partitionId=0
[2025-05-12T17:51:29.950+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO MicroBatchExecution: Async log purge executor pool for query [id = 71460171-bd4d-4562-8003-9df0ff8d9d3e, runId = 73a48db0-416a-4162-a7b4-27662cb3d32b] has been shutdown
[2025-05-12T17:51:29.978+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on d4e9ca837c07:38409 in memory (size: 13.5 KiB, free: 434.4 MiB)
[2025-05-12T17:51:29.991+0000] {subprocess.py:93} INFO - 25/05/12 17:51:29 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Seeking to offset 0 for partition gold-news-0
[2025-05-12T17:51:30.007+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Seeking to earliest offset of partition gold-news-0
[2025-05-12T17:51:30.513+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:51:30.514+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Seeking to latest offset of partition gold-news-0
[2025-05-12T17:51:30.516+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Resetting offset for partition gold-news-0 to position FetchPosition{offset=60, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-05-12T17:51:30.603+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO KafkaDataConsumer: From Kafka topicPartition=gold-news-0 groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor read 60 records through 1 polls (polled  out 60 records), taking 526242260 nanos, during time span of 613208653 nanos.
[2025-05-12T17:51:30.616+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 8817 bytes result sent to driver
[2025-05-12T17:51:30.631+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 775 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:51:30.632+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-05-12T17:51:30.633+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO DAGScheduler: ResultStage 2 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.814 s
[2025-05-12T17:51:30.633+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:51:30.634+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-05-12T17:51:30.634+0000] {subprocess.py:93} INFO - 25/05/12 17:51:30 INFO DAGScheduler: Job 2 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.827985 s
[2025-05-12T17:51:30.731+0000] {subprocess.py:93} INFO - Traitement du batch 0 avec 60 descriptions
[2025-05-12T17:51:55.545+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.546+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:51:55.547+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.547+0000] {subprocess.py:93} INFO -   "description_number": "1",
[2025-05-12T17:51:55.548+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.549+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase is driven by easing tariff tensions, which could reduce demand for safe-haven assets like gold. However, the overall impact is unclear as it depends on the market's reaction to the geopolitical development.",
[2025-05-12T17:51:55.550+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.550+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.551+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.551+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:51:55.552+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.552+0000] {subprocess.py:93} INFO -   "description_number": "2",
[2025-05-12T17:51:55.553+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.553+0000] {subprocess.py:93} INFO -   "impact_explanation": "This legal settlement has no direct impact on the gold market.",
[2025-05-12T17:51:55.554+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.555+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.555+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.556+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:51:55.556+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.557+0000] {subprocess.py:93} INFO -   "description_number": "3",
[2025-05-12T17:51:55.557+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:51:55.558+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold's role as a safe haven asset during market volatility, suggesting increased demand for gold.",
[2025-05-12T17:51:55.558+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:51:55.559+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.559+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.560+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:51:55.561+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.561+0000] {subprocess.py:93} INFO -   "description_number": "4",
[2025-05-12T17:51:55.562+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.563+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about blogging and SEO and has no relevance to the gold market.",
[2025-05-12T17:51:55.564+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.564+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.565+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.566+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:51:55.566+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.567+0000] {subprocess.py:93} INFO -   "description_number": "5",
[2025-05-12T17:51:55.568+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.568+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar can increase the price of gold, but this is a general statement and does not provide specific information about the current market situation.",
[2025-05-12T17:51:55.569+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.570+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.570+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.571+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:51:55.571+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.572+0000] {subprocess.py:93} INFO -   "description_number": "6",
[2025-05-12T17:51:55.573+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.573+0000] {subprocess.py:93} INFO -   "impact_explanation": "This news about Trump's business dealings could indirectly impact the market through uncertainty and investor sentiment but does not offer a clear direction for gold prices.",
[2025-05-12T17:51:55.574+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.575+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.575+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.576+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:51:55.577+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.578+0000] {subprocess.py:93} INFO -   "description_number": "7",
[2025-05-12T17:51:55.578+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.579+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article focuses on Web3 investing and is irrelevant to gold market analysis.",
[2025-05-12T17:51:55.580+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.580+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.581+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.581+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:51:55.582+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.582+0000] {subprocess.py:93} INFO -   "description_number": "8",
[2025-05-12T17:51:55.583+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.583+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about cybersecurity and has no impact on the gold market.",
[2025-05-12T17:51:55.584+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.584+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.584+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.585+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:51:55.585+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.586+0000] {subprocess.py:93} INFO -   "description_number": "9",
[2025-05-12T17:51:55.586+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.587+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article about a hotel in Northern California has no relevance to the gold market.",
[2025-05-12T17:51:55.587+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.587+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.588+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.588+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:51:55.589+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.589+0000] {subprocess.py:93} INFO -   "description_number": "10",
[2025-05-12T17:51:55.589+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.590+0000] {subprocess.py:93} INFO -   "impact_explanation": "The development of an XRP spot ETF could affect cryptocurrency markets, but it's unlikely to have a direct and significant effect on gold prices.",
[2025-05-12T17:51:55.590+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.590+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.590+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.591+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:51:55.591+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.591+0000] {subprocess.py:93} INFO -   "description_number": "11",
[2025-05-12T17:51:55.592+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.594+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase is driven by easing tariff tensions, which could reduce demand for safe-haven assets like gold. However, the overall impact is unclear as it depends on the market's reaction to the geopolitical development.",
[2025-05-12T17:51:55.594+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.595+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.596+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.597+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:51:55.597+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.598+0000] {subprocess.py:93} INFO -   "description_number": "12",
[2025-05-12T17:51:55.598+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.599+0000] {subprocess.py:93} INFO -   "impact_explanation": "This legal settlement has no direct impact on the gold market.",
[2025-05-12T17:51:55.599+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.600+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.600+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.601+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:51:55.602+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.602+0000] {subprocess.py:93} INFO -   "description_number": "13",
[2025-05-12T17:51:55.603+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:51:55.603+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold's role as a safe haven asset during market volatility, suggesting increased demand for gold.",
[2025-05-12T17:51:55.604+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:51:55.605+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.605+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.606+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:51:55.606+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.607+0000] {subprocess.py:93} INFO -   "description_number": "14",
[2025-05-12T17:51:55.607+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.608+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about blogging and SEO and has no relevance to the gold market.",
[2025-05-12T17:51:55.608+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.609+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.610+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.611+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:51:55.612+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.613+0000] {subprocess.py:93} INFO -   "description_number": "15",
[2025-05-12T17:51:55.613+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.614+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar can increase the price of gold, but this is a general statement and does not provide specific information about the current market situation.",
[2025-05-12T17:51:55.614+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.615+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.616+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.616+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:51:55.617+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.617+0000] {subprocess.py:93} INFO -   "description_number": "16",
[2025-05-12T17:51:55.618+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.619+0000] {subprocess.py:93} INFO -   "impact_explanation": "This news about Trump's business dealings could indirectly impact the market through uncertainty and investor sentiment but does not offer a clear direction for gold prices.",
[2025-05-12T17:51:55.619+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.620+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.620+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.621+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:51:55.621+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.622+0000] {subprocess.py:93} INFO -   "description_number": "17",
[2025-05-12T17:51:55.622+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.622+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article focuses on Web3 investing and is irrelevant to gold market analysis.",
[2025-05-12T17:51:55.623+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.623+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.624+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.624+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:51:55.624+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.625+0000] {subprocess.py:93} INFO -   "description_number": "18",
[2025-05-12T17:51:55.625+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.626+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about cybersecurity and has no impact on the gold market.",
[2025-05-12T17:51:55.626+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.628+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.628+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.629+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:51:55.629+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.630+0000] {subprocess.py:93} INFO -   "description_number": "19",
[2025-05-12T17:51:55.630+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.631+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article about a hotel in Northern California has no relevance to the gold market.",
[2025-05-12T17:51:55.631+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.631+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.631+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.632+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:51:55.632+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.633+0000] {subprocess.py:93} INFO -   "description_number": "20",
[2025-05-12T17:51:55.633+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.634+0000] {subprocess.py:93} INFO -   "impact_explanation": "The development of an XRP spot ETF could affect cryptocurrency markets, but it's unlikely to have a direct and significant effect on gold prices.",
[2025-05-12T17:51:55.634+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.634+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.634+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.635+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:51:55.635+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.635+0000] {subprocess.py:93} INFO -   "description_number": "21",
[2025-05-12T17:51:55.636+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.636+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase is driven by easing tariff tensions, which could reduce demand for safe-haven assets like gold. However, the overall impact is unclear as it depends on the market's reaction to the geopolitical development.",
[2025-05-12T17:51:55.636+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.636+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.637+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.637+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:51:55.638+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.638+0000] {subprocess.py:93} INFO -   "description_number": "22",
[2025-05-12T17:51:55.638+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.639+0000] {subprocess.py:93} INFO -   "impact_explanation": "This legal settlement has no direct impact on the gold market.",
[2025-05-12T17:51:55.639+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.640+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.640+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.640+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:51:55.641+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.641+0000] {subprocess.py:93} INFO -   "description_number": "23",
[2025-05-12T17:51:55.641+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:51:55.642+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold's role as a safe haven asset during market volatility, suggesting increased demand for gold.",
[2025-05-12T17:51:55.642+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:51:55.644+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.645+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.646+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:51:55.646+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.647+0000] {subprocess.py:93} INFO -   "description_number": "24",
[2025-05-12T17:51:55.647+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.648+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about blogging and SEO and has no relevance to the gold market.",
[2025-05-12T17:51:55.648+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.649+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.649+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.650+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:51:55.651+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.651+0000] {subprocess.py:93} INFO -   "description_number": "25",
[2025-05-12T17:51:55.652+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.652+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar can increase the price of gold, but this is a general statement and does not provide specific information about the current market situation.",
[2025-05-12T17:51:55.653+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.653+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.654+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.654+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:51:55.655+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.655+0000] {subprocess.py:93} INFO -   "description_number": "26",
[2025-05-12T17:51:55.656+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.656+0000] {subprocess.py:93} INFO -   "impact_explanation": "This news about Trump's business dealings could indirectly impact the market through uncertainty and investor sentiment but does not offer a clear direction for gold prices.",
[2025-05-12T17:51:55.657+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.657+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.658+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.658+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:51:55.659+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.661+0000] {subprocess.py:93} INFO -   "description_number": "27",
[2025-05-12T17:51:55.662+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.663+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article focuses on Web3 investing and is irrelevant to gold market analysis.",
[2025-05-12T17:51:55.663+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.664+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.665+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.665+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:51:55.666+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.667+0000] {subprocess.py:93} INFO -   "description_number": "28",
[2025-05-12T17:51:55.667+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.668+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about cybersecurity and has no impact on the gold market.",
[2025-05-12T17:51:55.669+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.669+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.670+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.670+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:51:55.671+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.671+0000] {subprocess.py:93} INFO -   "description_number": "29",
[2025-05-12T17:51:55.672+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.673+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article about a hotel in Northern California has no relevance to the gold market.",
[2025-05-12T17:51:55.673+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.674+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.674+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.675+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:51:55.676+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.678+0000] {subprocess.py:93} INFO -   "description_number": "30",
[2025-05-12T17:51:55.680+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.680+0000] {subprocess.py:93} INFO -   "impact_explanation": "The development of an XRP spot ETF could affect cryptocurrency markets, but it's unlikely to have a direct and significant effect on gold prices.",
[2025-05-12T17:51:55.681+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.682+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.682+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.683+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:51:55.684+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.684+0000] {subprocess.py:93} INFO -   "description_number": "31",
[2025-05-12T17:51:55.685+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.686+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase is driven by easing tariff tensions, which could reduce demand for safe-haven assets like gold. However, the overall impact is unclear as it depends on the market's reaction to the geopolitical development.",
[2025-05-12T17:51:55.686+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.687+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.688+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.688+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:51:55.689+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.690+0000] {subprocess.py:93} INFO -   "description_number": "32",
[2025-05-12T17:51:55.691+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.692+0000] {subprocess.py:93} INFO -   "impact_explanation": "This legal settlement has no direct impact on the gold market.",
[2025-05-12T17:51:55.692+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.695+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.697+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.697+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:51:55.698+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.698+0000] {subprocess.py:93} INFO -   "description_number": "33",
[2025-05-12T17:51:55.699+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:51:55.700+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold's role as a safe haven asset during market volatility, suggesting increased demand for gold.",
[2025-05-12T17:51:55.700+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:51:55.701+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.701+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.702+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:51:55.703+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.703+0000] {subprocess.py:93} INFO -   "description_number": "34",
[2025-05-12T17:51:55.704+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.705+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about blogging and SEO and has no relevance to the gold market.",
[2025-05-12T17:51:55.705+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.706+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.707+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.707+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:51:55.708+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.709+0000] {subprocess.py:93} INFO -   "description_number": "35",
[2025-05-12T17:51:55.709+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.711+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar can increase the price of gold, but this is a general statement and does not provide specific information about the current market situation.",
[2025-05-12T17:51:55.712+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.713+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.714+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.715+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:51:55.715+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.716+0000] {subprocess.py:93} INFO -   "description_number": "36",
[2025-05-12T17:51:55.716+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.717+0000] {subprocess.py:93} INFO -   "impact_explanation": "This news about Trump's business dealings could indirectly impact the market through uncertainty and investor sentiment but does not offer a clear direction for gold prices.",
[2025-05-12T17:51:55.718+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.718+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.719+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.720+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:51:55.720+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.721+0000] {subprocess.py:93} INFO -   "description_number": "37",
[2025-05-12T17:51:55.721+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.722+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article focuses on Web3 investing and is irrelevant to gold market analysis.",
[2025-05-12T17:51:55.723+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.723+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.724+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.724+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:51:55.725+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.727+0000] {subprocess.py:93} INFO -   "description_number": "38",
[2025-05-12T17:51:55.728+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.729+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about cybersecurity and has no impact on the gold market.",
[2025-05-12T17:51:55.729+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.730+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.731+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.731+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:51:55.732+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.733+0000] {subprocess.py:93} INFO -   "description_number": "39",
[2025-05-12T17:51:55.734+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.734+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article about a hotel in Northern California has no relevance to the gold market.",
[2025-05-12T17:51:55.735+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.736+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.736+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.737+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:51:55.738+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.738+0000] {subprocess.py:93} INFO -   "description_number": "40",
[2025-05-12T17:51:55.739+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.740+0000] {subprocess.py:93} INFO -   "impact_explanation": "The development of an XRP spot ETF could affect cryptocurrency markets, but it's unlikely to have a direct and significant effect on gold prices.",
[2025-05-12T17:51:55.740+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.741+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.742+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.742+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:51:55.744+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.745+0000] {subprocess.py:93} INFO -   "description_number": "41",
[2025-05-12T17:51:55.745+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.746+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase is driven by easing tariff tensions, which could reduce demand for safe-haven assets like gold. However, the overall impact is unclear as it depends on the market's reaction to the geopolitical development.",
[2025-05-12T17:51:55.747+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.747+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.748+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.749+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:51:55.749+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.750+0000] {subprocess.py:93} INFO -   "description_number": "42",
[2025-05-12T17:51:55.750+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.751+0000] {subprocess.py:93} INFO -   "impact_explanation": "This legal settlement has no direct impact on the gold market.",
[2025-05-12T17:51:55.752+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.753+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.753+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.754+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:51:55.754+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.755+0000] {subprocess.py:93} INFO -   "description_number": "43",
[2025-05-12T17:51:55.756+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:51:55.756+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold's role as a safe haven asset during market volatility, suggesting increased demand for gold.",
[2025-05-12T17:51:55.762+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:51:55.763+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.764+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.765+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:51:55.765+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.766+0000] {subprocess.py:93} INFO -   "description_number": "44",
[2025-05-12T17:51:55.766+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.767+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about blogging and SEO and has no relevance to the gold market.",
[2025-05-12T17:51:55.767+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.768+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.768+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.769+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:51:55.770+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.770+0000] {subprocess.py:93} INFO -   "description_number": "45",
[2025-05-12T17:51:55.771+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.772+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar can increase the price of gold, but this is a general statement and does not provide specific information about the current market situation.",
[2025-05-12T17:51:55.772+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.773+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.773+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.774+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:51:55.775+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.778+0000] {subprocess.py:93} INFO -   "description_number": "46",
[2025-05-12T17:51:55.779+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.780+0000] {subprocess.py:93} INFO -   "impact_explanation": "This news about Trump's business dealings could indirectly impact the market through uncertainty and investor sentiment but does not offer a clear direction for gold prices.",
[2025-05-12T17:51:55.780+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.781+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.781+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.782+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:51:55.782+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.783+0000] {subprocess.py:93} INFO -   "description_number": "47",
[2025-05-12T17:51:55.784+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.784+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article focuses on Web3 investing and is irrelevant to gold market analysis.",
[2025-05-12T17:51:55.785+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.785+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.786+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.786+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:51:55.787+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.787+0000] {subprocess.py:93} INFO -   "description_number": "48",
[2025-05-12T17:51:55.788+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.789+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about cybersecurity and has no impact on the gold market.",
[2025-05-12T17:51:55.789+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.790+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.790+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.791+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:51:55.791+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.792+0000] {subprocess.py:93} INFO -   "description_number": "49",
[2025-05-12T17:51:55.793+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.793+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article about a hotel in Northern California has no relevance to the gold market.",
[2025-05-12T17:51:55.795+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.796+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.796+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.796+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:51:55.797+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.797+0000] {subprocess.py:93} INFO -   "description_number": "50",
[2025-05-12T17:51:55.798+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.798+0000] {subprocess.py:93} INFO -   "impact_explanation": "The development of an XRP spot ETF could affect cryptocurrency markets, but it's unlikely to have a direct and significant effect on gold prices.",
[2025-05-12T17:51:55.799+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.799+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.800+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.800+0000] {subprocess.py:93} INFO - Analyse pour l'article : Best Altcoins to Buy as Bitcoin Nears All-Time Hig...
[2025-05-12T17:51:55.800+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.801+0000] {subprocess.py:93} INFO -   "description_number": "51",
[2025-05-12T17:51:55.801+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.802+0000] {subprocess.py:93} INFO -   "impact_explanation": "Bitcoin's price increase is driven by easing tariff tensions, which could reduce demand for safe-haven assets like gold. However, the overall impact is unclear as it depends on the market's reaction to the geopolitical development.",
[2025-05-12T17:51:55.802+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.802+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.803+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.803+0000] {subprocess.py:93} INFO - Analyse pour l'article : Google will pay Texas $1.4 billion over its locati...
[2025-05-12T17:51:55.804+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.804+0000] {subprocess.py:93} INFO -   "description_number": "52",
[2025-05-12T17:51:55.805+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.805+0000] {subprocess.py:93} INFO -   "impact_explanation": "This legal settlement has no direct impact on the gold market.",
[2025-05-12T17:51:55.805+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.806+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.806+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.807+0000] {subprocess.py:93} INFO - Analyse pour l'article : In Volatile Markets, RWAs Like Gold Are A Lifeline...
[2025-05-12T17:51:55.807+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.808+0000] {subprocess.py:93} INFO -   "description_number": "53",
[2025-05-12T17:51:55.808+0000] {subprocess.py:93} INFO -   "sentiment": "Positive",
[2025-05-12T17:51:55.810+0000] {subprocess.py:93} INFO -   "impact_explanation": "The article highlights gold's role as a safe haven asset during market volatility, suggesting increased demand for gold.",
[2025-05-12T17:51:55.811+0000] {subprocess.py:93} INFO -   "recommendation": "Buy"
[2025-05-12T17:51:55.812+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.813+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.814+0000] {subprocess.py:93} INFO - Analyse pour l'article : Top Blog SEO Tips for Higher Search Rankings | Sim...
[2025-05-12T17:51:55.814+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.815+0000] {subprocess.py:93} INFO -   "description_number": "54",
[2025-05-12T17:51:55.815+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.815+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about blogging and SEO and has no relevance to the gold market.",
[2025-05-12T17:51:55.816+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.816+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.817+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.817+0000] {subprocess.py:93} INFO - Analyse pour l'article : George W. Bush Lit The Dollar Fire On Which Trump ...
[2025-05-12T17:51:55.818+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.818+0000] {subprocess.py:93} INFO -   "description_number": "55",
[2025-05-12T17:51:55.818+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.819+0000] {subprocess.py:93} INFO -   "impact_explanation": "A weak dollar can increase the price of gold, but this is a general statement and does not provide specific information about the current market situation.",
[2025-05-12T17:51:55.819+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.820+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.820+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.821+0000] {subprocess.py:93} INFO - Analyse pour l'article : The 3 Easy New Ways Anyone Can Funnel Money Direct...
[2025-05-12T17:51:55.821+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.821+0000] {subprocess.py:93} INFO -   "description_number": "56",
[2025-05-12T17:51:55.822+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.822+0000] {subprocess.py:93} INFO -   "impact_explanation": "This news about Trump's business dealings could indirectly impact the market through uncertainty and investor sentiment but does not offer a clear direction for gold prices.",
[2025-05-12T17:51:55.823+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.823+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.824+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.824+0000] {subprocess.py:93} INFO - Analyse pour l'article : How To Invest In Web3 In 2025...
[2025-05-12T17:51:55.825+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.827+0000] {subprocess.py:93} INFO -   "description_number": "57",
[2025-05-12T17:51:55.829+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.829+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article focuses on Web3 investing and is irrelevant to gold market analysis.",
[2025-05-12T17:51:55.830+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.830+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.831+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.831+0000] {subprocess.py:93} INFO - Analyse pour l'article : Shodan-Dorks - Dorks for Shodan; a powerful tool u...
[2025-05-12T17:51:55.832+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.832+0000] {subprocess.py:93} INFO -   "description_number": "58",
[2025-05-12T17:51:55.832+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.833+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article is about cybersecurity and has no impact on the gold market.",
[2025-05-12T17:51:55.834+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.834+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.835+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.835+0000] {subprocess.py:93} INFO - Analyse pour l'article : A historic hotel might be the spark a California c...
[2025-05-12T17:51:55.836+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.836+0000] {subprocess.py:93} INFO -   "description_number": "59",
[2025-05-12T17:51:55.837+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.837+0000] {subprocess.py:93} INFO -   "impact_explanation": "This article about a hotel in Northern California has no relevance to the gold market.",
[2025-05-12T17:51:55.838+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.839+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.839+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:51:55.840+0000] {subprocess.py:93} INFO - Analyse pour l'article : What Is An XRP Spot ETF?...
[2025-05-12T17:51:55.840+0000] {subprocess.py:93} INFO - {
[2025-05-12T17:51:55.840+0000] {subprocess.py:93} INFO -   "description_number": "60",
[2025-05-12T17:51:55.841+0000] {subprocess.py:93} INFO -   "sentiment": "Neutral",
[2025-05-12T17:51:55.842+0000] {subprocess.py:93} INFO -   "impact_explanation": "The development of an XRP spot ETF could affect cryptocurrency markets, but it's unlikely to have a direct and significant effect on gold prices.",
[2025-05-12T17:51:55.844+0000] {subprocess.py:93} INFO -   "recommendation": "Hold"
[2025-05-12T17:51:55.845+0000] {subprocess.py:93} INFO - }
[2025-05-12T17:51:55.850+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO CodeGenerator: Code generated in 24.695269 ms
[2025-05-12T17:51:55.862+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO AppendDataExec: Start processing data source write support: CassandraBulkWrite(org.apache.spark.sql.SparkSession@7ac04918,com.datastax.spark.connector.cql.CassandraConnector@5f24df55,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(description,StringType,true),StructField(id,StringType,true),StructField(ingestion_time,StringType,true),StructField(published_at,StringType,true),StructField(recommendation,StringType,true),StructField(sentiment,StringType,true),StructField(source,StringType,true),StructField(title,StringType,true),StructField(url,StringType,true)),org.apache.spark.SparkConf@2455b6d5). The input RDD has 8 partitions.
[2025-05-12T17:51:55.868+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO SparkContext: Starting job: save at <unknown>:0
[2025-05-12T17:51:55.871+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO DAGScheduler: Got job 3 (save at <unknown>:0) with 8 output partitions
[2025-05-12T17:51:55.872+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO DAGScheduler: Final stage: ResultStage 3 (save at <unknown>:0)
[2025-05-12T17:51:55.873+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:51:55.875+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:51:55.878+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at save at <unknown>:0), which has no missing parents
[2025-05-12T17:51:55.894+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.8 KiB, free 434.3 MiB)
[2025-05-12T17:51:55.921+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 434.3 MiB)
[2025-05-12T17:51:55.922+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on d4e9ca837c07:38409 (size: 10.7 KiB, free: 434.4 MiB)
[2025-05-12T17:51:55.924+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:51:55.928+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO BlockManagerInfo: Removed broadcast_2_piece0 on d4e9ca837c07:38409 in memory (size: 16.1 KiB, free: 434.4 MiB)
[2025-05-12T17:51:55.929+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at save at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T17:51:55.930+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO TaskSchedulerImpl: Adding task set 3.0 with 8 tasks resource profile 0
[2025-05-12T17:51:55.934+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 16303 bytes)
[2025-05-12T17:51:55.936+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (d4e9ca837c07, executor driver, partition 1, PROCESS_LOCAL, 16431 bytes)
[2025-05-12T17:51:55.939+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5) (d4e9ca837c07, executor driver, partition 2, PROCESS_LOCAL, 16058 bytes)
[2025-05-12T17:51:55.940+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 6) (d4e9ca837c07, executor driver, partition 3, PROCESS_LOCAL, 16269 bytes)
[2025-05-12T17:51:55.945+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 7) (d4e9ca837c07, executor driver, partition 4, PROCESS_LOCAL, 16219 bytes)
[2025-05-12T17:51:55.947+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 8) (d4e9ca837c07, executor driver, partition 5, PROCESS_LOCAL, 16311 bytes)
[2025-05-12T17:51:55.948+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 9) (d4e9ca837c07, executor driver, partition 6, PROCESS_LOCAL, 16090 bytes)
[2025-05-12T17:51:55.949+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 10) (d4e9ca837c07, executor driver, partition 7, PROCESS_LOCAL, 18029 bytes)
[2025-05-12T17:51:55.951+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-05-12T17:51:55.954+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
[2025-05-12T17:51:55.966+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO Executor: Running task 2.0 in stage 3.0 (TID 5)
[2025-05-12T17:51:55.986+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO Executor: Running task 3.0 in stage 3.0 (TID 6)
[2025-05-12T17:51:55.987+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO Executor: Running task 4.0 in stage 3.0 (TID 7)
[2025-05-12T17:51:55.996+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO Executor: Running task 6.0 in stage 3.0 (TID 9)
[2025-05-12T17:51:55.999+0000] {subprocess.py:93} INFO - 25/05/12 17:51:55 INFO Executor: Running task 5.0 in stage 3.0 (TID 8)
[2025-05-12T17:51:56.011+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO Executor: Running task 7.0 in stage 3.0 (TID 10)
[2025-05-12T17:51:56.745+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO CodeGenerator: Code generated in 61.246233 ms
[2025-05-12T17:51:56.843+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO PythonRunner: Times: total = 702, boot = 650, init = 52, finish = 0
[2025-05-12T17:51:56.847+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO DataWritingSparkTask: Commit authorized for partition 2 (task 5, attempt 0, stage 3.0)
[2025-05-12T17:51:56.854+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO PythonRunner: Times: total = 695, boot = 631, init = 63, finish = 1
[2025-05-12T17:51:56.861+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO DataWritingSparkTask: Commit authorized for partition 1 (task 4, attempt 0, stage 3.0)
[2025-05-12T17:51:56.878+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO PythonRunner: Times: total = 726, boot = 660, init = 64, finish = 2
[2025-05-12T17:51:56.883+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-05-12T17:51:56.884+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO PythonRunner: Times: total = 663, boot = 610, init = 52, finish = 1
[2025-05-12T17:51:56.890+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO DataWritingSparkTask: Commit authorized for partition 6 (task 9, attempt 0, stage 3.0)
[2025-05-12T17:51:56.902+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO PythonRunner: Times: total = 677, boot = 616, init = 60, finish = 1
[2025-05-12T17:51:56.913+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO DataWritingSparkTask: Commit authorized for partition 3 (task 6, attempt 0, stage 3.0)
[2025-05-12T17:51:56.928+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO PythonRunner: Times: total = 664, boot = 597, init = 66, finish = 1
[2025-05-12T17:51:56.930+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO DataWritingSparkTask: Commit authorized for partition 4 (task 7, attempt 0, stage 3.0)
[2025-05-12T17:51:56.948+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO PythonRunner: Times: total = 705, boot = 644, init = 60, finish = 1
[2025-05-12T17:51:56.950+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO DataWritingSparkTask: Commit authorized for partition 5 (task 8, attempt 0, stage 3.0)
[2025-05-12T17:51:56.951+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO PythonRunner: Times: total = 672, boot = 603, init = 69, finish = 0
[2025-05-12T17:51:56.952+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO DataWritingSparkTask: Commit authorized for partition 7 (task 10, attempt 0, stage 3.0)
[2025-05-12T17:51:56.983+0000] {subprocess.py:93} INFO - 25/05/12 17:51:56 INFO DataWritingSparkTask: Committed partition 2 (task 5, attempt 0, stage 3.0)
[2025-05-12T17:51:57.022+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 2.0 in stage 3.0 (TID 5). 1939 bytes result sent to driver
[2025-05-12T17:51:57.030+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 1092 ms on d4e9ca837c07 (executor driver) (1/8)
[2025-05-12T17:51:57.034+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37585
[2025-05-12T17:51:57.037+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DataWritingSparkTask: Committed partition 1 (task 4, attempt 0, stage 3.0)
[2025-05-12T17:51:57.047+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 1896 bytes result sent to driver
[2025-05-12T17:51:57.049+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-05-12T17:51:57.050+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DataWritingSparkTask: Committed partition 6 (task 9, attempt 0, stage 3.0)
[2025-05-12T17:51:57.052+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 1115 ms on d4e9ca837c07 (executor driver) (2/8)
[2025-05-12T17:51:57.056+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1896 bytes result sent to driver
[2025-05-12T17:51:57.063+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DataWritingSparkTask: Committed partition 4 (task 7, attempt 0, stage 3.0)
[2025-05-12T17:51:57.064+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DataWritingSparkTask: Committed partition 5 (task 8, attempt 0, stage 3.0)
[2025-05-12T17:51:57.066+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 6.0 in stage 3.0 (TID 9). 1896 bytes result sent to driver
[2025-05-12T17:51:57.068+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DataWritingSparkTask: Committed partition 7 (task 10, attempt 0, stage 3.0)
[2025-05-12T17:51:57.069+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 9) in 1122 ms on d4e9ca837c07 (executor driver) (3/8)
[2025-05-12T17:51:57.073+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DataWritingSparkTask: Committed partition 3 (task 6, attempt 0, stage 3.0)
[2025-05-12T17:51:57.081+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 4.0 in stage 3.0 (TID 7). 1939 bytes result sent to driver
[2025-05-12T17:51:57.082+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1143 ms on d4e9ca837c07 (executor driver) (4/8)
[2025-05-12T17:51:57.082+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 5.0 in stage 3.0 (TID 8). 1939 bytes result sent to driver
[2025-05-12T17:51:57.083+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 7.0 in stage 3.0 (TID 10). 1939 bytes result sent to driver
[2025-05-12T17:51:57.084+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 7) in 1138 ms on d4e9ca837c07 (executor driver) (5/8)
[2025-05-12T17:51:57.085+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 8) in 1140 ms on d4e9ca837c07 (executor driver) (6/8)
[2025-05-12T17:51:57.085+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 10) in 1136 ms on d4e9ca837c07 (executor driver) (7/8)
[2025-05-12T17:51:57.086+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 3.0 in stage 3.0 (TID 6). 1939 bytes result sent to driver
[2025-05-12T17:51:57.089+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 6) in 1149 ms on d4e9ca837c07 (executor driver) (8/8)
[2025-05-12T17:51:57.090+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-05-12T17:51:57.095+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: ResultStage 3 (save at <unknown>:0) finished in 1.213 s
[2025-05-12T17:51:57.097+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:51:57.097+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-05-12T17:51:57.098+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: Job 3 finished: save at <unknown>:0, took 1.228656 s
[2025-05-12T17:51:57.099+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@7ac04918,com.datastax.spark.connector.cql.CassandraConnector@5f24df55,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(description,StringType,true),StructField(id,StringType,true),StructField(ingestion_time,StringType,true),StructField(published_at,StringType,true),StructField(recommendation,StringType,true),StructField(sentiment,StringType,true),StructField(source,StringType,true),StructField(title,StringType,true),StructField(url,StringType,true)),org.apache.spark.SparkConf@2455b6d5) is committing.
[2025-05-12T17:51:57.100+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@7ac04918,com.datastax.spark.connector.cql.CassandraConnector@5f24df55,TableDef(gold_news,articles,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,VarCharType)),ArrayBuffer(),Stream(ColumnDef(description,RegularColumn,VarCharType), ColumnDef(ingestion_time,RegularColumn,VarCharType), ColumnDef(published_at,RegularColumn,VarCharType), ColumnDef(recommendation,RegularColumn,VarCharType), ColumnDef(sentiment,RegularColumn,VarCharType), ColumnDef(source,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(url,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,ONE,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(description,StringType,true),StructField(id,StringType,true),StructField(ingestion_time,StringType,true),StructField(published_at,StringType,true),StructField(recommendation,StringType,true),StructField(sentiment,StringType,true),StructField(source,StringType,true),StructField(title,StringType,true),StructField(url,StringType,true)),org.apache.spark.SparkConf@2455b6d5) committed.
[2025-05-12T17:51:57.603+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO CodeGenerator: Code generated in 45.221382 ms
[2025-05-12T17:51:57.640+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: Registering RDD 20 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) as input to shuffle 0
[2025-05-12T17:51:57.649+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: Got map stage job 4 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 8 output partitions
[2025-05-12T17:51:57.650+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:51:57.650+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:51:57.653+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:51:57.661+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:51:57.685+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 25.9 KiB, free 434.3 MiB)
[2025-05-12T17:51:57.704+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 434.3 MiB)
[2025-05-12T17:51:57.706+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on d4e9ca837c07:38409 (size: 11.6 KiB, free: 434.4 MiB)
[2025-05-12T17:51:57.711+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on d4e9ca837c07:38409 in memory (size: 10.7 KiB, free: 434.4 MiB)
[2025-05-12T17:51:57.714+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:51:57.723+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T17:51:57.726+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSchedulerImpl: Adding task set 4.0 with 8 tasks resource profile 0
[2025-05-12T17:51:57.729+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 16292 bytes)
[2025-05-12T17:51:57.731+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 12) (d4e9ca837c07, executor driver, partition 1, PROCESS_LOCAL, 16420 bytes)
[2025-05-12T17:51:57.732+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 13) (d4e9ca837c07, executor driver, partition 2, PROCESS_LOCAL, 16047 bytes)
[2025-05-12T17:51:57.733+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 14) (d4e9ca837c07, executor driver, partition 3, PROCESS_LOCAL, 16258 bytes)
[2025-05-12T17:51:57.736+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 15) (d4e9ca837c07, executor driver, partition 4, PROCESS_LOCAL, 16208 bytes)
[2025-05-12T17:51:57.737+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 16) (d4e9ca837c07, executor driver, partition 5, PROCESS_LOCAL, 16300 bytes)
[2025-05-12T17:51:57.739+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 17) (d4e9ca837c07, executor driver, partition 6, PROCESS_LOCAL, 16079 bytes)
[2025-05-12T17:51:57.745+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 18) (d4e9ca837c07, executor driver, partition 7, PROCESS_LOCAL, 18018 bytes)
[2025-05-12T17:51:57.747+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Running task 0.0 in stage 4.0 (TID 11)
[2025-05-12T17:51:57.748+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Running task 1.0 in stage 4.0 (TID 12)
[2025-05-12T17:51:57.753+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Running task 3.0 in stage 4.0 (TID 14)
[2025-05-12T17:51:57.755+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Running task 6.0 in stage 4.0 (TID 17)
[2025-05-12T17:51:57.756+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Running task 4.0 in stage 4.0 (TID 15)
[2025-05-12T17:51:57.760+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Running task 5.0 in stage 4.0 (TID 16)
[2025-05-12T17:51:57.761+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Running task 7.0 in stage 4.0 (TID 18)
[2025-05-12T17:51:57.765+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Running task 2.0 in stage 4.0 (TID 13)
[2025-05-12T17:51:57.889+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO CodeGenerator: Code generated in 52.493046 ms
[2025-05-12T17:51:57.928+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO PythonRunner: Times: total = 72, boot = -1071, init = 1142, finish = 1
[2025-05-12T17:51:57.929+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO PythonRunner: Times: total = 61, boot = -1030, init = 1091, finish = 0
[2025-05-12T17:51:57.930+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO PythonRunner: Times: total = 82, boot = -1020, init = 1102, finish = 0
[2025-05-12T17:51:57.931+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO PythonRunner: Times: total = 52, boot = -1041, init = 1093, finish = 0
[2025-05-12T17:51:57.932+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO PythonRunner: Times: total = 81, boot = -1039, init = 1120, finish = 0
[2025-05-12T17:51:57.932+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO PythonRunner: Times: total = 73, boot = -1056, init = 1128, finish = 1
[2025-05-12T17:51:57.933+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO PythonRunner: Times: total = 52, boot = -1034, init = 1086, finish = 0
[2025-05-12T17:51:57.936+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO PythonRunner: Times: total = 52, boot = -1082, init = 1134, finish = 0
[2025-05-12T17:51:57.998+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 6.0 in stage 4.0 (TID 17). 2362 bytes result sent to driver
[2025-05-12T17:51:57.999+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 5.0 in stage 4.0 (TID 16). 2362 bytes result sent to driver
[2025-05-12T17:51:58.000+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 11). 2362 bytes result sent to driver
[2025-05-12T17:51:58.000+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 1.0 in stage 4.0 (TID 12). 2362 bytes result sent to driver
[2025-05-12T17:51:58.002+0000] {subprocess.py:93} INFO - 25/05/12 17:51:57 INFO Executor: Finished task 3.0 in stage 4.0 (TID 14). 2362 bytes result sent to driver
[2025-05-12T17:51:58.003+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 16) in 265 ms on d4e9ca837c07 (executor driver) (1/8)
[2025-05-12T17:51:58.004+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 17) in 263 ms on d4e9ca837c07 (executor driver) (2/8)
[2025-05-12T17:51:58.007+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 276 ms on d4e9ca837c07 (executor driver) (3/8)
[2025-05-12T17:51:58.008+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 12) in 274 ms on d4e9ca837c07 (executor driver) (4/8)
[2025-05-12T17:51:58.009+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 14) in 272 ms on d4e9ca837c07 (executor driver) (5/8)
[2025-05-12T17:51:58.012+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO Executor: Finished task 7.0 in stage 4.0 (TID 18). 2362 bytes result sent to driver
[2025-05-12T17:51:58.027+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO Executor: Finished task 4.0 in stage 4.0 (TID 15). 2319 bytes result sent to driver
[2025-05-12T17:51:58.029+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO Executor: Finished task 2.0 in stage 4.0 (TID 13). 2319 bytes result sent to driver
[2025-05-12T17:51:58.030+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 18) in 290 ms on d4e9ca837c07 (executor driver) (6/8)
[2025-05-12T17:51:58.031+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 13) in 300 ms on d4e9ca837c07 (executor driver) (7/8)
[2025-05-12T17:51:58.034+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 15) in 299 ms on d4e9ca837c07 (executor driver) (8/8)
[2025-05-12T17:51:58.036+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-05-12T17:51:58.040+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: ShuffleMapStage 4 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.373 s
[2025-05-12T17:51:58.041+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: looking for newly runnable stages
[2025-05-12T17:51:58.042+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: running: Set()
[2025-05-12T17:51:58.045+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: waiting: Set()
[2025-05-12T17:51:58.046+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: failed: Set()
[2025-05-12T17:51:58.177+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO CodeGenerator: Code generated in 34.227853 ms
[2025-05-12T17:51:58.225+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T17:51:58.229+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Got job 5 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T17:51:58.230+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Final stage: ResultStage 6 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:51:58.232+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2025-05-12T17:51:58.233+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:51:58.234+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:51:58.253+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.1 KiB, free 434.3 MiB)
[2025-05-12T17:51:58.278+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO BlockManagerInfo: Removed broadcast_4_piece0 on d4e9ca837c07:38409 in memory (size: 11.6 KiB, free: 434.4 MiB)
[2025-05-12T17:51:58.279+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.3 MiB)
[2025-05-12T17:51:58.286+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on d4e9ca837c07:38409 (size: 7.4 KiB, free: 434.4 MiB)
[2025-05-12T17:51:58.288+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:51:58.289+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:51:58.290+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-05-12T17:51:58.301+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 19) (d4e9ca837c07, executor driver, partition 0, NODE_LOCAL, 12874 bytes)
[2025-05-12T17:51:58.303+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO Executor: Running task 0.0 in stage 6.0 (TID 19)
[2025-05-12T17:51:58.416+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 (624.0 B) non-empty blocks including 8 (624.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-12T17:51:58.418+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 63 ms
[2025-05-12T17:51:58.455+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO CodeGenerator: Code generated in 28.664608 ms
[2025-05-12T17:51:58.504+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO Executor: Finished task 0.0 in stage 6.0 (TID 19). 4056 bytes result sent to driver
[2025-05-12T17:51:58.518+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 19) in 220 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:51:58.519+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: ResultStage 6 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.270 s
[2025-05-12T17:51:58.523+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:51:58.527+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-05-12T17:51:58.529+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-05-12T17:51:58.530+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Job 5 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.298466 s
[2025-05-12T17:51:58.895+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO CodeGenerator: Code generated in 125.013123 ms
[2025-05-12T17:51:58.932+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Registering RDD 25 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) as input to shuffle 1
[2025-05-12T17:51:58.933+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Got map stage job 6 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 8 output partitions
[2025-05-12T17:51:58.934+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:51:58.935+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Parents of final stage: List()
[2025-05-12T17:51:58.940+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:51:58.942+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[25] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:51:58.958+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.3 KiB, free 434.3 MiB)
[2025-05-12T17:51:58.980+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 434.3 MiB)
[2025-05-12T17:51:58.987+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on d4e9ca837c07:38409 (size: 18.4 KiB, free: 434.4 MiB)
[2025-05-12T17:51:58.990+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:51:58.991+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[25] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[2025-05-12T17:51:58.991+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSchedulerImpl: Adding task set 7.0 with 8 tasks resource profile 0
[2025-05-12T17:51:58.997+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 20) (d4e9ca837c07, executor driver, partition 0, PROCESS_LOCAL, 16292 bytes)
[2025-05-12T17:51:59.000+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 21) (d4e9ca837c07, executor driver, partition 1, PROCESS_LOCAL, 16420 bytes)
[2025-05-12T17:51:59.001+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 22) (d4e9ca837c07, executor driver, partition 2, PROCESS_LOCAL, 16047 bytes)
[2025-05-12T17:51:59.002+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 23) (d4e9ca837c07, executor driver, partition 3, PROCESS_LOCAL, 16258 bytes)
[2025-05-12T17:51:59.003+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 24) (d4e9ca837c07, executor driver, partition 4, PROCESS_LOCAL, 16208 bytes)
[2025-05-12T17:51:59.004+0000] {subprocess.py:93} INFO - 25/05/12 17:51:58 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 25) (d4e9ca837c07, executor driver, partition 5, PROCESS_LOCAL, 16300 bytes)
[2025-05-12T17:51:59.004+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 26) (d4e9ca837c07, executor driver, partition 6, PROCESS_LOCAL, 16079 bytes)
[2025-05-12T17:51:59.006+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO BlockManagerInfo: Removed broadcast_5_piece0 on d4e9ca837c07:38409 in memory (size: 7.4 KiB, free: 434.4 MiB)
[2025-05-12T17:51:59.022+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 27) (d4e9ca837c07, executor driver, partition 7, PROCESS_LOCAL, 18018 bytes)
[2025-05-12T17:51:59.023+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO Executor: Running task 0.0 in stage 7.0 (TID 20)
[2025-05-12T17:51:59.024+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO Executor: Running task 7.0 in stage 7.0 (TID 27)
[2025-05-12T17:51:59.025+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO Executor: Running task 3.0 in stage 7.0 (TID 23)
[2025-05-12T17:51:59.030+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO Executor: Running task 6.0 in stage 7.0 (TID 26)
[2025-05-12T17:51:59.031+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO Executor: Running task 4.0 in stage 7.0 (TID 24)
[2025-05-12T17:51:59.032+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO Executor: Running task 5.0 in stage 7.0 (TID 25)
[2025-05-12T17:51:59.033+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO Executor: Running task 1.0 in stage 7.0 (TID 21)
[2025-05-12T17:51:59.033+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO Executor: Running task 2.0 in stage 7.0 (TID 22)
[2025-05-12T17:51:59.184+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO CodeGenerator: Code generated in 118.283482 ms
[2025-05-12T17:51:59.313+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO CodeGenerator: Code generated in 90.915172 ms
[2025-05-12T17:51:59.355+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO CodeGenerator: Code generated in 24.80208 ms
[2025-05-12T17:51:59.503+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO CodeGenerator: Code generated in 39.69045 ms
[2025-05-12T17:51:59.571+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO CodeGenerator: Code generated in 32.559167 ms
[2025-05-12T17:51:59.648+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO PythonRunner: Times: total = 65, boot = -1191, init = 1256, finish = 0
[2025-05-12T17:51:59.650+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO PythonRunner: Times: total = 123, boot = -1161, init = 1284, finish = 0
[2025-05-12T17:51:59.651+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO PythonRunner: Times: total = 60, boot = -1142, init = 1202, finish = 0
[2025-05-12T17:51:59.652+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO PythonRunner: Times: total = 56, boot = -1181, init = 1237, finish = 0
[2025-05-12T17:51:59.652+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO PythonRunner: Times: total = 112, boot = -1183, init = 1295, finish = 0
[2025-05-12T17:51:59.664+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO PythonRunner: Times: total = 51, boot = -1169, init = 1220, finish = 0
[2025-05-12T17:51:59.672+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO PythonRunner: Times: total = 56, boot = -1131, init = 1187, finish = 0
[2025-05-12T17:51:59.692+0000] {subprocess.py:93} INFO - 25/05/12 17:51:59 INFO PythonRunner: Times: total = 119, boot = -1204, init = 1323, finish = 0
[2025-05-12T17:52:00.676+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO Executor: Finished task 7.0 in stage 7.0 (TID 27). 2925 bytes result sent to driver
[2025-05-12T17:52:00.692+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 27) in 1597 ms on d4e9ca837c07 (executor driver) (1/8)
[2025-05-12T17:52:00.697+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO Executor: Finished task 2.0 in stage 7.0 (TID 22). 2925 bytes result sent to driver
[2025-05-12T17:52:00.698+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO Executor: Finished task 6.0 in stage 7.0 (TID 26). 2925 bytes result sent to driver
[2025-05-12T17:52:00.700+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 22) in 1639 ms on d4e9ca837c07 (executor driver) (2/8)
[2025-05-12T17:52:00.701+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO Executor: Finished task 0.0 in stage 7.0 (TID 20). 2925 bytes result sent to driver
[2025-05-12T17:52:00.712+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO Executor: Finished task 1.0 in stage 7.0 (TID 21). 2925 bytes result sent to driver
[2025-05-12T17:52:00.715+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 26) in 1654 ms on d4e9ca837c07 (executor driver) (3/8)
[2025-05-12T17:52:00.716+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 21) in 1663 ms on d4e9ca837c07 (executor driver) (4/8)
[2025-05-12T17:52:00.726+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO Executor: Finished task 3.0 in stage 7.0 (TID 23). 2882 bytes result sent to driver
[2025-05-12T17:52:00.763+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO Executor: Finished task 5.0 in stage 7.0 (TID 25). 2925 bytes result sent to driver
[2025-05-12T17:52:00.807+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 25) in 1687 ms on d4e9ca837c07 (executor driver) (5/8)
[2025-05-12T17:52:00.821+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 23) in 1690 ms on d4e9ca837c07 (executor driver) (6/8)
[2025-05-12T17:52:00.850+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 20) in 1695 ms on d4e9ca837c07 (executor driver) (7/8)
[2025-05-12T17:52:00.864+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO Executor: Finished task 4.0 in stage 7.0 (TID 24). 2925 bytes result sent to driver
[2025-05-12T17:52:00.879+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 24) in 1734 ms on d4e9ca837c07 (executor driver) (8/8)
[2025-05-12T17:52:00.895+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-05-12T17:52:00.908+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO DAGScheduler: ShuffleMapStage 7 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.789 s
[2025-05-12T17:52:00.925+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO DAGScheduler: looking for newly runnable stages
[2025-05-12T17:52:00.953+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO DAGScheduler: running: Set()
[2025-05-12T17:52:00.969+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO DAGScheduler: waiting: Set()
[2025-05-12T17:52:00.983+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO DAGScheduler: failed: Set()
[2025-05-12T17:52:00.990+0000] {subprocess.py:93} INFO - 25/05/12 17:52:00 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-05-12T17:52:01.055+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2025-05-12T17:52:01.185+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO CodeGenerator: Code generated in 74.864279 ms
[2025-05-12T17:52:01.372+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO SparkContext: Starting job: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2025-05-12T17:52:01.398+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO DAGScheduler: Got job 7 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2025-05-12T17:52:01.403+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO DAGScheduler: Final stage: ResultStage 9 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2025-05-12T17:52:01.405+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[2025-05-12T17:52:01.406+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO DAGScheduler: Missing parents: List()
[2025-05-12T17:52:01.407+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[28] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2025-05-12T17:52:01.417+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 40.5 KiB, free 434.3 MiB)
[2025-05-12T17:52:01.429+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 434.3 MiB)
[2025-05-12T17:52:01.444+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on d4e9ca837c07:38409 (size: 19.5 KiB, free: 434.4 MiB)
[2025-05-12T17:52:01.445+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-05-12T17:52:01.447+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[28] at call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2025-05-12T17:52:01.459+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-05-12T17:52:01.461+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 28) (d4e9ca837c07, executor driver, partition 0, NODE_LOCAL, 12874 bytes)
[2025-05-12T17:52:01.463+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO Executor: Running task 0.0 in stage 9.0 (TID 28)
[2025-05-12T17:52:01.488+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO ShuffleBlockFetcherIterator: Getting 8 (1008.0 B) non-empty blocks including 8 (1008.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-12T17:52:01.490+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2025-05-12T17:52:01.555+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO CodeGenerator: Code generated in 53.327998 ms
[2025-05-12T17:52:01.608+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO Executor: Finished task 0.0 in stage 9.0 (TID 28). 5262 bytes result sent to driver
[2025-05-12T17:52:01.622+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 28) in 176 ms on d4e9ca837c07 (executor driver) (1/1)
[2025-05-12T17:52:01.637+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-05-12T17:52:01.651+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO DAGScheduler: ResultStage 9 (call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.239 s
[2025-05-12T17:52:01.670+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-12T17:52:01.703+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-05-12T17:52:01.725+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO DAGScheduler: Job 7 finished: call at /opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.266390 s
[2025-05-12T17:52:01.769+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on d4e9ca837c07:38409 in memory (size: 18.4 KiB, free: 434.4 MiB)
[2025-05-12T17:52:01.790+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 ERROR MicroBatchExecution: Query [id = 7c53f13a-bae6-410c-8999-b3f236a45e54, runId = cbaeb3ee-efcd-4429-81f8-2bcfa7ac9950] terminated with error
[2025-05-12T17:52:01.813+0000] {subprocess.py:93} INFO - py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
[2025-05-12T17:52:01.814+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
[2025-05-12T17:52:01.819+0000] {subprocess.py:93} INFO -     return_value = getattr(self.pool[obj_id], method)(*params)
[2025-05-12T17:52:01.820+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
[2025-05-12T17:52:01.824+0000] {subprocess.py:93} INFO -     raise e
[2025-05-12T17:52:01.825+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
[2025-05-12T17:52:01.828+0000] {subprocess.py:93} INFO -     self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
[2025-05-12T17:52:01.828+0000] {subprocess.py:93} INFO -   File "/scripts/spark_news.py", line 175, in process_batch
[2025-05-12T17:52:01.831+0000] {subprocess.py:93} INFO -     dominant_recommendation = max(recommendation_counts, key=lambda x: x["count"])["recommendation"]
[2025-05-12T17:52:01.832+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 174, in wrapped
[2025-05-12T17:52:01.833+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-12T17:52:01.848+0000] {subprocess.py:93} INFO - TypeError: max() got an unexpected keyword argument 'key'
[2025-05-12T17:52:01.849+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:52:01.850+0000] {subprocess.py:93} INFO - 	at py4j.Protocol.getReturnValue(Protocol.java:476)
[2025-05-12T17:52:01.869+0000] {subprocess.py:93} INFO - 	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
[2025-05-12T17:52:01.884+0000] {subprocess.py:93} INFO - 	at com.sun.proxy.$Proxy37.call(Unknown Source)
[2025-05-12T17:52:01.889+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
[2025-05-12T17:52:01.891+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
[2025-05-12T17:52:01.893+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
[2025-05-12T17:52:01.895+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
[2025-05-12T17:52:01.896+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-05-12T17:52:01.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-05-12T17:52:01.911+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-05-12T17:52:01.916+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-05-12T17:52:01.917+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-05-12T17:52:01.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-05-12T17:52:01.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-05-12T17:52:01.919+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-05-12T17:52:01.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-05-12T17:52:01.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-05-12T17:52:01.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-05-12T17:52:01.921+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-05-12T17:52:01.922+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-05-12T17:52:01.922+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-05-12T17:52:01.924+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-05-12T17:52:01.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-05-12T17:52:01.932+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
[2025-05-12T17:52:01.934+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-05-12T17:52:01.935+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-05-12T17:52:01.936+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-05-12T17:52:01.937+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-05-12T17:52:01.937+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-05-12T17:52:01.938+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-05-12T17:52:01.938+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-05-12T17:52:01.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-05-12T17:52:01.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-05-12T17:52:01.940+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
[2025-05-12T17:52:01.955+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:52:01.957+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:52:01.960+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO Metrics: Metrics reporters closed
[2025-05-12T17:52:01.961+0000] {subprocess.py:93} INFO - 25/05/12 17:52:01 INFO MicroBatchExecution: Async log purge executor pool for query [id = 7c53f13a-bae6-410c-8999-b3f236a45e54, runId = cbaeb3ee-efcd-4429-81f8-2bcfa7ac9950] has been shutdown
[2025-05-12T17:52:02.049+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-05-12T17:52:02.055+0000] {subprocess.py:93} INFO -   File "/scripts/spark_news.py", line 228, in <module>
[2025-05-12T17:52:02.056+0000] {subprocess.py:93} INFO -     analysis_query.awaitTermination()
[2025-05-12T17:52:02.057+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
[2025-05-12T17:52:02.057+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-05-12T17:52:02.058+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-05-12T17:52:02.058+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 7c53f13a-bae6-410c-8999-b3f236a45e54, runId = cbaeb3ee-efcd-4429-81f8-2bcfa7ac9950] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
[2025-05-12T17:52:02.059+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
[2025-05-12T17:52:02.060+0000] {subprocess.py:93} INFO -     return_value = getattr(self.pool[obj_id], method)(*params)
[2025-05-12T17:52:02.061+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
[2025-05-12T17:52:02.061+0000] {subprocess.py:93} INFO -     raise e
[2025-05-12T17:52:02.062+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
[2025-05-12T17:52:02.063+0000] {subprocess.py:93} INFO -     self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
[2025-05-12T17:52:02.063+0000] {subprocess.py:93} INFO -   File "/scripts/spark_news.py", line 175, in process_batch
[2025-05-12T17:52:02.063+0000] {subprocess.py:93} INFO -     dominant_recommendation = max(recommendation_counts, key=lambda x: x["count"])["recommendation"]
[2025-05-12T17:52:02.064+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 174, in wrapped
[2025-05-12T17:52:02.064+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-12T17:52:02.067+0000] {subprocess.py:93} INFO - TypeError: max() got an unexpected keyword argument 'key'
[2025-05-12T17:52:02.069+0000] {subprocess.py:93} INFO - 
[2025-05-12T17:52:02.196+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-05-12T17:52:02.197+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2, groupId=spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-05-12T17:52:02.198+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:52:02.199+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:52:02.200+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO Metrics: Metrics reporters closed
[2025-05-12T17:52:02.217+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-a8e2e341-58e1-47f7-a569-5ffb0638c8bd-1979046052-executor-2 unregistered
[2025-05-12T17:52:02.223+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-05-12T17:52:02.225+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1, groupId=spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-05-12T17:52:02.227+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO Metrics: Metrics scheduler closed
[2025-05-12T17:52:02.227+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-05-12T17:52:02.228+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO Metrics: Metrics reporters closed
[2025-05-12T17:52:02.229+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-73fec697-518b-4631-84b5-9cd3be567dfd-354523732-executor-1 unregistered
[2025-05-12T17:52:02.230+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO SparkContext: Invoking stop() from shutdown hook
[2025-05-12T17:52:02.231+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-05-12T17:52:02.286+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO CassandraConnector: Disconnected from Cassandra cluster.
[2025-05-12T17:52:02.288+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
[2025-05-12T17:52:02.288+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO SparkUI: Stopped Spark web UI at http://d4e9ca837c07:4040
[2025-05-12T17:52:02.353+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-05-12T17:52:02.395+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO MemoryStore: MemoryStore cleared
[2025-05-12T17:52:02.396+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO BlockManager: BlockManager stopped
[2025-05-12T17:52:02.405+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-05-12T17:52:02.409+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-05-12T17:52:02.428+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO SparkContext: Successfully stopped SparkContext
[2025-05-12T17:52:02.429+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO ShutdownHookManager: Shutdown hook called
[2025-05-12T17:52:02.429+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-171331d1-6446-49cb-b96a-fff6ab371127
[2025-05-12T17:52:02.434+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713/pyspark-0ed42106-694a-4454-ac2c-1da20141e59c
[2025-05-12T17:52:02.438+0000] {subprocess.py:93} INFO - 25/05/12 17:52:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-49f3781f-e74c-4037-9703-c9760a5f8713
[2025-05-12T17:52:02.546+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-05-12T17:52:02.563+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-05-12T17:52:02.570+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=gold_news_pipeline, task_id=run_consumer, execution_date=20250512T174103, start_date=20250512T175109, end_date=20250512T175202
[2025-05-12T17:52:02.596+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 91 for task run_consumer (Bash command failed. The command returned a non-zero exit code 1.; 5451)
[2025-05-12T17:52:02.630+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-05-12T17:52:02.674+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
