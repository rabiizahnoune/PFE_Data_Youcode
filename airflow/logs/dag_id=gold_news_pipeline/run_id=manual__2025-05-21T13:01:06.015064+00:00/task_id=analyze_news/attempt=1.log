[2025-05-21T13:01:25.224+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: gold_news_pipeline.analyze_news manual__2025-05-21T13:01:06.015064+00:00 [queued]>
[2025-05-21T13:01:25.237+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: gold_news_pipeline.analyze_news manual__2025-05-21T13:01:06.015064+00:00 [queued]>
[2025-05-21T13:01:25.238+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-05-21T13:01:25.255+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): analyze_news> on 2025-05-21 13:01:06.015064+00:00
[2025-05-21T13:01:25.264+0000] {standard_task_runner.py:57} INFO - Started process 3276 to run task
[2025-05-21T13:01:25.267+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'gold_news_pipeline', 'analyze_news', 'manual__2025-05-21T13:01:06.015064+00:00', '--job-id', '134', '--raw', '--subdir', 'DAGS_FOLDER/news_batch.py', '--cfg-path', '/tmp/tmpuxg2ja1v']
[2025-05-21T13:01:25.271+0000] {standard_task_runner.py:85} INFO - Job 134: Subtask analyze_news
[2025-05-21T13:01:25.327+0000] {task_command.py:415} INFO - Running <TaskInstance: gold_news_pipeline.analyze_news manual__2025-05-21T13:01:06.015064+00:00 [running]> on host 5c6832b8ad2a
[2025-05-21T13:01:25.439+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='gold_news_pipeline' AIRFLOW_CTX_TASK_ID='analyze_news' AIRFLOW_CTX_EXECUTION_DATE='2025-05-21T13:01:06.015064+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-21T13:01:06.015064+00:00'
[2025-05-21T13:01:25.442+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-05-21T13:01:25.444+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec gold_price_project-spark-1 spark-submit /scripts/analyze_news.py hdfs://namenode:9000/gold_price/news/raw/year=2025/month=5/day=21']
[2025-05-21T13:01:25.463+0000] {subprocess.py:86} INFO - Output:
[2025-05-21T13:01:29.957+0000] {subprocess.py:93} INFO - 25/05/21 13:01:29 INFO SparkContext: Running Spark version 3.5.1
[2025-05-21T13:01:29.959+0000] {subprocess.py:93} INFO - 25/05/21 13:01:29 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-21T13:01:29.961+0000] {subprocess.py:93} INFO - 25/05/21 13:01:29 INFO SparkContext: Java version 11.0.22
[2025-05-21T13:01:30.040+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-21T13:01:30.165+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO ResourceUtils: ==============================================================
[2025-05-21T13:01:30.166+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-05-21T13:01:30.167+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO ResourceUtils: ==============================================================
[2025-05-21T13:01:30.168+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SparkContext: Submitted application: GoldMarketNewsAnalysis
[2025-05-21T13:01:30.204+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-05-21T13:01:30.228+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO ResourceProfile: Limiting resource is cpu
[2025-05-21T13:01:30.230+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-05-21T13:01:30.298+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SecurityManager: Changing view acls to: root
[2025-05-21T13:01:30.299+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SecurityManager: Changing modify acls to: root
[2025-05-21T13:01:30.300+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SecurityManager: Changing view acls groups to:
[2025-05-21T13:01:30.302+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SecurityManager: Changing modify acls groups to:
[2025-05-21T13:01:30.303+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
[2025-05-21T13:01:30.694+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO Utils: Successfully started service 'sparkDriver' on port 44273.
[2025-05-21T13:01:30.745+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SparkEnv: Registering MapOutputTracker
[2025-05-21T13:01:30.813+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SparkEnv: Registering BlockManagerMaster
[2025-05-21T13:01:30.857+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-05-21T13:01:30.858+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-05-21T13:01:30.868+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-05-21T13:01:30.909+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7fdebf7c-fa00-42b8-a181-e8efa7291e13
[2025-05-21T13:01:30.930+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-05-21T13:01:30.954+0000] {subprocess.py:93} INFO - 25/05/21 13:01:30 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-05-21T13:01:31.178+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-05-21T13:01:31.293+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-05-21T13:01:31.434+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO Executor: Starting executor ID driver on host 8567accc62ca
[2025-05-21T13:01:31.435+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-05-21T13:01:31.435+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO Executor: Java version 11.0.22
[2025-05-21T13:01:31.447+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-05-21T13:01:31.448+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@8911b8e for default.
[2025-05-21T13:01:31.482+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34307.
[2025-05-21T13:01:31.483+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO NettyBlockTransferService: Server created on 8567accc62ca:34307
[2025-05-21T13:01:31.486+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-05-21T13:01:31.494+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8567accc62ca, 34307, None)
[2025-05-21T13:01:31.500+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO BlockManagerMasterEndpoint: Registering block manager 8567accc62ca:34307 with 434.4 MiB RAM, BlockManagerId(driver, 8567accc62ca, 34307, None)
[2025-05-21T13:01:31.506+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8567accc62ca, 34307, None)
[2025-05-21T13:01:31.508+0000] {subprocess.py:93} INFO - 25/05/21 13:01:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8567accc62ca, 34307, None)
[2025-05-21T13:01:32.105+0000] {subprocess.py:93} INFO - INFO:__main__:Lecture du fichier JSON depuis : /gold_price/news/raw/year=2025/month=5/day=21/news_data.json
[2025-05-21T13:01:32.193+0000] {subprocess.py:93} INFO - 25/05/21 13:01:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-05-21T13:01:32.208+0000] {subprocess.py:93} INFO - 25/05/21 13:01:32 INFO SharedState: Warehouse path is 'file:/opt/spark-apps/spark-warehouse'.
[2025-05-21T13:01:34.312+0000] {subprocess.py:93} INFO - 25/05/21 13:01:34 INFO InMemoryFileIndex: It took 96 ms to list leaf files for 1 paths.
[2025-05-21T13:01:36.808+0000] {subprocess.py:93} INFO - 25/05/21 13:01:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(title),IsNotNull(_corrupt_record)
[2025-05-21T13:01:36.813+0000] {subprocess.py:93} INFO - 25/05/21 13:01:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(title#0),isnotnull(_corrupt_record#6)
[2025-05-21T13:01:38.077+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO CodeGenerator: Code generated in 410.998895 ms
[2025-05-21T13:01:38.146+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.9 KiB, free 434.2 MiB)
[2025-05-21T13:01:38.220+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
[2025-05-21T13:01:38.226+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8567accc62ca:34307 (size: 34.4 KiB, free: 434.4 MiB)
[2025-05-21T13:01:38.233+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO SparkContext: Created broadcast 0 from count at <unknown>:0
[2025-05-21T13:01:38.274+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-21T13:01:38.515+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO DAGScheduler: Registering RDD 3 (count at <unknown>:0) as input to shuffle 0
[2025-05-21T13:01:38.529+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO DAGScheduler: Got map stage job 0 (count at <unknown>:0) with 1 output partitions
[2025-05-21T13:01:38.530+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (count at <unknown>:0)
[2025-05-21T13:01:38.531+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO DAGScheduler: Parents of final stage: List()
[2025-05-21T13:01:38.538+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO DAGScheduler: Missing parents: List()
[2025-05-21T13:01:38.544+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at <unknown>:0), which has no missing parents
[2025-05-21T13:01:38.734+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.2 KiB, free 434.2 MiB)
[2025-05-21T13:01:38.742+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 434.1 MiB)
[2025-05-21T13:01:38.743+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8567accc62ca:34307 (size: 8.7 KiB, free: 434.4 MiB)
[2025-05-21T13:01:38.744+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-05-21T13:01:38.772+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-05-21T13:01:38.775+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-05-21T13:01:38.898+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8567accc62ca, executor driver, partition 0, ANY, 8247 bytes)
[2025-05-21T13:01:38.931+0000] {subprocess.py:93} INFO - 25/05/21 13:01:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-05-21T13:01:39.212+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO CodeGenerator: Code generated in 33.158442 ms
[2025-05-21T13:01:39.253+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO FileScanRDD: Reading File path: hdfs://namenode:9000/gold_price/news/raw/year=2025/month=5/day=21/news_data.json, range: 0-83497, partition values: [empty row]
[2025-05-21T13:01:39.286+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO CodeGenerator: Code generated in 21.582821 ms
[2025-05-21T13:01:39.337+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO CodeGenerator: Code generated in 10.222881 ms
[2025-05-21T13:01:39.839+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2070 bytes result sent to driver
[2025-05-21T13:01:39.859+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 990 ms on 8567accc62ca (executor driver) (1/1)
[2025-05-21T13:01:39.864+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-05-21T13:01:39.883+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO DAGScheduler: ShuffleMapStage 0 (count at <unknown>:0) finished in 1.311 s
[2025-05-21T13:01:39.887+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO DAGScheduler: looking for newly runnable stages
[2025-05-21T13:01:39.888+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO DAGScheduler: running: Set()
[2025-05-21T13:01:39.889+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO DAGScheduler: waiting: Set()
[2025-05-21T13:01:39.889+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO DAGScheduler: failed: Set()
[2025-05-21T13:01:39.994+0000] {subprocess.py:93} INFO - 25/05/21 13:01:39 INFO CodeGenerator: Code generated in 23.892219 ms
[2025-05-21T13:01:40.057+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO SparkContext: Starting job: count at <unknown>:0
[2025-05-21T13:01:40.062+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO DAGScheduler: Got job 1 (count at <unknown>:0) with 1 output partitions
[2025-05-21T13:01:40.063+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO DAGScheduler: Final stage: ResultStage 2 (count at <unknown>:0)
[2025-05-21T13:01:40.064+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2025-05-21T13:01:40.065+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO DAGScheduler: Missing parents: List()
[2025-05-21T13:01:40.068+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at count at <unknown>:0), which has no missing parents
[2025-05-21T13:01:40.086+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-05-21T13:01:40.090+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)
[2025-05-21T13:01:40.091+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8567accc62ca:34307 (size: 5.9 KiB, free: 434.4 MiB)
[2025-05-21T13:01:40.091+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-05-21T13:01:40.095+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-05-21T13:01:40.096+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-05-21T13:01:40.102+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (8567accc62ca, executor driver, partition 0, NODE_LOCAL, 7615 bytes)
[2025-05-21T13:01:40.105+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
[2025-05-21T13:01:40.185+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-21T13:01:40.189+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2025-05-21T13:01:40.212+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO CodeGenerator: Code generated in 13.709607 ms
[2025-05-21T13:01:40.262+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 4031 bytes result sent to driver
[2025-05-21T13:01:40.268+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 167 ms on 8567accc62ca (executor driver) (1/1)
[2025-05-21T13:01:40.269+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-05-21T13:01:40.270+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO DAGScheduler: ResultStage 2 (count at <unknown>:0) finished in 0.188 s
[2025-05-21T13:01:40.274+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-21T13:01:40.275+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-05-21T13:01:40.278+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO DAGScheduler: Job 1 finished: count at <unknown>:0, took 0.219825 s
[2025-05-21T13:01:40.300+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO FileSourceStrategy: Pushed Filters:
[2025-05-21T13:01:40.301+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO FileSourceStrategy: Post-Scan Filters:
[2025-05-21T13:01:40.312+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 199.9 KiB, free 433.9 MiB)
[2025-05-21T13:01:40.382+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
[2025-05-21T13:01:40.402+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8567accc62ca:34307 (size: 34.4 KiB, free: 434.3 MiB)
[2025-05-21T13:01:40.405+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO SparkContext: Created broadcast 3 from javaToPython at <unknown>:0
[2025-05-21T13:01:40.420+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-21T13:01:40.434+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 8567accc62ca:34307 in memory (size: 5.9 KiB, free: 434.3 MiB)
[2025-05-21T13:01:40.466+0000] {subprocess.py:93} INFO - 25/05/21 13:01:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8567accc62ca:34307 in memory (size: 8.7 KiB, free: 434.3 MiB)
[2025-05-21T13:01:41.851+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181
[2025-05-21T13:01:41.855+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:181) with 1 output partitions
[2025-05-21T13:01:41.856+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:181)
[2025-05-21T13:01:41.856+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO DAGScheduler: Parents of final stage: List()
[2025-05-21T13:01:41.861+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO DAGScheduler: Missing parents: List()
[2025-05-21T13:01:41.862+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[12] at RDD at PythonRDD.scala:53), which has no missing parents
[2025-05-21T13:01:41.880+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.2 KiB, free 433.9 MiB)
[2025-05-21T13:01:41.883+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 433.9 MiB)
[2025-05-21T13:01:41.886+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8567accc62ca:34307 (size: 10.5 KiB, free: 434.3 MiB)
[2025-05-21T13:01:41.887+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-05-21T13:01:41.890+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[12] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
[2025-05-21T13:01:41.891+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-05-21T13:01:41.894+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (8567accc62ca, executor driver, partition 0, ANY, 8258 bytes)
[2025-05-21T13:01:41.895+0000] {subprocess.py:93} INFO - 25/05/21 13:01:41 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
[2025-05-21T13:01:42.831+0000] {subprocess.py:93} INFO - 25/05/21 13:01:42 INFO FileScanRDD: Reading File path: hdfs://namenode:9000/gold_price/news/raw/year=2025/month=5/day=21/news_data.json, range: 0-83497, partition values: [empty row]
[2025-05-21T13:01:42.864+0000] {subprocess.py:93} INFO - 25/05/21 13:01:42 INFO CodeGenerator: Code generated in 19.353954 ms
[2025-05-21T13:01:43.114+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 2)
[2025-05-21T13:01:43.115+0000] {subprocess.py:93} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-05-21T13:01:43.116+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1247, in main
[2025-05-21T13:01:43.116+0000] {subprocess.py:93} INFO -     process()
[2025-05-21T13:01:43.117+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1239, in process
[2025-05-21T13:01:43.118+0000] {subprocess.py:93} INFO -     serializer.dump_stream(out_iter, outfile)
[2025-05-21T13:01:43.118+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 274, in dump_stream
[2025-05-21T13:01:43.118+0000] {subprocess.py:93} INFO -     vs = list(itertools.islice(iterator, batch))
[2025-05-21T13:01:43.119+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2849, in takeUpToNumLeft
[2025-05-21T13:01:43.119+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 83, in wrapper
[2025-05-21T13:01:43.120+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-21T13:01:43.120+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 76, in <lambda>
[2025-05-21T13:01:43.121+0000] {subprocess.py:93} INFO -     analyze_news_with_gemini(row.content or row.description or row.title)
[2025-05-21T13:01:43.122+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 62, in analyze_news_with_gemini
[2025-05-21T13:01:43.122+0000] {subprocess.py:93} INFO -     logger.info(f"Analyse de la news : {news_content[:50]}...")
[2025-05-21T13:01:43.122+0000] {subprocess.py:93} INFO - TypeError: 'NoneType' object is not subscriptable
[2025-05-21T13:01:43.123+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-05-21T13:01:43.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2025-05-21T13:01:43.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2025-05-21T13:01:43.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-05-21T13:01:43.126+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-05-21T13:01:43.126+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-05-21T13:01:43.127+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-05-21T13:01:43.127+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.128+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2025-05-21T13:01:43.128+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2025-05-21T13:01:43.129+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2025-05-21T13:01:43.129+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2025-05-21T13:01:43.130+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2025-05-21T13:01:43.130+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2025-05-21T13:01:43.130+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.131+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2025-05-21T13:01:43.131+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2025-05-21T13:01:43.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.132+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2025-05-21T13:01:43.132+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2025-05-21T13:01:43.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2025-05-21T13:01:43.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
[2025-05-21T13:01:43.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-21T13:01:43.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-21T13:01:43.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-21T13:01:43.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-21T13:01:43.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-21T13:01:43.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-21T13:01:43.136+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-21T13:01:43.136+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-21T13:01:43.137+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-21T13:01:43.137+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-21T13:01:43.138+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-21T13:01:43.156+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 2) (8567accc62ca executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-05-21T13:01:43.157+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1247, in main
[2025-05-21T13:01:43.158+0000] {subprocess.py:93} INFO -     process()
[2025-05-21T13:01:43.159+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1239, in process
[2025-05-21T13:01:43.160+0000] {subprocess.py:93} INFO -     serializer.dump_stream(out_iter, outfile)
[2025-05-21T13:01:43.160+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 274, in dump_stream
[2025-05-21T13:01:43.161+0000] {subprocess.py:93} INFO -     vs = list(itertools.islice(iterator, batch))
[2025-05-21T13:01:43.162+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2849, in takeUpToNumLeft
[2025-05-21T13:01:43.162+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 83, in wrapper
[2025-05-21T13:01:43.163+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-21T13:01:43.163+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 76, in <lambda>
[2025-05-21T13:01:43.164+0000] {subprocess.py:93} INFO -     analyze_news_with_gemini(row.content or row.description or row.title)
[2025-05-21T13:01:43.164+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 62, in analyze_news_with_gemini
[2025-05-21T13:01:43.165+0000] {subprocess.py:93} INFO -     logger.info(f"Analyse de la news : {news_content[:50]}...")
[2025-05-21T13:01:43.166+0000] {subprocess.py:93} INFO - TypeError: 'NoneType' object is not subscriptable
[2025-05-21T13:01:43.166+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.166+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-05-21T13:01:43.167+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2025-05-21T13:01:43.168+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2025-05-21T13:01:43.168+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-05-21T13:01:43.169+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-05-21T13:01:43.169+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-05-21T13:01:43.170+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-05-21T13:01:43.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.171+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2025-05-21T13:01:43.171+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2025-05-21T13:01:43.172+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2025-05-21T13:01:43.173+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2025-05-21T13:01:43.174+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2025-05-21T13:01:43.174+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2025-05-21T13:01:43.175+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.175+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2025-05-21T13:01:43.176+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2025-05-21T13:01:43.176+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.177+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2025-05-21T13:01:43.177+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2025-05-21T13:01:43.178+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.179+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2025-05-21T13:01:43.179+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
[2025-05-21T13:01:43.180+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-21T13:01:43.180+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-21T13:01:43.180+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-21T13:01:43.181+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-21T13:01:43.181+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-21T13:01:43.182+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-21T13:01:43.182+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-21T13:01:43.183+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-21T13:01:43.183+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-21T13:01:43.183+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-21T13:01:43.184+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-21T13:01:43.184+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.185+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
[2025-05-21T13:01:43.185+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-05-21T13:01:43.186+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO TaskSchedulerImpl: Cancelling stage 3
[2025-05-21T13:01:43.186+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 2) (8567accc62ca executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-05-21T13:01:43.187+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1247, in main
[2025-05-21T13:01:43.187+0000] {subprocess.py:93} INFO -     process()
[2025-05-21T13:01:43.188+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1239, in process
[2025-05-21T13:01:43.188+0000] {subprocess.py:93} INFO -     serializer.dump_stream(out_iter, outfile)
[2025-05-21T13:01:43.189+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 274, in dump_stream
[2025-05-21T13:01:43.189+0000] {subprocess.py:93} INFO -     vs = list(itertools.islice(iterator, batch))
[2025-05-21T13:01:43.189+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2849, in takeUpToNumLeft
[2025-05-21T13:01:43.189+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 83, in wrapper
[2025-05-21T13:01:43.190+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-21T13:01:43.190+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 76, in <lambda>
[2025-05-21T13:01:43.191+0000] {subprocess.py:93} INFO -     analyze_news_with_gemini(row.content or row.description or row.title)
[2025-05-21T13:01:43.191+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 62, in analyze_news_with_gemini
[2025-05-21T13:01:43.192+0000] {subprocess.py:93} INFO -     logger.info(f"Analyse de la news : {news_content[:50]}...")
[2025-05-21T13:01:43.193+0000] {subprocess.py:93} INFO - TypeError: 'NoneType' object is not subscriptable
[2025-05-21T13:01:43.193+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.194+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-05-21T13:01:43.194+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2025-05-21T13:01:43.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2025-05-21T13:01:43.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-05-21T13:01:43.196+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-05-21T13:01:43.196+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-05-21T13:01:43.197+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-05-21T13:01:43.197+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.198+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2025-05-21T13:01:43.198+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2025-05-21T13:01:43.199+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2025-05-21T13:01:43.199+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2025-05-21T13:01:43.200+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2025-05-21T13:01:43.200+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2025-05-21T13:01:43.201+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.202+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2025-05-21T13:01:43.202+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2025-05-21T13:01:43.203+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.203+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2025-05-21T13:01:43.204+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2025-05-21T13:01:43.204+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.205+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2025-05-21T13:01:43.205+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
[2025-05-21T13:01:43.206+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-21T13:01:43.206+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-21T13:01:43.206+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-21T13:01:43.207+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-21T13:01:43.207+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-21T13:01:43.208+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-21T13:01:43.208+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-21T13:01:43.208+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-21T13:01:43.209+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-21T13:01:43.209+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-21T13:01:43.210+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-21T13:01:43.210+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.211+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-05-21T13:01:43.212+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:181) failed in 1.303 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 2) (8567accc62ca executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-05-21T13:01:43.212+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1247, in main
[2025-05-21T13:01:43.213+0000] {subprocess.py:93} INFO -     process()
[2025-05-21T13:01:43.213+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1239, in process
[2025-05-21T13:01:43.214+0000] {subprocess.py:93} INFO -     serializer.dump_stream(out_iter, outfile)
[2025-05-21T13:01:43.214+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 274, in dump_stream
[2025-05-21T13:01:43.215+0000] {subprocess.py:93} INFO -     vs = list(itertools.islice(iterator, batch))
[2025-05-21T13:01:43.215+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2849, in takeUpToNumLeft
[2025-05-21T13:01:43.215+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 83, in wrapper
[2025-05-21T13:01:43.216+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-21T13:01:43.216+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 76, in <lambda>
[2025-05-21T13:01:43.217+0000] {subprocess.py:93} INFO -     analyze_news_with_gemini(row.content or row.description or row.title)
[2025-05-21T13:01:43.217+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 62, in analyze_news_with_gemini
[2025-05-21T13:01:43.217+0000] {subprocess.py:93} INFO -     logger.info(f"Analyse de la news : {news_content[:50]}...")
[2025-05-21T13:01:43.218+0000] {subprocess.py:93} INFO - TypeError: 'NoneType' object is not subscriptable
[2025-05-21T13:01:43.218+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-05-21T13:01:43.219+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2025-05-21T13:01:43.219+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2025-05-21T13:01:43.219+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-05-21T13:01:43.219+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-05-21T13:01:43.220+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-05-21T13:01:43.220+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-05-21T13:01:43.220+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.221+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2025-05-21T13:01:43.221+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2025-05-21T13:01:43.222+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2025-05-21T13:01:43.222+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2025-05-21T13:01:43.222+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2025-05-21T13:01:43.223+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2025-05-21T13:01:43.223+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.223+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2025-05-21T13:01:43.223+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2025-05-21T13:01:43.224+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.224+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2025-05-21T13:01:43.224+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2025-05-21T13:01:43.225+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.225+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2025-05-21T13:01:43.226+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
[2025-05-21T13:01:43.226+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-21T13:01:43.227+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-21T13:01:43.227+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-21T13:01:43.228+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-21T13:01:43.228+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-21T13:01:43.228+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-21T13:01:43.229+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-21T13:01:43.230+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-21T13:01:43.230+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-21T13:01:43.231+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-21T13:01:43.231+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-21T13:01:43.232+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.232+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-05-21T13:01:43.233+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO DAGScheduler: Job 2 failed: runJob at PythonRDD.scala:181, took 1.321737 s
[2025-05-21T13:01:43.362+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-05-21T13:01:43.363+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 80, in <module>
[2025-05-21T13:01:43.365+0000] {subprocess.py:93} INFO -     analyzed_df = news_rdd.toDF(columns)
[2025-05-21T13:01:43.366+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 122, in toDF
[2025-05-21T13:01:43.367+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1443, in createDataFrame
[2025-05-21T13:01:43.367+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1483, in _create_dataframe
[2025-05-21T13:01:43.368+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1056, in _createFromRDD
[2025-05-21T13:01:43.370+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 996, in _inferSchema
[2025-05-21T13:01:43.372+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2888, in first
[2025-05-21T13:01:43.373+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2855, in take
[2025-05-21T13:01:43.373+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/context.py", line 2510, in runJob
[2025-05-21T13:01:43.374+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-05-21T13:01:43.375+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
[2025-05-21T13:01:43.375+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-05-21T13:01:43.416+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
[2025-05-21T13:01:43.417+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 2) (8567accc62ca executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-05-21T13:01:43.418+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1247, in main
[2025-05-21T13:01:43.418+0000] {subprocess.py:93} INFO -     process()
[2025-05-21T13:01:43.419+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1239, in process
[2025-05-21T13:01:43.419+0000] {subprocess.py:93} INFO -     serializer.dump_stream(out_iter, outfile)
[2025-05-21T13:01:43.420+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 274, in dump_stream
[2025-05-21T13:01:43.420+0000] {subprocess.py:93} INFO -     vs = list(itertools.islice(iterator, batch))
[2025-05-21T13:01:43.421+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2849, in takeUpToNumLeft
[2025-05-21T13:01:43.421+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 83, in wrapper
[2025-05-21T13:01:43.422+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-21T13:01:43.422+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 76, in <lambda>
[2025-05-21T13:01:43.423+0000] {subprocess.py:93} INFO -     analyze_news_with_gemini(row.content or row.description or row.title)
[2025-05-21T13:01:43.423+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 62, in analyze_news_with_gemini
[2025-05-21T13:01:43.424+0000] {subprocess.py:93} INFO -     logger.info(f"Analyse de la news : {news_content[:50]}...")
[2025-05-21T13:01:43.425+0000] {subprocess.py:93} INFO - TypeError: 'NoneType' object is not subscriptable
[2025-05-21T13:01:43.425+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-05-21T13:01:43.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2025-05-21T13:01:43.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2025-05-21T13:01:43.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-05-21T13:01:43.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-05-21T13:01:43.428+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-05-21T13:01:43.428+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-05-21T13:01:43.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.429+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2025-05-21T13:01:43.430+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2025-05-21T13:01:43.430+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2025-05-21T13:01:43.431+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2025-05-21T13:01:43.432+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2025-05-21T13:01:43.432+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2025-05-21T13:01:43.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.433+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2025-05-21T13:01:43.434+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2025-05-21T13:01:43.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.435+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2025-05-21T13:01:43.435+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2025-05-21T13:01:43.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2025-05-21T13:01:43.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
[2025-05-21T13:01:43.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-21T13:01:43.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-21T13:01:43.438+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-21T13:01:43.439+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-21T13:01:43.439+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-21T13:01:43.440+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-21T13:01:43.440+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-21T13:01:43.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-21T13:01:43.441+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-21T13:01:43.442+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-21T13:01:43.443+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-21T13:01:43.443+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.444+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-05-21T13:01:43.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-05-21T13:01:43.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-05-21T13:01:43.446+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-05-21T13:01:43.446+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-05-21T13:01:43.447+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-05-21T13:01:43.448+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-05-21T13:01:43.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-05-21T13:01:43.449+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-05-21T13:01:43.450+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-05-21T13:01:43.450+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2025-05-21T13:01:43.451+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-05-21T13:01:43.451+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-05-21T13:01:43.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-05-21T13:01:43.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-05-21T13:01:43.453+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-05-21T13:01:43.453+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-05-21T13:01:43.454+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
[2025-05-21T13:01:43.454+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
[2025-05-21T13:01:43.455+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
[2025-05-21T13:01:43.455+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)
[2025-05-21T13:01:43.456+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
[2025-05-21T13:01:43.456+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-21T13:01:43.457+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-21T13:01:43.457+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-21T13:01:43.458+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-21T13:01:43.458+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-05-21T13:01:43.459+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-05-21T13:01:43.459+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2025-05-21T13:01:43.460+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-05-21T13:01:43.460+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-05-21T13:01:43.461+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-05-21T13:01:43.461+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-05-21T13:01:43.462+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-21T13:01:43.462+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2025-05-21T13:01:43.462+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1247, in main
[2025-05-21T13:01:43.463+0000] {subprocess.py:93} INFO -     process()
[2025-05-21T13:01:43.463+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 1239, in process
[2025-05-21T13:01:43.464+0000] {subprocess.py:93} INFO -     serializer.dump_stream(out_iter, outfile)
[2025-05-21T13:01:43.464+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 274, in dump_stream
[2025-05-21T13:01:43.465+0000] {subprocess.py:93} INFO -     vs = list(itertools.islice(iterator, batch))
[2025-05-21T13:01:43.465+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2849, in takeUpToNumLeft
[2025-05-21T13:01:43.465+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 83, in wrapper
[2025-05-21T13:01:43.466+0000] {subprocess.py:93} INFO -     return f(*args, **kwargs)
[2025-05-21T13:01:43.466+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 76, in <lambda>
[2025-05-21T13:01:43.467+0000] {subprocess.py:93} INFO -     analyze_news_with_gemini(row.content or row.description or row.title)
[2025-05-21T13:01:43.467+0000] {subprocess.py:93} INFO -   File "/scripts/analyze_news.py", line 62, in analyze_news_with_gemini
[2025-05-21T13:01:43.468+0000] {subprocess.py:93} INFO -     logger.info(f"Analyse de la news : {news_content[:50]}...")
[2025-05-21T13:01:43.469+0000] {subprocess.py:93} INFO - TypeError: 'NoneType' object is not subscriptable
[2025-05-21T13:01:43.469+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.470+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2025-05-21T13:01:43.470+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2025-05-21T13:01:43.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2025-05-21T13:01:43.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2025-05-21T13:01:43.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2025-05-21T13:01:43.473+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-05-21T13:01:43.473+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-05-21T13:01:43.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.474+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2025-05-21T13:01:43.475+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2025-05-21T13:01:43.475+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2025-05-21T13:01:43.476+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2025-05-21T13:01:43.476+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2025-05-21T13:01:43.477+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2025-05-21T13:01:43.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.478+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2025-05-21T13:01:43.478+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2025-05-21T13:01:43.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.479+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2025-05-21T13:01:43.480+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2025-05-21T13:01:43.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2025-05-21T13:01:43.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2025-05-21T13:01:43.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
[2025-05-21T13:01:43.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-21T13:01:43.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-21T13:01:43.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-21T13:01:43.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-21T13:01:43.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-21T13:01:43.484+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-21T13:01:43.484+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-21T13:01:43.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-21T13:01:43.485+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-21T13:01:43.486+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-21T13:01:43.486+0000] {subprocess.py:93} INFO - 	... 1 more
[2025-05-21T13:01:43.487+0000] {subprocess.py:93} INFO - 
[2025-05-21T13:01:43.600+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO SparkContext: Invoking stop() from shutdown hook
[2025-05-21T13:01:43.601+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-05-21T13:01:43.615+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO SparkUI: Stopped Spark web UI at http://8567accc62ca:4040
[2025-05-21T13:01:43.634+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-05-21T13:01:43.652+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO MemoryStore: MemoryStore cleared
[2025-05-21T13:01:43.654+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO BlockManager: BlockManager stopped
[2025-05-21T13:01:43.660+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-05-21T13:01:43.665+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-05-21T13:01:43.683+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO SparkContext: Successfully stopped SparkContext
[2025-05-21T13:01:43.684+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO ShutdownHookManager: Shutdown hook called
[2025-05-21T13:01:43.686+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-5e3ba18d-7f91-4823-9909-823559515ef4
[2025-05-21T13:01:43.694+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-5e3ba18d-7f91-4823-9909-823559515ef4/pyspark-6968fccf-ec8b-4b4e-b0c6-fe1571441ea8
[2025-05-21T13:01:43.700+0000] {subprocess.py:93} INFO - 25/05/21 13:01:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-7477a6f7-e2e3-46cf-917e-ce7b6cda521a
[2025-05-21T13:01:43.781+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-05-21T13:01:43.801+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-05-21T13:01:43.811+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=gold_news_pipeline, task_id=analyze_news, execution_date=20250521T130106, start_date=20250521T130125, end_date=20250521T130143
[2025-05-21T13:01:43.813+0000] {news_batch.py:190} ERROR - Tche choue : analyze_news, logs : http://localhost:8080/log?execution_date=2025-05-21T13%3A01%3A06.015064%2B00%3A00&task_id=analyze_news&dag_id=gold_news_pipeline&map_index=-1
[2025-05-21T13:01:43.846+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 134 for task analyze_news (Bash command failed. The command returned a non-zero exit code 1.; 3276)
[2025-05-21T13:01:43.869+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-05-21T13:01:43.887+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
